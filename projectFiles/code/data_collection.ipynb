{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting and Organizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to make a master dataframe with all the relevant data. This master dataframe will contain an entry for every single onset, for every single wav file in the audio file. If an audio file is multiple drum sounds, then there is a single onset for each drum sound, and an single audio file will contirnbute to multiple entries in the dataset. We will have to parse AVP and LVT seperately and then combine them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse AVP csv, get the onset time, instrument label, onset phoneme, coda phoneme, dataset, participant id, subset, csv file path, wav file path\n",
    "def parse_avp_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Parses an AVP CSV with no header, returning a list of dicts.\n",
    "    Each dict has:\n",
    "      - onset_time (float)\n",
    "      - instrument_label (str)\n",
    "      - onset_phoneme (str)\n",
    "      - coda_phoneme (str)\n",
    "      - dataset (str) = \"AVP\"\n",
    "      - participant_id (str)\n",
    "      - subset (str) = \"personal\"\n",
    "      - csv_file_path (str)\n",
    "      - wav_file_path (str)\n",
    "    \"\"\"\n",
    "    # Extract some metadata from the file path\n",
    "    csv_dir = os.path.dirname(csv_path)             \n",
    "    csv_file_name = os.path.basename(csv_path)     \n",
    "    base_name, _ = os.path.splitext(csv_file_name) \n",
    "    \n",
    "    # Determine participant_id from the file name\n",
    "    parts = base_name.split(\"_\")  \n",
    "    participant_id = parts[0]    \n",
    "    \n",
    "    # Build the wav path. \n",
    "    wav_file_name = base_name + \".wav\"\n",
    "    wav_file_path = os.path.join(csv_dir, wav_file_name)\n",
    "    \n",
    "    dataset = \"AVP\"\n",
    "    subset = \"personal\"\n",
    "    \n",
    "    # Parse each row of the CSV\n",
    "    data = []\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue  # skip empty or malformed lines\n",
    "            onset_time = float(row[0])\n",
    "            instrument_label = row[1]\n",
    "            onset_phoneme = row[2] if len(row) > 2 else ''\n",
    "            coda_phoneme = row[3] if len(row) > 3 else ''\n",
    "            \n",
    "            entry = {\n",
    "                'onset_time': onset_time,\n",
    "                'instrument_label': instrument_label,\n",
    "                'onset_phoneme': onset_phoneme,\n",
    "                'coda_phoneme': coda_phoneme,\n",
    "                'dataset': dataset,\n",
    "                'participant_id': participant_id,\n",
    "                'subset': subset,\n",
    "                'csv_file_path': csv_path,\n",
    "                'wav_file_path': wav_file_path\n",
    "            }\n",
    "            data.append(entry)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_avp_data(root_dir):\n",
    "    \"\"\"\n",
    "    Walks through the AVP dataset directory and collects all CSV data into a master DataFrame,\n",
    "    maintaining the grouping of entries from the same CSV file\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    personal_dir = os.path.join(root_dir, \"Personal\")\n",
    "    \n",
    "    # Walk through all participant directories in sorted order\n",
    "    for participant_dir in sorted(os.listdir(personal_dir)):\n",
    "        participant_path = os.path.join(personal_dir, participant_dir)\n",
    "        \n",
    "        # Skip if not a directory or hidden files\n",
    "        if not os.path.isdir(participant_path) or participant_dir.startswith('.'):\n",
    "            continue\n",
    "            \n",
    "        # Process CSV files in sorted order\n",
    "        for file_name in sorted(os.listdir(participant_path)):\n",
    "            if file_name.endswith('.csv'):\n",
    "                csv_path = os.path.join(participant_path, file_name)\n",
    "                \n",
    "                try:\n",
    "                    parsed_data = parse_avp_csv(csv_path)\n",
    "                    # Add the source filename as a field for sorting\n",
    "                    for entry in parsed_data:\n",
    "                        entry['source_file'] = file_name\n",
    "                    all_data.extend(parsed_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {csv_path}: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Sort to maintain grouping:\n",
    "    df = df.sort_values(['participant_id', 'source_file', 'onset_time'])\n",
    "    \n",
    "    # Optionally remove the temporary source_file column if you don't need it\n",
    "    df = df.drop('source_file', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lvt_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Parses an LVT CSV with no header, returning a list of dicts.\n",
    "    Similar to parse_avp_csv but handles LVT-specific formatting.\n",
    "    \"\"\"\n",
    "    # Extract metadata from the file path\n",
    "    csv_dir = os.path.dirname(csv_path)             \n",
    "    csv_file_name = os.path.basename(csv_path)      # e.g., \"AFRP.csv\"\n",
    "    base_name, _ = os.path.splitext(csv_file_name)  # e.g., \"AFRP\"\n",
    "    \n",
    "    # Determine if this is from Frase or Improviso folder\n",
    "    subset = \"Frase\" if \"Frase\" in csv_dir else \"Improviso\"\n",
    "    \n",
    "    participant_id = base_name\n",
    "    \n",
    "    # Build the wav path\n",
    "    wav_file_name = base_name + \"3.wav\"\n",
    "    wav_file_path = os.path.join(csv_dir, wav_file_name)\n",
    "    \n",
    "    # Mapping for instrument labels\n",
    "    instrument_map = {\n",
    "        \"Kick\": \"kd\",\n",
    "        \"Snare\": \"sd\",\n",
    "        \"HH\": \"hhc\"  # Assuming all HH in LVT are closed hi-hats\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue \n",
    "                \n",
    "            onset_time = float(row[0])\n",
    "            original_label = row[1]\n",
    "            instrument_label = instrument_map.get(original_label, original_label)\n",
    "            onset_phoneme = row[2] if len(row) > 2 else ''\n",
    "            coda_phoneme = row[3] if len(row) > 3 else ''\n",
    "            \n",
    "            # onset_phoneme = get_standardized_phoneme(row[2], is_onset=True)   # converts 'ts' if needed\n",
    "            # coda_phoneme = get_standardized_phoneme(row[3], is_onset=False)   # converts 'x' if needed\n",
    "            \n",
    "            entry = {\n",
    "                'onset_time': onset_time,\n",
    "                'instrument_label': instrument_label,\n",
    "                'onset_phoneme': onset_phoneme,\n",
    "                'coda_phoneme': coda_phoneme,\n",
    "                'dataset': \"LVT\",\n",
    "                'participant_id': participant_id,\n",
    "                'subset': subset,\n",
    "                'csv_file_path': csv_path,\n",
    "                'wav_file_path': wav_file_path\n",
    "            }\n",
    "            data.append(entry)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_lvt_data(root_dir):\n",
    "    \"\"\"\n",
    "    Walks through the LVT dataset directory and collects all CSV data into a master DataFrame\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # Process both Frase and Improviso folders\n",
    "    for subset_dir in [\"Frase\", \"Improviso\"]:\n",
    "        subset_path = os.path.join(root_dir, subset_dir)\n",
    "        \n",
    "        # Skip if directory doesn't exist\n",
    "        if not os.path.isdir(subset_path):\n",
    "            continue\n",
    "            \n",
    "        # Process CSV files in sorted order\n",
    "        for file_name in sorted(os.listdir(subset_path)):\n",
    "            if file_name.endswith('.csv') and not file_name.startswith('.'):\n",
    "                csv_path = os.path.join(subset_path, file_name)\n",
    "                \n",
    "                try:\n",
    "                    parsed_data = parse_lvt_csv(csv_path)\n",
    "                    # Add source file for sorting\n",
    "                    for entry in parsed_data:\n",
    "                        entry['source_file'] = file_name\n",
    "                    all_data.extend(parsed_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {csv_path}: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Sort to maintain grouping\n",
    "    df = df.sort_values(['subset', 'participant_id', 'source_file', 'onset_time'])\n",
    "    \n",
    "    # Remove temporary sorting column\n",
    "    df = df.drop('source_file', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_datasets():\n",
    "    avp_dataset_path = \"../../AVP-LVT_Dataset/AVP_Dataset\"\n",
    "    lvt_dataset_path = \"../../AVP-LVT_Dataset/LVT_Dataset\"\n",
    "    \n",
    "    # Collect data from both datasets\n",
    "    print(\"Processing AVP dataset...\")\n",
    "    avp_df = collect_all_avp_data(avp_dataset_path)\n",
    "    \n",
    "    print(\"Processing LVT dataset...\")\n",
    "    lvt_df = collect_all_lvt_data(lvt_dataset_path)\n",
    "    \n",
    "    # Save individual datasets\n",
    "    print(\"\\nSaving individual datasets...\")\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    \n",
    "    avp_df.to_csv('../data/avp_dataset.csv', index=False)\n",
    "    lvt_df.to_csv('../data/lvt_dataset.csv', index=False)\n",
    "    \n",
    "    # Combine and save master dataset\n",
    "    print(\"Creating and saving master dataset...\")\n",
    "    master_df = pd.concat([avp_df, lvt_df], ignore_index=True)\n",
    "    master_df.to_csv('../data/master_dataset.csv', index=False)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nDataset Summaries:\")\n",
    "    print(f\"AVP Dataset: {len(avp_df)} events\")\n",
    "    print(\"\\nAVP participants:\", len(avp_df['participant_id'].unique()))\n",
    "    print(\"AVP instrument distribution:\")\n",
    "    print(avp_df['instrument_label'].value_counts())\n",
    "    \n",
    "    print(f\"\\nLVT Dataset: {len(lvt_df)} events\")\n",
    "    print(\"LVT subsets:\", lvt_df['subset'].unique())\n",
    "    print(\"LVT participants:\", len(lvt_df['participant_id'].unique()))\n",
    "    print(\"LVT instrument distribution:\")\n",
    "    print(lvt_df['instrument_label'].value_counts())\n",
    "    \n",
    "    print(f\"\\nMaster Dataset: {len(master_df)} total events\")\n",
    "    print(\"Distribution by dataset:\")\n",
    "    print(master_df['dataset'].value_counts())\n",
    "    \n",
    "    return avp_df, lvt_df, master_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AVP dataset...\n",
      "Processing LVT dataset...\n",
      "\n",
      "Saving individual datasets...\n",
      "Creating and saving master dataset...\n",
      "\n",
      "Dataset Summaries:\n",
      "AVP Dataset: 4873 events\n",
      "\n",
      "AVP participants: 28\n",
      "AVP instrument distribution:\n",
      "instrument_label\n",
      "kd     1447\n",
      "sd     1253\n",
      "hhc    1164\n",
      "hho    1009\n",
      "Name: count, dtype: int64\n",
      "\n",
      "LVT Dataset: 841 events\n",
      "LVT subsets: ['Frase' 'Improviso']\n",
      "LVT participants: 40\n",
      "LVT instrument distribution:\n",
      "instrument_label\n",
      "hhc    334\n",
      "kd     329\n",
      "sd     178\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Master Dataset: 5714 total events\n",
      "Distribution by dataset:\n",
      "dataset\n",
      "AVP    4873\n",
      "LVT     841\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "avp_df, lvt_df, master_df = create_all_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have master_dataset.csv, and master_df, both of which contain the info for every single onset for every single sound in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVP Unique Onset Phonemes:\n",
      "['!', 'dʒ', 'k', 'kg', 'kʃ', 'p', 's', 't', 'ts', 'tɕ', 'tʃ', 'tʒ', 'ʡʢ']\n",
      "\n",
      "AVP Unique Coda Phonemes:\n",
      "['I', 'a', 'e', 'h', 'i', 'o', 'u', 'x', 'æ', 'œ', 'ɐ', 'ɘ', 'ə', 'ɪ', 'ɯ', 'ʊ', 'ʌ']\n",
      "\n",
      "LVT Unique Onset Phonemes:\n",
      "['!', 'k', 'p', 's', 't', 'ts', 'tʃ', 'ʔ', 'ʡʢ']\n",
      "\n",
      "LVT Unique Coda Phonemes:\n",
      "['a', 'h', 'u', 'x', 'ʊ']\n"
     ]
    }
   ],
   "source": [
    "def analyze_phonemes():\n",
    "    \"\"\"\n",
    "    Analyze and compare phonemes between AVP and LVT datasets\n",
    "    \"\"\"\n",
    "    # Read both datasets\n",
    "    avp_df = pd.read_csv('../data/avp_dataset.csv')\n",
    "    lvt_df = pd.read_csv('../data/lvt_dataset.csv')\n",
    "    \n",
    "    print(\"AVP Unique Onset Phonemes:\")\n",
    "    print(sorted(avp_df['onset_phoneme'].unique()))\n",
    "    print(\"\\nAVP Unique Coda Phonemes:\")\n",
    "    print(sorted(avp_df['coda_phoneme'].unique()))\n",
    "    \n",
    "    print(\"\\nLVT Unique Onset Phonemes:\")\n",
    "    print(sorted(lvt_df['onset_phoneme'].unique()))\n",
    "    print(\"\\nLVT Unique Coda Phonemes:\")\n",
    "    print(sorted(lvt_df['coda_phoneme'].unique()))\n",
    "\n",
    "# Run the analysis\n",
    "analyze_phonemes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
