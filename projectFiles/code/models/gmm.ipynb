{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_cluster(n_components=4, features_path='features', output_dir='', covariance_type='full'):\n",
    "    \"\"\"\n",
    "    Perform GMM clustering on features\n",
    "    \n",
    "    Args:\n",
    "        n_components: Number of Gaussian components (clusters)\n",
    "        features_path: Path to feature file\n",
    "        output_dir: Where to save cluster assignments\n",
    "        covariance_type: Type of covariance parameter ('full', 'tied', 'diag', 'spherical')\n",
    "    \"\"\"\n",
    "    # Load features\n",
    "    X = np.load(features_path)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Fit GMM\n",
    "    print(f\"Fitting GMM with {n_components} components...\")\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=n_components,\n",
    "        covariance_type=covariance_type,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Get cluster assignments and probabilities\n",
    "    cluster_labels = gmm.fit_predict(X_scaled)\n",
    "    probabilities = gmm.predict_proba(X_scaled)\n",
    "    \n",
    "    # Save cluster assignments\n",
    "    np.save(output_dir, cluster_labels)\n",
    "    \n",
    "    # Print cluster sizes\n",
    "    print(\"\\nCluster sizes:\")\n",
    "    for i in range(n_components):\n",
    "        print(f\"Cluster {i}: {np.sum(cluster_labels == i)} sounds\")\n",
    "    \n",
    "    # Calculate average probability for each cluster\n",
    "    avg_probs = np.mean(probabilities, axis=0)\n",
    "    print(\"\\nAverage probability for each cluster:\")\n",
    "    for i, prob in enumerate(avg_probs):\n",
    "        print(f\"Cluster {i}: {prob:.3f}\")\n",
    "    \n",
    "    return cluster_labels, probabilities, gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating GMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gmm_clustering(features_path='features', labels_path='labels.npy', \n",
    "                          cluster_labels_path='cluster_labels.npy', \n",
    "                          probabilities=None, viz_dir=''):\n",
    "    \"\"\"\n",
    "    Evaluate GMM clustering results using both internal and external metrics\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    X = np.load(features_path)\n",
    "    y_true = np.load(labels_path)\n",
    "    cluster_labels = np.load(cluster_labels_path)\n",
    "    \n",
    "    # Standardize features\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # 1. Internal Metrics\n",
    "    silhouette = silhouette_score(X_scaled, cluster_labels)\n",
    "    davies_bouldin = davies_bouldin_score(X_scaled, cluster_labels)\n",
    "    calinski_harabasz = calinski_harabasz_score(X_scaled, cluster_labels)\n",
    "    \n",
    "    print(\"Internal Metrics:\")\n",
    "    print(f\"Silhouette Score: {silhouette:.3f} (ranges from -1 to 1, higher is better)\")\n",
    "    print(f\"Davies-Bouldin Index: {davies_bouldin:.3f} (lower is better)\")\n",
    "    print(f\"Calinski-Harabasz Index: {calinski_harabasz:.3f} (higher is better)\")\n",
    "    \n",
    "    # 2. External Metrics\n",
    "    ari = adjusted_rand_score(y_true, cluster_labels)\n",
    "    nmi = normalized_mutual_info_score(y_true, cluster_labels)\n",
    "    \n",
    "    print(\"\\nExternal Metrics:\")\n",
    "    print(f\"Adjusted Rand Index: {ari:.3f} (ranges from -1 to 1, higher is better)\")\n",
    "    print(f\"Normalized Mutual Information: {nmi:.3f} (ranges from 0 to 1, higher is better)\")\n",
    "    \n",
    "    # 3. Create confusion matrix with probabilities\n",
    "    unique_labels = np.unique(y_true)\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    confusion_matrix = np.zeros((len(unique_labels), len(unique_clusters)))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        for j, cluster in enumerate(unique_clusters):\n",
    "            confusion_matrix[i, j] = np.sum((y_true == label) & (cluster_labels == cluster))\n",
    "    \n",
    "    # Normalize by row (true labels)\n",
    "    confusion_matrix_normalized = confusion_matrix / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(confusion_matrix_normalized, \n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                xticklabels=[f'Cluster {i}' for i in range(len(unique_clusters))],\n",
    "                yticklabels=unique_labels,\n",
    "                cmap='YlOrRd')\n",
    "    plt.title('Normalized Confusion Matrix:\\nTrue Labels vs Cluster Assignments')\n",
    "    plt.xlabel('Predicted Cluster')\n",
    "    plt.ylabel('True Label')\n",
    "    \n",
    "    # Save confusion matrix\n",
    "    os.makedirs(os.path.dirname(viz_dir), exist_ok=True)\n",
    "    plt.savefig(viz_dir, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Print cluster composition with probability information\n",
    "    print(\"\\nCluster Composition:\")\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_mask = cluster_labels == cluster\n",
    "        print(f\"\\nCluster {cluster}:\")\n",
    "        for label in unique_labels:\n",
    "            count = np.sum((y_true == label) & cluster_mask)\n",
    "            percentage = (count / np.sum(cluster_mask)) * 100\n",
    "            \n",
    "            # Add average probability for this label in this cluster\n",
    "            if probabilities is not None:\n",
    "                avg_prob = np.mean(probabilities[cluster_mask & (y_true == label)][:, cluster])\n",
    "                print(f\"{label}: {count} samples ({percentage:.1f}%), Avg Probability: {avg_prob:.3f}\")\n",
    "            else:\n",
    "                print(f\"{label}: {count} samples ({percentage:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'silhouette': silhouette,\n",
    "        'davies_bouldin': davies_bouldin,\n",
    "        'calinski_harabasz': calinski_harabasz,\n",
    "        'ari': ari,\n",
    "        'nmi': nmi,\n",
    "        'confusion_matrix': confusion_matrix,\n",
    "        'confusion_matrix_normalized': confusion_matrix_normalized\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing MFCC Basic...\n",
      "Fitting GMM with 4 components...\n",
      "\n",
      "Cluster sizes:\n",
      "Cluster 0: 1072 sounds\n",
      "Cluster 1: 1562 sounds\n",
      "Cluster 2: 1714 sounds\n",
      "Cluster 3: 1366 sounds\n",
      "\n",
      "Average probability for each cluster:\n",
      "Cluster 0: 0.187\n",
      "Cluster 1: 0.269\n",
      "Cluster 2: 0.299\n",
      "Cluster 3: 0.245\n",
      "Internal Metrics:\n",
      "Silhouette Score: 0.091 (ranges from -1 to 1, higher is better)\n",
      "Davies-Bouldin Index: 2.362 (lower is better)\n",
      "Calinski-Harabasz Index: 501.644 (higher is better)\n",
      "\n",
      "External Metrics:\n",
      "Adjusted Rand Index: 0.085 (ranges from -1 to 1, higher is better)\n",
      "Normalized Mutual Information: 0.090 (ranges from 0 to 1, higher is better)\n",
      "\n",
      "Cluster Composition:\n",
      "\n",
      "Cluster 0:\n",
      "hhc: 164 samples (15.3%), Avg Probability: 0.902\n",
      "hho: 310 samples (28.9%), Avg Probability: 0.961\n",
      "kd: 165 samples (15.4%), Avg Probability: 0.856\n",
      "sd: 433 samples (40.4%), Avg Probability: 0.926\n",
      "\n",
      "Cluster 1:\n",
      "hhc: 316 samples (20.2%), Avg Probability: 0.920\n",
      "hho: 129 samples (8.3%), Avg Probability: 0.921\n",
      "kd: 860 samples (55.1%), Avg Probability: 0.956\n",
      "sd: 257 samples (16.5%), Avg Probability: 0.898\n",
      "\n",
      "Cluster 2:\n",
      "hhc: 453 samples (26.4%), Avg Probability: 0.937\n",
      "hho: 125 samples (7.3%), Avg Probability: 0.955\n",
      "kd: 588 samples (34.3%), Avg Probability: 0.922\n",
      "sd: 548 samples (32.0%), Avg Probability: 0.935\n",
      "\n",
      "Cluster 3:\n",
      "hhc: 565 samples (41.4%), Avg Probability: 0.933\n",
      "hho: 445 samples (32.6%), Avg Probability: 0.978\n",
      "kd: 163 samples (11.9%), Avg Probability: 0.793\n",
      "sd: 193 samples (14.1%), Avg Probability: 0.898\n",
      "\n",
      "Processing MFCC + Envelope...\n",
      "Fitting GMM with 4 components...\n",
      "\n",
      "Cluster sizes:\n",
      "Cluster 0: 1440 sounds\n",
      "Cluster 1: 1498 sounds\n",
      "Cluster 2: 1000 sounds\n",
      "Cluster 3: 1776 sounds\n",
      "\n",
      "Average probability for each cluster:\n",
      "Cluster 0: 0.253\n",
      "Cluster 1: 0.262\n",
      "Cluster 2: 0.175\n",
      "Cluster 3: 0.310\n",
      "Internal Metrics:\n",
      "Silhouette Score: 0.053 (ranges from -1 to 1, higher is better)\n",
      "Davies-Bouldin Index: 3.682 (lower is better)\n",
      "Calinski-Harabasz Index: 282.171 (higher is better)\n",
      "\n",
      "External Metrics:\n",
      "Adjusted Rand Index: 0.059 (ranges from -1 to 1, higher is better)\n",
      "Normalized Mutual Information: 0.077 (ranges from 0 to 1, higher is better)\n",
      "\n",
      "Cluster Composition:\n",
      "\n",
      "Cluster 0:\n",
      "hhc: 522 samples (36.2%), Avg Probability: 0.979\n",
      "hho: 433 samples (30.1%), Avg Probability: 0.980\n",
      "kd: 198 samples (13.8%), Avg Probability: 0.942\n",
      "sd: 287 samples (19.9%), Avg Probability: 0.962\n",
      "\n",
      "Cluster 1:\n",
      "hhc: 351 samples (23.4%), Avg Probability: 0.966\n",
      "hho: 358 samples (23.9%), Avg Probability: 0.974\n",
      "kd: 324 samples (21.6%), Avg Probability: 0.961\n",
      "sd: 465 samples (31.0%), Avg Probability: 0.968\n",
      "\n",
      "Cluster 2:\n",
      "hhc: 88 samples (8.8%), Avg Probability: 0.986\n",
      "hho: 96 samples (9.6%), Avg Probability: 0.997\n",
      "kd: 672 samples (67.2%), Avg Probability: 0.991\n",
      "sd: 144 samples (14.4%), Avg Probability: 0.986\n",
      "\n",
      "Cluster 3:\n",
      "hhc: 537 samples (30.2%), Avg Probability: 0.980\n",
      "hho: 122 samples (6.9%), Avg Probability: 0.939\n",
      "kd: 582 samples (32.8%), Avg Probability: 0.973\n",
      "sd: 535 samples (30.1%), Avg Probability: 0.973\n",
      "\n",
      "Processing MFCC Optimized...\n",
      "Fitting GMM with 4 components...\n",
      "\n",
      "Cluster sizes:\n",
      "Cluster 0: 2342 sounds\n",
      "Cluster 1: 1210 sounds\n",
      "Cluster 2: 840 sounds\n",
      "Cluster 3: 1322 sounds\n",
      "\n",
      "Average probability for each cluster:\n",
      "Cluster 0: 0.407\n",
      "Cluster 1: 0.216\n",
      "Cluster 2: 0.146\n",
      "Cluster 3: 0.232\n",
      "Internal Metrics:\n",
      "Silhouette Score: 0.071 (ranges from -1 to 1, higher is better)\n",
      "Davies-Bouldin Index: 2.720 (lower is better)\n",
      "Calinski-Harabasz Index: 541.215 (higher is better)\n",
      "\n",
      "External Metrics:\n",
      "Adjusted Rand Index: 0.065 (ranges from -1 to 1, higher is better)\n",
      "Normalized Mutual Information: 0.092 (ranges from 0 to 1, higher is better)\n",
      "\n",
      "Cluster Composition:\n",
      "\n",
      "Cluster 0:\n",
      "hhc: 544 samples (23.2%), Avg Probability: 0.894\n",
      "hho: 297 samples (12.7%), Avg Probability: 0.920\n",
      "kd: 677 samples (28.9%), Avg Probability: 0.933\n",
      "sd: 824 samples (35.2%), Avg Probability: 0.942\n",
      "\n",
      "Cluster 1:\n",
      "hhc: 455 samples (37.6%), Avg Probability: 0.898\n",
      "hho: 83 samples (6.9%), Avg Probability: 0.884\n",
      "kd: 404 samples (33.4%), Avg Probability: 0.922\n",
      "sd: 268 samples (22.1%), Avg Probability: 0.924\n",
      "\n",
      "Cluster 2:\n",
      "hhc: 99 samples (11.8%), Avg Probability: 0.972\n",
      "hho: 78 samples (9.3%), Avg Probability: 0.957\n",
      "kd: 573 samples (68.2%), Avg Probability: 0.978\n",
      "sd: 90 samples (10.7%), Avg Probability: 0.935\n",
      "\n",
      "Cluster 3:\n",
      "hhc: 400 samples (30.3%), Avg Probability: 0.895\n",
      "hho: 551 samples (41.7%), Avg Probability: 0.940\n",
      "kd: 122 samples (9.2%), Avg Probability: 0.893\n",
      "sd: 249 samples (18.8%), Avg Probability: 0.898\n",
      "\n",
      "Processing MFCC Basic Augmented...\n",
      "Fitting GMM with 4 components...\n",
      "\n",
      "Cluster sizes:\n",
      "Cluster 0: 6874 sounds\n",
      "Cluster 1: 10369 sounds\n",
      "Cluster 2: 7739 sounds\n",
      "Cluster 3: 9302 sounds\n",
      "\n",
      "Average probability for each cluster:\n",
      "Cluster 0: 0.199\n",
      "Cluster 1: 0.296\n",
      "Cluster 2: 0.224\n",
      "Cluster 3: 0.281\n",
      "Internal Metrics:\n",
      "Silhouette Score: 0.060 (ranges from -1 to 1, higher is better)\n",
      "Davies-Bouldin Index: 3.315 (lower is better)\n",
      "Calinski-Harabasz Index: 2400.611 (higher is better)\n",
      "\n",
      "External Metrics:\n",
      "Adjusted Rand Index: 0.091 (ranges from -1 to 1, higher is better)\n",
      "Normalized Mutual Information: 0.115 (ranges from 0 to 1, higher is better)\n",
      "\n",
      "Cluster Composition:\n",
      "\n",
      "Cluster 0:\n",
      "hhc: 1146 samples (16.7%), Avg Probability: 0.910\n",
      "hho: 290 samples (4.2%), Avg Probability: 0.907\n",
      "kd: 4650 samples (67.6%), Avg Probability: 0.963\n",
      "sd: 788 samples (11.5%), Avg Probability: 0.878\n",
      "\n",
      "Cluster 1:\n",
      "hhc: 3399 samples (32.8%), Avg Probability: 0.912\n",
      "hho: 3974 samples (38.3%), Avg Probability: 0.962\n",
      "kd: 546 samples (5.3%), Avg Probability: 0.850\n",
      "sd: 2450 samples (23.6%), Avg Probability: 0.930\n",
      "\n",
      "Cluster 2:\n",
      "hhc: 2158 samples (27.9%), Avg Probability: 0.900\n",
      "hho: 463 samples (6.0%), Avg Probability: 0.881\n",
      "kd: 2793 samples (36.1%), Avg Probability: 0.894\n",
      "sd: 2325 samples (30.0%), Avg Probability: 0.919\n",
      "\n",
      "Cluster 3:\n",
      "hhc: 2285 samples (24.6%), Avg Probability: 0.898\n",
      "hho: 1327 samples (14.3%), Avg Probability: 0.931\n",
      "kd: 2667 samples (28.7%), Avg Probability: 0.873\n",
      "sd: 3023 samples (32.5%), Avg Probability: 0.901\n",
      "\n",
      "Processing MFCC + Envelope Augmented...\n",
      "Fitting GMM with 4 components...\n",
      "\n",
      "Cluster sizes:\n",
      "Cluster 0: 6315 sounds\n",
      "Cluster 1: 13262 sounds\n",
      "Cluster 2: 8646 sounds\n",
      "Cluster 3: 6061 sounds\n",
      "\n",
      "Average probability for each cluster:\n",
      "Cluster 0: 0.184\n",
      "Cluster 1: 0.387\n",
      "Cluster 2: 0.254\n",
      "Cluster 3: 0.176\n",
      "Internal Metrics:\n",
      "Silhouette Score: 0.043 (ranges from -1 to 1, higher is better)\n",
      "Davies-Bouldin Index: 3.704 (lower is better)\n",
      "Calinski-Harabasz Index: 1666.029 (higher is better)\n",
      "\n",
      "External Metrics:\n",
      "Adjusted Rand Index: 0.051 (ranges from -1 to 1, higher is better)\n",
      "Normalized Mutual Information: 0.073 (ranges from 0 to 1, higher is better)\n",
      "\n",
      "Cluster Composition:\n",
      "\n",
      "Cluster 0:\n",
      "hhc: 705 samples (11.2%), Avg Probability: 0.978\n",
      "hho: 576 samples (9.1%), Avg Probability: 0.985\n",
      "kd: 4041 samples (64.0%), Avg Probability: 0.983\n",
      "sd: 993 samples (15.7%), Avg Probability: 0.963\n",
      "\n",
      "Cluster 1:\n",
      "hhc: 3164 samples (23.9%), Avg Probability: 0.952\n",
      "hho: 4248 samples (32.0%), Avg Probability: 0.973\n",
      "kd: 2535 samples (19.1%), Avg Probability: 0.938\n",
      "sd: 3315 samples (25.0%), Avg Probability: 0.953\n",
      "\n",
      "Cluster 2:\n",
      "hhc: 2737 samples (31.7%), Avg Probability: 0.956\n",
      "hho: 511 samples (5.9%), Avg Probability: 0.912\n",
      "kd: 2864 samples (33.1%), Avg Probability: 0.953\n",
      "sd: 2534 samples (29.3%), Avg Probability: 0.961\n",
      "\n",
      "Cluster 3:\n",
      "hhc: 2382 samples (39.3%), Avg Probability: 0.960\n",
      "hho: 719 samples (11.9%), Avg Probability: 0.938\n",
      "kd: 1216 samples (20.1%), Avg Probability: 0.939\n",
      "sd: 1744 samples (28.8%), Avg Probability: 0.953\n",
      "\n",
      "Processing MFCC Optimized Augmented...\n",
      "Fitting GMM with 4 components...\n",
      "\n",
      "Cluster sizes:\n",
      "Cluster 0: 4819 sounds\n",
      "Cluster 1: 8181 sounds\n",
      "Cluster 2: 7375 sounds\n",
      "Cluster 3: 13909 sounds\n",
      "\n",
      "Average probability for each cluster:\n",
      "Cluster 0: 0.140\n",
      "Cluster 1: 0.237\n",
      "Cluster 2: 0.224\n",
      "Cluster 3: 0.399\n",
      "Internal Metrics:\n",
      "Silhouette Score: 0.072 (ranges from -1 to 1, higher is better)\n",
      "Davies-Bouldin Index: 2.591 (lower is better)\n",
      "Calinski-Harabasz Index: 3291.657 (higher is better)\n",
      "\n",
      "External Metrics:\n",
      "Adjusted Rand Index: 0.066 (ranges from -1 to 1, higher is better)\n",
      "Normalized Mutual Information: 0.098 (ranges from 0 to 1, higher is better)\n",
      "\n",
      "Cluster Composition:\n",
      "\n",
      "Cluster 0:\n",
      "hhc: 555 samples (11.5%), Avg Probability: 0.976\n",
      "hho: 414 samples (8.6%), Avg Probability: 0.941\n",
      "kd: 3387 samples (70.3%), Avg Probability: 0.973\n",
      "sd: 463 samples (9.6%), Avg Probability: 0.917\n",
      "\n",
      "Cluster 1:\n",
      "hhc: 3173 samples (38.8%), Avg Probability: 0.893\n",
      "hho: 3091 samples (37.8%), Avg Probability: 0.938\n",
      "kd: 571 samples (7.0%), Avg Probability: 0.875\n",
      "sd: 1346 samples (16.5%), Avg Probability: 0.885\n",
      "\n",
      "Cluster 2:\n",
      "hhc: 2414 samples (32.7%), Avg Probability: 0.907\n",
      "hho: 462 samples (6.3%), Avg Probability: 0.862\n",
      "kd: 2420 samples (32.8%), Avg Probability: 0.902\n",
      "sd: 2079 samples (28.2%), Avg Probability: 0.922\n",
      "\n",
      "Cluster 3:\n",
      "hhc: 2846 samples (20.5%), Avg Probability: 0.883\n",
      "hho: 2087 samples (15.0%), Avg Probability: 0.904\n",
      "kd: 4278 samples (30.8%), Avg Probability: 0.911\n",
      "sd: 4698 samples (33.8%), Avg Probability: 0.916\n"
     ]
    }
   ],
   "source": [
    "# Create necessary directories\n",
    "os.makedirs('../../cluster_assignments/gmm', exist_ok=True)\n",
    "os.makedirs('../../visualization/clustering_eval/gmm', exist_ok=True)\n",
    "\n",
    "# Run GMM clustering for each feature set\n",
    "feature_sets = [\n",
    "    {\n",
    "        'name': 'MFCC Basic',\n",
    "        'features': '../../extracted_features/features/mfcc_features.npy',\n",
    "        'labels': '../../extracted_features/labels/mfcc_labels.npy',\n",
    "        'output': '../../cluster_assignments/gmm/gmm_mfcc_cluster.npy',\n",
    "        'viz': '../../visualization/clustering_eval/gmm/gmm_mfcc.png'\n",
    "    },\n",
    "    {\n",
    "        'name': 'MFCC + Envelope',\n",
    "        'features': '../../extracted_features/features/mfcc_env_features.npy',\n",
    "        'labels': '../../extracted_features/labels/mfcc_env_labels.npy',\n",
    "        'output': '../../cluster_assignments/gmm/gmm_mfcc_env_cluster.npy',\n",
    "        'viz': '../../visualization/clustering_eval/gmm/gmm_mfcc_env.png'\n",
    "    },\n",
    "    {\n",
    "        'name': 'MFCC Optimized',\n",
    "        'features': '../../extracted_features/features/mfcc_extracted_features.npy',\n",
    "        'labels': '../../extracted_features/labels/mfcc_extracted_labels.npy',\n",
    "        'output': '../../cluster_assignments/gmm/gmm_mfcc_extracted_cluster.npy',\n",
    "        'viz': '../../visualization/clustering_eval/gmm/gmm_mfcc_extracted.png'\n",
    "    },\n",
    "    {\n",
    "        'name': 'MFCC Basic Augmented',\n",
    "        'features': '../../extracted_features/features/mfcc_features_aug.npy',\n",
    "        'labels': '../../extracted_features/labels/mfcc_labels_aug.npy',\n",
    "        'output': '../../cluster_assignments/gmm/gmm_mfcc_aug_cluster.npy',\n",
    "        'viz': '../../visualization/clustering_eval/gmm/gmm_mfcc_aug.png'\n",
    "    },\n",
    "    {\n",
    "        'name': 'MFCC + Envelope Augmented',\n",
    "        'features': '../../extracted_features/features/mfcc_env_aug_features.npy',\n",
    "        'labels': '../../extracted_features/labels/mfcc_env_aug_labels.npy',\n",
    "        'output': '../../cluster_assignments/gmm/gmm_mfcc_env_aug_cluster.npy',\n",
    "        'viz': '../../visualization/clustering_eval/gmm/gmm_mfcc_env_aug.png'\n",
    "    },\n",
    "    {\n",
    "        'name': 'MFCC Optimized Augmented',\n",
    "        'features': '../../extracted_features/features/mfcc_extracted_aug_features.npy',\n",
    "        'labels': '../../extracted_features/labels/mfcc_extracted_aug_labels.npy',\n",
    "        'output': '../../cluster_assignments/gmm/gmm_mfcc_extracted_aug_cluster.npy',\n",
    "        'viz': '../../visualization/clustering_eval/gmm/gmm_mfcc_extracted_aug.png'\n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "# Run clustering and evaluation for each feature set\n",
    "for feature_set in feature_sets:\n",
    "    print(f\"\\nProcessing {feature_set['name']}...\")\n",
    "    \n",
    "    # Run GMM clustering\n",
    "    cluster_labels, probabilities, gmm_model = gmm_cluster(\n",
    "        n_components=4,\n",
    "        features_path=feature_set['features'],\n",
    "        output_dir=feature_set['output']\n",
    "    )\n",
    "    \n",
    "    # Evaluate results\n",
    "    results = evaluate_gmm_clustering(\n",
    "        features_path=feature_set['features'],\n",
    "        labels_path=feature_set['labels'],\n",
    "        cluster_labels_path=feature_set['output'],\n",
    "        probabilities=probabilities,\n",
    "        viz_dir=feature_set['viz']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding CV and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing GMM for MFCC Basic...\n",
      "Performing cross-validation for different covariance types...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization Results (fixed 4 components):\n",
      "Optimal covariance type: tied\n",
      "\n",
      "Metrics with optimal covariance:\n",
      "Silhouette Score: 0.104\n",
      "Calinski-Harabasz Score: 107.277\n",
      "Adjusted Rand Index: 0.098\n",
      "Normalized Mutual Information: 0.127\n",
      "BIC Score: 40018.381\n",
      "\n",
      "Optimizing GMM for MFCC + Envelope...\n",
      "Performing cross-validation for different covariance types...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:04<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization Results (fixed 4 components):\n",
      "Optimal covariance type: tied\n",
      "\n",
      "Metrics with optimal covariance:\n",
      "Silhouette Score: 0.060\n",
      "Calinski-Harabasz Score: 60.748\n",
      "Adjusted Rand Index: 0.080\n",
      "Normalized Mutual Information: 0.112\n",
      "BIC Score: 96849.209\n",
      "\n",
      "Optimizing GMM for MFCC Optimized...\n",
      "Performing cross-validation for different covariance types...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization Results (fixed 4 components):\n",
      "Optimal covariance type: tied\n",
      "\n",
      "Metrics with optimal covariance:\n",
      "Silhouette Score: 0.121\n",
      "Calinski-Harabasz Score: 133.039\n",
      "Adjusted Rand Index: 0.080\n",
      "Normalized Mutual Information: 0.113\n",
      "BIC Score: 34789.077\n",
      "\n",
      "Optimizing GMM for MFCC Basic Augmented...\n",
      "Performing cross-validation for different covariance types...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:16<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization Results (fixed 4 components):\n",
      "Optimal covariance type: tied\n",
      "\n",
      "Metrics with optimal covariance:\n",
      "Silhouette Score: 0.116\n",
      "Calinski-Harabasz Score: 724.051\n",
      "Adjusted Rand Index: 0.073\n",
      "Normalized Mutual Information: 0.084\n",
      "BIC Score: 234604.425\n",
      "\n",
      "Optimizing GMM for MFCC + Envelope Augmented...\n",
      "Performing cross-validation for different covariance types...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:23<00:07,  7.90s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 146\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature_set \u001b[38;5;129;01min\u001b[39;00m feature_sets:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOptimizing GMM for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 146\u001b[0m     opt_covariance, results \u001b[38;5;241m=\u001b[39m optimize_gmm_fixed_components(\n\u001b[1;32m    147\u001b[0m         feature_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    148\u001b[0m         feature_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    149\u001b[0m     )\n\u001b[1;32m    150\u001b[0m     optimization_results[feature_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimal_covariance\u001b[39m\u001b[38;5;124m'\u001b[39m: opt_covariance,\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m: results\n\u001b[1;32m    153\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[5], line 56\u001b[0m, in \u001b[0;36moptimize_gmm_fixed_components\u001b[0;34m(features_path, labels_path, n_splits, n_components)\u001b[0m\n\u001b[1;32m     49\u001b[0m gmm \u001b[38;5;241m=\u001b[39m GaussianMixture(\n\u001b[1;32m     50\u001b[0m     n_components\u001b[38;5;241m=\u001b[39mn_components,\n\u001b[1;32m     51\u001b[0m     covariance_type\u001b[38;5;241m=\u001b[39mcov_type,\n\u001b[1;32m     52\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Fit and predict\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m gmm\u001b[38;5;241m.\u001b[39mfit(X_train)\n\u001b[1;32m     57\u001b[0m train_clusters \u001b[38;5;241m=\u001b[39m gmm\u001b[38;5;241m.\u001b[39mpredict(X_train)\n\u001b[1;32m     58\u001b[0m val_clusters \u001b[38;5;241m=\u001b[39m gmm\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/mixture/_base.py:181\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_predict(X, y)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/mixture/_base.py:247\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    244\u001b[0m prev_lower_bound \u001b[38;5;241m=\u001b[39m lower_bound\n\u001b[1;32m    246\u001b[0m log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e_step(X)\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_m_step(X, log_resp)\n\u001b[1;32m    248\u001b[0m lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_lower_bound(log_resp, log_prob_norm)\n\u001b[1;32m    250\u001b[0m change \u001b[38;5;241m=\u001b[39m lower_bound \u001b[38;5;241m-\u001b[39m prev_lower_bound\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/mixture/_gaussian_mixture.py:812\u001b[0m, in \u001b[0;36mGaussianMixture._m_step\u001b[0;34m(self, X, log_resp)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, log_resp):\n\u001b[1;32m    802\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"M step.\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \n\u001b[1;32m    804\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;124;03m        the point of each sample in X.\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 812\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariances_ \u001b[38;5;241m=\u001b[39m _estimate_gaussian_parameters(\n\u001b[1;32m    813\u001b[0m         X, np\u001b[38;5;241m.\u001b[39mexp(log_resp), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_covar, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_type\n\u001b[1;32m    814\u001b[0m     )\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_ \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecisions_cholesky_ \u001b[38;5;241m=\u001b[39m _compute_precision_cholesky(\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariances_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_type\n\u001b[1;32m    818\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/mixture/_gaussian_mixture.py:289\u001b[0m, in \u001b[0;36m_estimate_gaussian_parameters\u001b[0;34m(X, resp, reg_covar, covariance_type)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate the Gaussian distribution parameters.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    The shape depends of the covariance_type.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    288\u001b[0m nk \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(resp\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39meps\n\u001b[0;32m--> 289\u001b[0m means \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(resp\u001b[38;5;241m.\u001b[39mT, X) \u001b[38;5;241m/\u001b[39m nk[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m    290\u001b[0m covariances \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m: _estimate_gaussian_covariances_full,\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtied\u001b[39m\u001b[38;5;124m\"\u001b[39m: _estimate_gaussian_covariances_tied,\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiag\u001b[39m\u001b[38;5;124m\"\u001b[39m: _estimate_gaussian_covariances_diag,\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspherical\u001b[39m\u001b[38;5;124m\"\u001b[39m: _estimate_gaussian_covariances_spherical,\n\u001b[1;32m    295\u001b[0m }[covariance_type](resp, X, nk, means, reg_covar)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nk, means, covariances\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def optimize_gmm_fixed_components(features_path, labels_path, n_splits=5, n_components=4):\n",
    "    \"\"\"\n",
    "    Optimize GMM parameters using cross-validation, fixing number of components to 4\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    X = np.load(features_path)\n",
    "    y = np.load(labels_path)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Covariance types to try\n",
    "    covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'covariance_type': [],\n",
    "        'silhouette_scores': [],\n",
    "        'ari_scores': [],\n",
    "        'nmi_scores': [],\n",
    "        'calinski_scores': [],\n",
    "        'bic_scores': []\n",
    "    }\n",
    "    \n",
    "    print(\"Performing cross-validation for different covariance types...\")\n",
    "    \n",
    "    # Try different covariance types\n",
    "    for cov_type in tqdm(covariance_types):\n",
    "        fold_silhouette = []\n",
    "        fold_ari = []\n",
    "        fold_nmi = []\n",
    "        fold_calinski = []\n",
    "        fold_bic = []\n",
    "        \n",
    "        # Cross validation\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X_scaled)):\n",
    "            X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            # Train GMM\n",
    "            gmm = GaussianMixture(\n",
    "                n_components=n_components,\n",
    "                covariance_type=cov_type,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # Fit and predict\n",
    "            gmm.fit(X_train)\n",
    "            train_clusters = gmm.predict(X_train)\n",
    "            val_clusters = gmm.predict(X_val)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            fold_silhouette.append(silhouette_score(X_val, val_clusters))\n",
    "            fold_ari.append(adjusted_rand_score(y_val, val_clusters))\n",
    "            fold_nmi.append(normalized_mutual_info_score(y_val, val_clusters))\n",
    "            fold_calinski.append(calinski_harabasz_score(X_val, val_clusters))\n",
    "            fold_bic.append(gmm.bic(X_val))\n",
    "        \n",
    "        # Store average results\n",
    "        results['covariance_type'].append(cov_type)\n",
    "        results['silhouette_scores'].append(np.mean(fold_silhouette))\n",
    "        results['ari_scores'].append(np.mean(fold_ari))\n",
    "        results['nmi_scores'].append(np.mean(fold_nmi))\n",
    "        results['calinski_scores'].append(np.mean(fold_calinski))\n",
    "        results['bic_scores'].append(np.mean(fold_bic))\n",
    "    \n",
    "    # Find best configuration\n",
    "    # Weighted combination of metrics\n",
    "    metric_weights = {\n",
    "        'silhouette': 0.3,\n",
    "        'ari': 0.3,\n",
    "        'nmi': 0.2,\n",
    "        'calinski': 0.1,\n",
    "        'bic': 0.1\n",
    "    }\n",
    "    \n",
    "    combined_scores = (\n",
    "        metric_weights['silhouette'] * np.array(results['silhouette_scores']) +\n",
    "        metric_weights['ari'] * np.array(results['ari_scores']) +\n",
    "        metric_weights['nmi'] * np.array(results['nmi_scores']) +\n",
    "        metric_weights['calinski'] * (np.array(results['calinski_scores']) / np.max(results['calinski_scores'])) +\n",
    "        metric_weights['bic'] * (-np.array(results['bic_scores']) / np.max(-np.array(results['bic_scores'])))\n",
    "    )\n",
    "    \n",
    "    best_idx = np.argmax(combined_scores)\n",
    "    optimal_covariance = results['covariance_type'][best_idx]\n",
    "    \n",
    "    print(f\"\\nOptimization Results (fixed 4 components):\")\n",
    "    print(f\"Optimal covariance type: {optimal_covariance}\")\n",
    "    print(\"\\nMetrics with optimal covariance:\")\n",
    "    print(f\"Silhouette Score: {results['silhouette_scores'][best_idx]:.3f}\")\n",
    "    print(f\"Calinski-Harabasz Score: {results['calinski_scores'][best_idx]:.3f}\")\n",
    "    print(f\"Adjusted Rand Index: {results['ari_scores'][best_idx]:.3f}\")\n",
    "    print(f\"Normalized Mutual Information: {results['nmi_scores'][best_idx]:.3f}\")\n",
    "    print(f\"BIC Score: {results['bic_scores'][best_idx]:.3f}\")\n",
    "    \n",
    "    return optimal_covariance, results\n",
    "\n",
    "# Run optimization for each feature set\n",
    "feature_sets = [\n",
    "    {\n",
    "        'name': 'MFCC Basic',\n",
    "        'features': '../../extracted_features/features/mfcc_features.npy',\n",
    "        'labels': '../../extracted_features/labels/mfcc_labels.npy',\n",
    "    },\n",
    "    {\n",
    "        'name': 'MFCC + Envelope',\n",
    "        'features': '../../extracted_features/features/mfcc_env_features.npy',\n",
    "        'labels': '../../extracted_features/labels/mfcc_env_labels.npy',\n",
    "    },\n",
    "    {\n",
    "        'name': 'MFCC Optimized',\n",
    "        'features': '../../extracted_features/features/mfcc_extracted_features.npy',\n",
    "        'labels': '../../extracted_features/labels/mfcc_extracted_labels.npy',\n",
    "    },\n",
    "    {\n",
    "        'name': 'MFCC Basic Augmented',\n",
    "        'features': '../../extracted_features/features/mfcc_features_aug.npy',\n",
    "        'labels': '../../extracted_features/labels/mfcc_labels_aug.npy',\n",
    "    },\n",
    "    {\n",
    "        'name': 'MFCC + Envelope Augmented',\n",
    "        'features': '../../extracted_features/features/mfcc_env_aug_features.npy',\n",
    "        'labels': '../../extracted_features/labels/mfcc_env_aug_labels.npy',\n",
    "    },\n",
    "    {\n",
    "        'name': 'MFCC Optimized Augmented',\n",
    "        'features': '../../extracted_features/features/mfcc_extracted_aug_features.npy',\n",
    "        'labels': '../../extracted_features/labels/mfcc_extracted_aug_labels.npy',\n",
    "    }\n",
    "    \n",
    "]\n",
    "\n",
    "# Run optimization for each feature set\n",
    "optimization_results = {}\n",
    "for feature_set in feature_sets:\n",
    "    print(f\"\\nOptimizing GMM for {feature_set['name']}...\")\n",
    "    opt_covariance, results = optimize_gmm_fixed_components(\n",
    "        feature_set['features'],\n",
    "        feature_set['labels']\n",
    "    )\n",
    "    optimization_results[feature_set['name']] = {\n",
    "        'optimal_covariance': opt_covariance,\n",
    "        'results': results\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
