{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU\n",
      "\n",
      "Detailed Participant Analysis:\n",
      "AVP Dataset (28 participants):\n",
      "['P1', 'P10', 'P11', 'P12', 'P13', 'P14', 'P15', 'P16', 'P17', 'P18', 'P19', 'P2', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9']\n",
      "\n",
      "LVT Dataset (20 participants):\n",
      "AFR: IP\n",
      "AZi: IP\n",
      "Bea: IP\n",
      "Bic: IP\n",
      "Cat: IP\n",
      "Cav: IP\n",
      "Cra: IP\n",
      "Isa: IP\n",
      "JOl: IP\n",
      "JSi: IP\n",
      "JSo: IP\n",
      "MCo: IP\n",
      "Maf: IP\n",
      "Mar: IP\n",
      "Nor: IP\n",
      "Ric: IP\n",
      "Rob: IP\n",
      "Sof: IP\n",
      "Zga: IP\n",
      "Ziz: IP\n",
      "\n",
      "Total unique participants: 48\n",
      "- AVP Dataset: 28 participants\n",
      "- LVT Dataset: 20 participants\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/100:\n",
      "Train Loss: 1.002 | Train Acc: 87.372%\n",
      "Val Loss: 0.516 | Val Acc: 90.501%\n",
      "Epoch 2/100:\n",
      "Train Loss: 0.028 | Train Acc: 99.061%\n",
      "Val Loss: 0.746 | Val Acc: 92.260%\n",
      "Epoch 3/100:\n",
      "Train Loss: 0.002 | Train Acc: 100.000%\n",
      "Val Loss: 0.860 | Val Acc: 92.524%\n",
      "Epoch 4/100:\n",
      "Train Loss: 0.015 | Train Acc: 99.497%\n",
      "Val Loss: 0.804 | Val Acc: 86.368%\n",
      "Epoch 5/100:\n",
      "Train Loss: 0.075 | Train Acc: 97.881%\n",
      "Val Loss: 0.848 | Val Acc: 90.150%\n",
      "Epoch 6/100:\n",
      "Train Loss: 0.024 | Train Acc: 99.301%\n",
      "Val Loss: 0.842 | Val Acc: 92.348%\n",
      "Epoch 7/100:\n",
      "Train Loss: 0.012 | Train Acc: 99.629%\n",
      "Val Loss: 0.619 | Val Acc: 95.339%\n",
      "Epoch 8/100:\n",
      "Train Loss: 0.001 | Train Acc: 99.978%\n",
      "Val Loss: 0.953 | Val Acc: 93.140%\n",
      "Epoch 9/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 1.013 | Val Acc: 93.492%\n",
      "Epoch 10/100:\n",
      "Train Loss: 0.037 | Train Acc: 98.820%\n",
      "Val Loss: 1.427 | Val Acc: 93.404%\n",
      "Epoch 11/100:\n",
      "Train Loss: 0.027 | Train Acc: 99.235%\n",
      "Val Loss: 0.996 | Val Acc: 91.557%\n",
      "Epoch 12/100:\n",
      "Train Loss: 0.026 | Train Acc: 99.279%\n",
      "Val Loss: 1.073 | Val Acc: 92.260%\n",
      "Epoch 13/100:\n",
      "Train Loss: 0.029 | Train Acc: 99.279%\n",
      "Val Loss: 1.363 | Val Acc: 94.723%\n",
      "Epoch 14/100:\n",
      "Train Loss: 0.003 | Train Acc: 99.956%\n",
      "Val Loss: 1.328 | Val Acc: 94.635%\n",
      "Epoch 15/100:\n",
      "Train Loss: 0.002 | Train Acc: 99.913%\n",
      "Val Loss: 1.264 | Val Acc: 94.107%\n",
      "Epoch 16/100:\n",
      "Train Loss: 0.001 | Train Acc: 99.978%\n",
      "Val Loss: 1.553 | Val Acc: 93.228%\n",
      "Epoch 17/100:\n",
      "Train Loss: 0.002 | Train Acc: 99.956%\n",
      "Val Loss: 1.469 | Val Acc: 93.843%\n",
      "Early stopping triggered\n",
      "\n",
      "Fold 2/5\n",
      "Epoch 1/100:\n",
      "Train Loss: 1.286 | Train Acc: 79.987%\n",
      "Val Loss: 0.087 | Val Acc: 96.829%\n",
      "Epoch 2/100:\n",
      "Train Loss: 0.062 | Train Acc: 98.175%\n",
      "Val Loss: 0.128 | Val Acc: 96.915%\n",
      "Epoch 3/100:\n",
      "Train Loss: 0.018 | Train Acc: 99.538%\n",
      "Val Loss: 0.060 | Val Acc: 96.915%\n",
      "Epoch 4/100:\n",
      "Train Loss: 0.084 | Train Acc: 97.031%\n",
      "Val Loss: 0.067 | Val Acc: 97.858%\n",
      "Epoch 5/100:\n",
      "Train Loss: 0.006 | Train Acc: 99.758%\n",
      "Val Loss: 0.029 | Val Acc: 98.458%\n",
      "Epoch 6/100:\n",
      "Train Loss: 0.032 | Train Acc: 99.296%\n",
      "Val Loss: 0.244 | Val Acc: 94.173%\n",
      "Epoch 7/100:\n",
      "Train Loss: 0.046 | Train Acc: 98.505%\n",
      "Val Loss: 0.049 | Val Acc: 97.515%\n",
      "Epoch 8/100:\n",
      "Train Loss: 0.013 | Train Acc: 99.648%\n",
      "Val Loss: 0.117 | Val Acc: 97.515%\n",
      "Epoch 9/100:\n",
      "Train Loss: 0.005 | Train Acc: 99.780%\n",
      "Val Loss: 0.040 | Val Acc: 98.115%\n",
      "Epoch 10/100:\n",
      "Train Loss: 0.001 | Train Acc: 99.956%\n",
      "Val Loss: 0.115 | Val Acc: 97.515%\n",
      "Epoch 11/100:\n",
      "Train Loss: 0.014 | Train Acc: 99.538%\n",
      "Val Loss: 0.049 | Val Acc: 97.858%\n",
      "Epoch 12/100:\n",
      "Train Loss: 0.003 | Train Acc: 99.934%\n",
      "Val Loss: 0.063 | Val Acc: 97.686%\n",
      "Epoch 13/100:\n",
      "Train Loss: 0.002 | Train Acc: 99.934%\n",
      "Val Loss: 0.077 | Val Acc: 97.772%\n",
      "Epoch 14/100:\n",
      "Train Loss: 0.001 | Train Acc: 99.978%\n",
      "Val Loss: 0.065 | Val Acc: 97.601%\n",
      "Epoch 15/100:\n",
      "Train Loss: 0.002 | Train Acc: 99.934%\n",
      "Val Loss: 0.077 | Val Acc: 97.686%\n",
      "Early stopping triggered\n",
      "\n",
      "Fold 3/5\n",
      "Epoch 1/100:\n",
      "Train Loss: 1.076 | Train Acc: 83.574%\n",
      "Val Loss: 0.046 | Val Acc: 99.296%\n",
      "Epoch 2/100:\n",
      "Train Loss: 0.070 | Train Acc: 98.012%\n",
      "Val Loss: 0.003 | Val Acc: 100.000%\n",
      "Epoch 3/100:\n",
      "Train Loss: 0.029 | Train Acc: 98.952%\n",
      "Val Loss: 0.001 | Val Acc: 100.000%\n",
      "Epoch 4/100:\n",
      "Train Loss: 0.035 | Train Acc: 98.733%\n",
      "Val Loss: 0.002 | Val Acc: 99.912%\n",
      "Epoch 5/100:\n",
      "Train Loss: 0.032 | Train Acc: 98.930%\n",
      "Val Loss: 0.005 | Val Acc: 100.000%\n",
      "Epoch 6/100:\n",
      "Train Loss: 0.004 | Train Acc: 99.956%\n",
      "Val Loss: 0.001 | Val Acc: 100.000%\n",
      "Epoch 7/100:\n",
      "Train Loss: 0.051 | Train Acc: 98.318%\n",
      "Val Loss: 0.055 | Val Acc: 97.975%\n",
      "Epoch 8/100:\n",
      "Train Loss: 0.055 | Train Acc: 98.493%\n",
      "Val Loss: 0.002 | Val Acc: 99.912%\n",
      "Epoch 9/100:\n",
      "Train Loss: 0.004 | Train Acc: 99.891%\n",
      "Val Loss: 0.000 | Val Acc: 100.000%\n",
      "Epoch 10/100:\n",
      "Train Loss: 0.002 | Train Acc: 99.934%\n",
      "Val Loss: 0.000 | Val Acc: 100.000%\n",
      "Epoch 11/100:\n",
      "Train Loss: 0.003 | Train Acc: 99.934%\n",
      "Val Loss: 0.001 | Val Acc: 100.000%\n",
      "Epoch 12/100:\n",
      "Train Loss: 0.002 | Train Acc: 99.978%\n",
      "Val Loss: 0.000 | Val Acc: 100.000%\n",
      "Early stopping triggered\n",
      "\n",
      "Fold 4/5\n",
      "Epoch 1/100:\n",
      "Train Loss: 0.994 | Train Acc: 84.357%\n",
      "Val Loss: 0.304 | Val Acc: 91.733%\n",
      "Epoch 2/100:\n",
      "Train Loss: 0.061 | Train Acc: 97.859%\n",
      "Val Loss: 0.083 | Val Acc: 97.186%\n",
      "Epoch 3/100:\n",
      "Train Loss: 0.037 | Train Acc: 98.798%\n",
      "Val Loss: 0.152 | Val Acc: 95.251%\n",
      "Epoch 4/100:\n",
      "Train Loss: 0.017 | Train Acc: 99.563%\n",
      "Val Loss: 0.189 | Val Acc: 96.394%\n",
      "Epoch 5/100:\n",
      "Train Loss: 0.018 | Train Acc: 99.301%\n",
      "Val Loss: 0.155 | Val Acc: 94.635%\n",
      "Epoch 6/100:\n",
      "Train Loss: 0.034 | Train Acc: 99.082%\n",
      "Val Loss: 0.386 | Val Acc: 93.140%\n",
      "Epoch 7/100:\n",
      "Train Loss: 0.028 | Train Acc: 98.908%\n",
      "Val Loss: 0.119 | Val Acc: 96.570%\n",
      "Epoch 8/100:\n",
      "Train Loss: 0.019 | Train Acc: 99.432%\n",
      "Val Loss: 0.132 | Val Acc: 96.746%\n",
      "Epoch 9/100:\n",
      "Train Loss: 0.004 | Train Acc: 99.978%\n",
      "Val Loss: 0.078 | Val Acc: 97.977%\n",
      "Epoch 10/100:\n",
      "Train Loss: 0.011 | Train Acc: 99.738%\n",
      "Val Loss: 0.125 | Val Acc: 96.834%\n",
      "Epoch 11/100:\n",
      "Train Loss: 0.002 | Train Acc: 99.913%\n",
      "Val Loss: 0.055 | Val Acc: 98.241%\n",
      "Epoch 12/100:\n",
      "Train Loss: 0.001 | Train Acc: 100.000%\n",
      "Val Loss: 0.048 | Val Acc: 98.681%\n",
      "Epoch 13/100:\n",
      "Train Loss: 0.001 | Train Acc: 99.978%\n",
      "Val Loss: 0.051 | Val Acc: 98.329%\n",
      "Epoch 14/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.050 | Val Acc: 98.329%\n",
      "Epoch 15/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.049 | Val Acc: 98.241%\n",
      "Epoch 16/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.057 | Val Acc: 98.153%\n",
      "Epoch 17/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.057 | Val Acc: 98.065%\n",
      "Epoch 18/100:\n",
      "Train Loss: 0.001 | Train Acc: 99.978%\n",
      "Val Loss: 0.046 | Val Acc: 98.505%\n",
      "Epoch 19/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.049 | Val Acc: 98.505%\n",
      "Epoch 20/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.059 | Val Acc: 98.065%\n",
      "Epoch 21/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.045 | Val Acc: 98.593%\n",
      "Epoch 22/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.047 | Val Acc: 98.769%\n",
      "Epoch 23/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.043 | Val Acc: 98.769%\n",
      "Epoch 24/100:\n",
      "Train Loss: 0.000 | Train Acc: 99.978%\n",
      "Val Loss: 0.043 | Val Acc: 99.033%\n",
      "Epoch 25/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.045 | Val Acc: 98.593%\n",
      "Epoch 26/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.055 | Val Acc: 98.065%\n",
      "Epoch 27/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.047 | Val Acc: 98.505%\n",
      "Epoch 28/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.056 | Val Acc: 98.153%\n",
      "Epoch 29/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.046 | Val Acc: 98.681%\n",
      "Epoch 30/100:\n",
      "Train Loss: 0.003 | Train Acc: 100.000%\n",
      "Val Loss: 0.070 | Val Acc: 97.889%\n",
      "Epoch 31/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.046 | Val Acc: 98.505%\n",
      "Epoch 32/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.044 | Val Acc: 98.593%\n",
      "Epoch 33/100:\n",
      "Train Loss: 0.000 | Train Acc: 100.000%\n",
      "Val Loss: 0.051 | Val Acc: 98.065%\n",
      "Epoch 34/100:\n",
      "Train Loss: 0.001 | Train Acc: 99.978%\n",
      "Val Loss: 0.059 | Val Acc: 98.065%\n",
      "Early stopping triggered\n",
      "\n",
      "Fold 5/5\n",
      "Epoch 1/100:\n",
      "Train Loss: 1.241 | Train Acc: 81.101%\n",
      "Val Loss: 0.039 | Val Acc: 99.560%\n",
      "Epoch 2/100:\n",
      "Train Loss: 0.082 | Train Acc: 97.728%\n",
      "Val Loss: 0.023 | Val Acc: 99.384%\n",
      "Epoch 3/100:\n",
      "Train Loss: 0.042 | Train Acc: 98.776%\n",
      "Val Loss: 0.018 | Val Acc: 99.472%\n",
      "Epoch 4/100:\n",
      "Train Loss: 0.024 | Train Acc: 99.257%\n",
      "Val Loss: 0.005 | Val Acc: 100.000%\n",
      "Epoch 5/100:\n",
      "Train Loss: 0.033 | Train Acc: 99.017%\n",
      "Val Loss: 0.028 | Val Acc: 98.857%\n",
      "Epoch 6/100:\n",
      "Train Loss: 0.022 | Train Acc: 99.235%\n",
      "Val Loss: 0.004 | Val Acc: 100.000%\n",
      "Epoch 7/100:\n",
      "Train Loss: 0.019 | Train Acc: 99.345%\n",
      "Val Loss: 0.000 | Val Acc: 100.000%\n",
      "Epoch 8/100:\n",
      "Train Loss: 0.010 | Train Acc: 99.563%\n",
      "Val Loss: 0.000 | Val Acc: 100.000%\n",
      "Epoch 9/100:\n",
      "Train Loss: 0.008 | Train Acc: 99.629%\n",
      "Val Loss: 0.014 | Val Acc: 99.472%\n",
      "Epoch 10/100:\n",
      "Train Loss: 0.036 | Train Acc: 98.908%\n",
      "Val Loss: 0.000 | Val Acc: 100.000%\n",
      "Epoch 11/100:\n",
      "Train Loss: 0.006 | Train Acc: 99.803%\n",
      "Val Loss: 0.000 | Val Acc: 100.000%\n",
      "Epoch 12/100:\n",
      "Train Loss: 0.004 | Train Acc: 99.891%\n",
      "Val Loss: 0.001 | Val Acc: 100.000%\n",
      "Epoch 13/100:\n",
      "Train Loss: 0.002 | Train Acc: 99.913%\n",
      "Val Loss: 0.000 | Val Acc: 100.000%\n",
      "Epoch 14/100:\n",
      "Train Loss: 0.002 | Train Acc: 99.934%\n",
      "Val Loss: 0.001 | Val Acc: 100.000%\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 363\u001b[39m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMean participant-independent accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.mean(participant_accuracies)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 333\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Prepare for k-NN evaluation\u001b[39;00m\n\u001b[32m    332\u001b[39m embeddings = np.concatenate(all_embeddings)\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m labels = np.array(all_labels)\n\u001b[32m    334\u001b[39m participants = np.array(all_participants)\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# Scale embeddings (as per paper)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import os\n",
    "from pathlib import Path\n",
    "import platform\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, segment_info_path, transform=None, train=True):\n",
    "        self.df = pd.read_csv(segment_info_path)\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        \n",
    "        # Standardize JSo/JoS naming\n",
    "        self.df['participant_id'] = self.df['participant_id'].replace('JoSP', 'JSoP')\n",
    "        \n",
    "        self.label_to_id = {'hhc': 0, 'hho': 1, 'kd': 2, 'sd': 3}\n",
    "        \n",
    "        # Parameters for mel spectrogram (as per paper)\n",
    "        self.sample_rate = 16000\n",
    "        self.n_mels = 128\n",
    "        self.n_fft = 1024\n",
    "        self.hop_length = 512\n",
    "        \n",
    "        # Parameters for augmentation (10x as per paper)\n",
    "        self.pitch_shifts = [-2, -1, 0, 1, 2]  # 5 pitch shifts\n",
    "        self.time_stretches = [0.9, 0.95, 1.0, 1.05, 1.1]  # 5 time stretches\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.df) * len(self.pitch_shifts) * len(self.time_stretches)\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _load_and_process_audio(self, audio_path, pitch_shift=0, time_stretch=1.0):\n",
    "        \"\"\"Load and process audio following paper's normalization\"\"\"\n",
    "        # Fix path if needed\n",
    "        if audio_path.startswith('../'):\n",
    "            audio_path = f\"../{audio_path}\"\n",
    "            \n",
    "        # Load audio\n",
    "        y, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if time_stretch != 1.0:\n",
    "            y = librosa.effects.time_stretch(y, rate=time_stretch)\n",
    "        if pitch_shift != 0:\n",
    "            y = librosa.effects.pitch_shift(y, sr=sr, n_steps=pitch_shift)\n",
    "            \n",
    "        # Compute mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=y, sr=sr, n_mels=self.n_mels, n_fft=self.n_fft, hop_length=self.hop_length\n",
    "        )\n",
    "        \n",
    "        # Convert to log scale\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "        \n",
    "        # Min-max normalize each patch to [0, 1] as per paper\n",
    "        log_mel_spec = (log_mel_spec - log_mel_spec.min()) / (log_mel_spec.max() - log_mel_spec.min())\n",
    "        \n",
    "        # Convert to tensor and ensure fixed size (128 x 64)\n",
    "        spec_tensor = torch.FloatTensor(log_mel_spec)\n",
    "        target_length = 64\n",
    "        current_length = spec_tensor.size(1)\n",
    "        \n",
    "        if current_length < target_length:\n",
    "            pad_amount = target_length - current_length\n",
    "            spec_tensor = torch.nn.functional.pad(spec_tensor, (0, pad_amount))\n",
    "        elif current_length > target_length:\n",
    "            start = (current_length - target_length) // 2\n",
    "            spec_tensor = spec_tensor[:, start:start + target_length]\n",
    "        \n",
    "        return spec_tensor\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            n_augs = len(self.pitch_shifts) * len(self.time_stretches)\n",
    "            orig_idx = idx // n_augs\n",
    "            aug_idx = idx % n_augs\n",
    "            pitch_idx = aug_idx // len(self.time_stretches)\n",
    "            time_idx = aug_idx % len(self.time_stretches)\n",
    "            \n",
    "            pitch_shift = self.pitch_shifts[pitch_idx]\n",
    "            time_stretch = self.time_stretches[time_idx]\n",
    "        else:\n",
    "            orig_idx = idx\n",
    "            pitch_shift = 0\n",
    "            time_stretch = 1.0\n",
    "            \n",
    "        row = self.df.iloc[orig_idx]\n",
    "        audio_path = row['segment_path']\n",
    "        label = self.label_to_id[row['instrument_label']]\n",
    "        participant = row['participant_id']\n",
    "        \n",
    "        spec = self._load_and_process_audio(audio_path, pitch_shift, time_stretch)\n",
    "        return spec, label, participant\n",
    "\n",
    "class DrumCNN(nn.Module):\n",
    "    def __init__(self, num_classes=4, embedding_dim=1024):\n",
    "        super(DrumCNN, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 64 x 64 x 32\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 128 x 32 x 16\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 256 x 16 x 8\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)   # 512 x 8 x 4\n",
    "        )\n",
    "        \n",
    "        self.flatten_size = 512 * 8 * 4\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(self.flatten_size, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x, return_embedding=False):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        embedding = self.embedding(x)\n",
    "        \n",
    "        if return_embedding:\n",
    "            return embedding\n",
    "            \n",
    "        return self.classifier(embedding)\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Check for GPU availability including 4090\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            if \"4090\" in gpu_name:\n",
    "                print(f\"Found NVIDIA RTX 4090: {gpu_name}\")\n",
    "                return torch.device(f\"cuda:{i}\")\n",
    "        print(f\"Using available GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        return torch.device(\"cuda:0\")\n",
    "    elif platform.system() == \"Darwin\":  # macOS\n",
    "        if torch.backends.mps.is_available():\n",
    "            print(\"Using Apple Silicon GPU\")\n",
    "            return torch.device(\"mps\")\n",
    "    print(\"Using CPU\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def train_model(train_loader, val_loader, model, device, num_epochs=100):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    patience = 10  # Early stopping as per paper\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for specs, labels, _ in train_loader:\n",
    "            specs = specs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(specs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_acc = 100. * correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for specs, labels, _ in val_loader:\n",
    "                specs = specs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(specs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_acc = 100. * correct / total\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss/len(train_loader):.3f} | Train Acc: {train_acc:.3f}%')\n",
    "        print(f'Val Loss: {val_loss/len(val_loader):.3f} | Val Acc: {val_acc:.3f}%')\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "def analyze_participants(df):\n",
    "    \"\"\"Detailed analysis of participants in both datasets\"\"\"\n",
    "    # AVP participants (P1, P2, etc.)\n",
    "    avp_participants = sorted([p for p in df['participant_id'].unique() if p.startswith('P')])\n",
    "    \n",
    "    # LVT participants (handling I/P pairs)\n",
    "    lvt_ids = [p for p in df['participant_id'].unique() if not p.startswith('P')]\n",
    "    lvt_base_names = sorted(list(set([p[:-1] for p in lvt_ids if p not in ['JoSP']])))\n",
    "    \n",
    "    print(\"\\nDetailed Participant Analysis:\")\n",
    "    print(f\"AVP Dataset ({len(avp_participants)} participants):\")\n",
    "    print(avp_participants)\n",
    "    \n",
    "    print(f\"\\nLVT Dataset ({len(lvt_base_names)} participants):\")\n",
    "    for base_name in lvt_base_names:\n",
    "        has_I = f\"{base_name}I\" in lvt_ids\n",
    "        has_P = f\"{base_name}P\" in lvt_ids or (base_name == 'JSo' and 'JoSP' in lvt_ids)\n",
    "        print(f\"{base_name}: {'I' if has_I else '-'}{'P' if has_P else '-'}\")\n",
    "    \n",
    "    total_participants = len(avp_participants) + len(lvt_base_names)\n",
    "    print(f\"\\nTotal unique participants: {total_participants}\")\n",
    "    print(f\"- AVP Dataset: {len(avp_participants)} participants\")\n",
    "    print(f\"- LVT Dataset: {len(lvt_base_names)} participants\")\n",
    "    \n",
    "    return avp_participants, lvt_base_names\n",
    "\n",
    "def main():\n",
    "    # Set device and random seed\n",
    "    device = get_device()\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = AudioDataset(\n",
    "        segment_info_path='../../segment_info/segment_info.csv',\n",
    "        train=True\n",
    "    )\n",
    "    \n",
    "    # Analyze participants\n",
    "    avp_participants, lvt_base_names = analyze_participants(dataset.df)\n",
    "    \n",
    "    # Create participant groups for splitting\n",
    "    def get_participant_group(participant_id):\n",
    "        \"\"\"Convert participant ID to a group number, handling the JSo/JoS case\"\"\"\n",
    "        if participant_id.startswith('P'):\n",
    "            return int(participant_id[1:])\n",
    "        if participant_id == 'JoSP':\n",
    "            return hash('JSo')\n",
    "        return hash(participant_id[:-1])\n",
    "    \n",
    "    groups = dataset.df['participant_id'].apply(get_participant_group).values\n",
    "    \n",
    "    # Cross-validation setup\n",
    "    n_splits = 5\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "    # Storage for embeddings and labels\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    all_participants = []\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X=np.zeros(len(dataset.df)), groups=groups)):\n",
    "        print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "        \n",
    "        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=0)\n",
    "        \n",
    "        model = DrumCNN().to(device)\n",
    "        train_model(train_loader, val_loader, model, device)\n",
    "        \n",
    "        # Extract embeddings\n",
    "        model.eval()\n",
    "        fold_embeddings = []\n",
    "        fold_labels = []\n",
    "        fold_participants = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for specs, labels, parts in val_loader:\n",
    "                specs = specs.to(device)\n",
    "                embeddings = model(specs, return_embedding=True)\n",
    "                \n",
    "                fold_embeddings.append(embeddings.cpu().numpy())\n",
    "                fold_labels.extend(labels.numpy())  # Changed from append to extend\n",
    "                fold_participants.extend(parts)\n",
    "        \n",
    "        all_embeddings.append(np.concatenate(fold_embeddings))\n",
    "        all_labels.extend(fold_labels)  # Changed from append to extend\n",
    "        all_participants.extend(fold_participants)\n",
    "    \n",
    "    # Prepare for k-NN evaluation\n",
    "    embeddings = np.concatenate(all_embeddings)\n",
    "    labels = np.array(all_labels)  # Now this should work\n",
    "    participants = np.array(all_participants)\n",
    "    \n",
    "    print(\"\\nFinal dataset sizes:\")\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Participants shape: {participants.shape}\")\n",
    "    \n",
    "    # Scale embeddings (as per paper)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "    \n",
    "    # Evaluate using leave-one-participant-out\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "    unique_participants = np.unique(participants)\n",
    "    participant_accuracies = []\n",
    "    \n",
    "    print(\"\\nParticipant-wise evaluation:\")\n",
    "    for test_participant in unique_participants:\n",
    "        train_mask = participants != test_participant\n",
    "        test_mask = participants == test_participant\n",
    "        \n",
    "        # Scale using only training data\n",
    "        train_embeddings = scaled_embeddings[train_mask]\n",
    "        test_embeddings = scaled_embeddings[test_mask]\n",
    "        \n",
    "        knn.fit(train_embeddings, labels[train_mask])\n",
    "        accuracy = knn.score(test_embeddings, labels[test_mask])\n",
    "        participant_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"Participant {test_participant}: {accuracy:.3f}\")\n",
    "    \n",
    "    print(f\"\\nMean participant-independent accuracy: {np.mean(participant_accuracies):.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beatbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
