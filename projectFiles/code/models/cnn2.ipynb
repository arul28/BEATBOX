{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU\n",
      "\n",
      "Detailed Participant Analysis:\n",
      "AVP Dataset (28 participants):\n",
      "['P1', 'P10', 'P11', 'P12', 'P13', 'P14', 'P15', 'P16', 'P17', 'P18', 'P19', 'P2', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26', 'P27', 'P28', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9']\n",
      "\n",
      "LVT Dataset (20 participants):\n",
      "AFR: IP\n",
      "AZi: IP\n",
      "Bea: IP\n",
      "Bic: IP\n",
      "Cat: IP\n",
      "Cav: IP\n",
      "Cra: IP\n",
      "Isa: IP\n",
      "JOl: IP\n",
      "JSi: IP\n",
      "JSo: IP\n",
      "MCo: IP\n",
      "Maf: IP\n",
      "Mar: IP\n",
      "Nor: IP\n",
      "Ric: IP\n",
      "Rob: IP\n",
      "Sof: IP\n",
      "Zga: IP\n",
      "Ziz: IP\n",
      "\n",
      "Total unique participants: 48\n",
      "- AVP Dataset: 28 participants\n",
      "- LVT Dataset: 20 participants\n",
      "\n",
      "Fold 1/5\n",
      "Epoch 1/100:\n",
      "Train Loss: 1.002 | Train Acc: 87.372%\n",
      "Val Loss: 0.516 | Val Acc: 90.501%\n",
      "Epoch 2/100:\n",
      "Train Loss: 0.028 | Train Acc: 99.061%\n",
      "Val Loss: 0.746 | Val Acc: 92.260%\n",
      "Epoch 3/100:\n",
      "Train Loss: 0.002 | Train Acc: 100.000%\n",
      "Val Loss: 0.860 | Val Acc: 92.524%\n",
      "Epoch 4/100:\n",
      "Train Loss: 0.015 | Train Acc: 99.497%\n",
      "Val Loss: 0.804 | Val Acc: 86.368%\n",
      "Epoch 5/100:\n",
      "Train Loss: 0.075 | Train Acc: 97.881%\n",
      "Val Loss: 0.848 | Val Acc: 90.150%\n",
      "Epoch 6/100:\n",
      "Train Loss: 0.024 | Train Acc: 99.301%\n",
      "Val Loss: 0.842 | Val Acc: 92.348%\n",
      "Epoch 7/100:\n",
      "Train Loss: 0.012 | Train Acc: 99.629%\n",
      "Val Loss: 0.619 | Val Acc: 95.339%\n",
      "Epoch 8/100:\n",
      "Train Loss: 0.001 | Train Acc: 99.978%\n",
      "Val Loss: 0.953 | Val Acc: 93.140%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import os\n",
    "from pathlib import Path\n",
    "import platform\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, segment_info_path, transform=None, train=True):\n",
    "        self.df = pd.read_csv(segment_info_path)\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        \n",
    "        # Standardize JSo/JoS naming\n",
    "        self.df['participant_id'] = self.df['participant_id'].replace('JoSP', 'JSoP')\n",
    "        \n",
    "        self.label_to_id = {'hhc': 0, 'hho': 1, 'kd': 2, 'sd': 3}\n",
    "        \n",
    "        # Parameters for mel spectrogram (as per paper)\n",
    "        self.sample_rate = 16000\n",
    "        self.n_mels = 128\n",
    "        self.n_fft = 1024\n",
    "        self.hop_length = 512\n",
    "        \n",
    "        # Parameters for augmentation (10x as per paper)\n",
    "        self.pitch_shifts = [-2, -1, 0, 1, 2]  # 5 pitch shifts\n",
    "        self.time_stretches = [0.9, 0.95, 1.0, 1.05, 1.1]  # 5 time stretches\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.df) * len(self.pitch_shifts) * len(self.time_stretches)\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _load_and_process_audio(self, audio_path, pitch_shift=0, time_stretch=1.0):\n",
    "        \"\"\"Load and process audio following paper's normalization\"\"\"\n",
    "        # Fix path if needed\n",
    "        if audio_path.startswith('../'):\n",
    "            audio_path = f\"../{audio_path}\"\n",
    "            \n",
    "        # Load audio\n",
    "        y, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if time_stretch != 1.0:\n",
    "            y = librosa.effects.time_stretch(y, rate=time_stretch)\n",
    "        if pitch_shift != 0:\n",
    "            y = librosa.effects.pitch_shift(y, sr=sr, n_steps=pitch_shift)\n",
    "            \n",
    "        # Compute mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=y, sr=sr, n_mels=self.n_mels, n_fft=self.n_fft, hop_length=self.hop_length\n",
    "        )\n",
    "        \n",
    "        # Convert to log scale\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "        \n",
    "        # Min-max normalize each patch to [0, 1] as per paper\n",
    "        log_mel_spec = (log_mel_spec - log_mel_spec.min()) / (log_mel_spec.max() - log_mel_spec.min())\n",
    "        \n",
    "        # Convert to tensor and ensure fixed size (128 x 64)\n",
    "        spec_tensor = torch.FloatTensor(log_mel_spec)\n",
    "        target_length = 64\n",
    "        current_length = spec_tensor.size(1)\n",
    "        \n",
    "        if current_length < target_length:\n",
    "            pad_amount = target_length - current_length\n",
    "            spec_tensor = torch.nn.functional.pad(spec_tensor, (0, pad_amount))\n",
    "        elif current_length > target_length:\n",
    "            start = (current_length - target_length) // 2\n",
    "            spec_tensor = spec_tensor[:, start:start + target_length]\n",
    "        \n",
    "        return spec_tensor\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            n_augs = len(self.pitch_shifts) * len(self.time_stretches)\n",
    "            orig_idx = idx // n_augs\n",
    "            aug_idx = idx % n_augs\n",
    "            pitch_idx = aug_idx // len(self.time_stretches)\n",
    "            time_idx = aug_idx % len(self.time_stretches)\n",
    "            \n",
    "            pitch_shift = self.pitch_shifts[pitch_idx]\n",
    "            time_stretch = self.time_stretches[time_idx]\n",
    "        else:\n",
    "            orig_idx = idx\n",
    "            pitch_shift = 0\n",
    "            time_stretch = 1.0\n",
    "            \n",
    "        row = self.df.iloc[orig_idx]\n",
    "        audio_path = row['segment_path']\n",
    "        label = self.label_to_id[row['instrument_label']]\n",
    "        participant = row['participant_id']\n",
    "        \n",
    "        spec = self._load_and_process_audio(audio_path, pitch_shift, time_stretch)\n",
    "        return spec, label, participant\n",
    "\n",
    "class DrumCNN(nn.Module):\n",
    "    def __init__(self, num_classes=4, embedding_dim=1024):\n",
    "        super(DrumCNN, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 64 x 64 x 32\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 128 x 32 x 16\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 256 x 16 x 8\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)   # 512 x 8 x 4\n",
    "        )\n",
    "        \n",
    "        self.flatten_size = 512 * 8 * 4\n",
    "        \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(self.flatten_size, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x, return_embedding=False):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        embedding = self.embedding(x)\n",
    "        \n",
    "        if return_embedding:\n",
    "            return embedding\n",
    "            \n",
    "        return self.classifier(embedding)\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Check for GPU availability including 4090\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            if \"4090\" in gpu_name:\n",
    "                print(f\"Found NVIDIA RTX 4090: {gpu_name}\")\n",
    "                return torch.device(f\"cuda:{i}\")\n",
    "        print(f\"Using available GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        return torch.device(\"cuda:0\")\n",
    "    elif platform.system() == \"Darwin\":  # macOS\n",
    "        if torch.backends.mps.is_available():\n",
    "            print(\"Using Apple Silicon GPU\")\n",
    "            return torch.device(\"mps\")\n",
    "    print(\"Using CPU\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def train_model(train_loader, val_loader, model, device, num_epochs=100):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    patience = 10  # Early stopping as per paper\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for specs, labels, _ in train_loader:\n",
    "            specs = specs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(specs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_acc = 100. * correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for specs, labels, _ in val_loader:\n",
    "                specs = specs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(specs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_acc = 100. * correct / total\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss/len(train_loader):.3f} | Train Acc: {train_acc:.3f}%')\n",
    "        print(f'Val Loss: {val_loss/len(val_loader):.3f} | Val Acc: {val_acc:.3f}%')\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "def analyze_participants(df):\n",
    "    \"\"\"Detailed analysis of participants in both datasets\"\"\"\n",
    "    # AVP participants (P1, P2, etc.)\n",
    "    avp_participants = sorted([p for p in df['participant_id'].unique() if p.startswith('P')])\n",
    "    \n",
    "    # LVT participants (handling I/P pairs)\n",
    "    lvt_ids = [p for p in df['participant_id'].unique() if not p.startswith('P')]\n",
    "    lvt_base_names = sorted(list(set([p[:-1] for p in lvt_ids if p not in ['JoSP']])))\n",
    "    \n",
    "    print(\"\\nDetailed Participant Analysis:\")\n",
    "    print(f\"AVP Dataset ({len(avp_participants)} participants):\")\n",
    "    print(avp_participants)\n",
    "    \n",
    "    print(f\"\\nLVT Dataset ({len(lvt_base_names)} participants):\")\n",
    "    for base_name in lvt_base_names:\n",
    "        has_I = f\"{base_name}I\" in lvt_ids\n",
    "        has_P = f\"{base_name}P\" in lvt_ids or (base_name == 'JSo' and 'JoSP' in lvt_ids)\n",
    "        print(f\"{base_name}: {'I' if has_I else '-'}{'P' if has_P else '-'}\")\n",
    "    \n",
    "    total_participants = len(avp_participants) + len(lvt_base_names)\n",
    "    print(f\"\\nTotal unique participants: {total_participants}\")\n",
    "    print(f\"- AVP Dataset: {len(avp_participants)} participants\")\n",
    "    print(f\"- LVT Dataset: {len(lvt_base_names)} participants\")\n",
    "    \n",
    "    return avp_participants, lvt_base_names\n",
    "\n",
    "def main():\n",
    "    # Set device and random seed\n",
    "    device = get_device()\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = AudioDataset(\n",
    "        segment_info_path='../../segment_info/segment_info.csv',\n",
    "        train=True\n",
    "    )\n",
    "    \n",
    "    # Analyze participants\n",
    "    avp_participants, lvt_base_names = analyze_participants(dataset.df)\n",
    "    \n",
    "    # Create participant groups for splitting\n",
    "    def get_participant_group(participant_id):\n",
    "        \"\"\"Convert participant ID to a group number, handling the JSo/JoS case\"\"\"\n",
    "        if participant_id.startswith('P'):\n",
    "            return int(participant_id[1:])\n",
    "        if participant_id == 'JoSP':\n",
    "            return hash('JSo')\n",
    "        return hash(participant_id[:-1])\n",
    "    \n",
    "    groups = dataset.df['participant_id'].apply(get_participant_group).values\n",
    "    \n",
    "    # Cross-validation setup\n",
    "    n_splits = 5\n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "    # Storage for embeddings and labels\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    all_participants = []\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X=np.zeros(len(dataset.df)), groups=groups)):\n",
    "        print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "        \n",
    "        train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, num_workers=0)\n",
    "        \n",
    "        model = DrumCNN().to(device)\n",
    "        train_model(train_loader, val_loader, model, device)\n",
    "        \n",
    "        # Extract embeddings\n",
    "        model.eval()\n",
    "        fold_embeddings = []\n",
    "        fold_labels = []\n",
    "        fold_participants = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for specs, labels, parts in val_loader:\n",
    "                specs = specs.to(device)\n",
    "                embeddings = model(specs, return_embedding=True)\n",
    "                \n",
    "                fold_embeddings.append(embeddings.cpu().numpy())\n",
    "                fold_labels.append(labels.numpy())\n",
    "                fold_participants.extend(parts)\n",
    "        \n",
    "        all_embeddings.append(np.concatenate(fold_embeddings))\n",
    "        all_labels.append(np.concatenate(fold_labels))\n",
    "        all_participants.extend(fold_participants)\n",
    "    \n",
    "    # Prepare for k-NN evaluation\n",
    "    embeddings = np.concatenate(all_embeddings)\n",
    "    labels = np.array(all_labels)\n",
    "    participants = np.array(all_participants)\n",
    "    \n",
    "    # Scale embeddings (as per paper)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "    \n",
    "    # Evaluate using leave-one-participant-out\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "    unique_participants = np.unique(participants)\n",
    "    participant_accuracies = []\n",
    "    \n",
    "    print(\"\\nParticipant-wise evaluation:\")\n",
    "    for test_participant in unique_participants:\n",
    "        train_mask = participants != test_participant\n",
    "        test_mask = participants == test_participant\n",
    "        \n",
    "        # Scale using only training data\n",
    "        train_embeddings = scaled_embeddings[train_mask]\n",
    "        test_embeddings = scaled_embeddings[test_mask]\n",
    "        \n",
    "        knn.fit(train_embeddings, labels[train_mask])\n",
    "        accuracy = knn.score(test_embeddings, labels[test_mask])\n",
    "        participant_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"Participant {test_participant}: {accuracy:.3f}\")\n",
    "    \n",
    "    print(f\"\\nMean participant-independent accuracy: {np.mean(participant_accuracies):.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beatbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
