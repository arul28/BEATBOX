{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC Feature Extraction with extra envelope coeffecients\n",
    "\n",
    "Extract mfcc coeffecients and extra envelope coeffecients from both normal and augmented segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_features_expanded(segment_info_path, segments_dir='segments', n_mfcc=14, features_output_dir='../../extracted_features/features/mfcc_features.npy', labels_output_dir='../../extracted_features/labels/mfcc_labels.npy'):\n",
    "    \"\"\"\n",
    "    Extract expanded feature set including MFCCs, their deltas, and envelope descriptors.\n",
    "    Added robustness checks for empty or corrupted audio segments.\n",
    "    \"\"\"\n",
    "    # Load segment info\n",
    "    metadata = pd.read_csv(segment_info_path)\n",
    "    \n",
    "    # Update paths to use the specified segments directory\n",
    "    segments_path = Path(segments_dir)\n",
    "    metadata['segment_path'] = metadata['segment_path'].apply(\n",
    "        lambda x: str(segments_path / Path(x).name))\n",
    "    \n",
    "    # Initialize arrays to store features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"Extracting expanded feature set from {segments_dir}...\")\n",
    "    for idx, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "        try:\n",
    "            # Load audio segment\n",
    "            y, sr = librosa.load(row['segment_path'])\n",
    "            \n",
    "            # Check if the audio segment is valid\n",
    "            if len(y) == 0:\n",
    "                print(f\"Skipping empty audio file: {row['segment_path']}\")\n",
    "                metadata = metadata.drop(idx)\n",
    "                continue\n",
    "                \n",
    "            # 1. Extract MFCCs and their statistics\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "            mfcc_mean = np.mean(mfcc, axis=1)\n",
    "            \n",
    "            # 2. Compute MFCC deltas (first derivatives)\n",
    "            mfcc_delta = librosa.feature.delta(mfcc)\n",
    "            mfcc_delta_mean = np.mean(mfcc_delta, axis=1)\n",
    "            \n",
    "            # 3. Extract envelope-based descriptors\n",
    "            # Find the amplitude envelope\n",
    "            envelope = np.abs(y)\n",
    "            \n",
    "            # Safety check for empty envelope\n",
    "            if len(envelope) == 0:\n",
    "                print(f\"Skipping file with empty envelope: {row['segment_path']}\")\n",
    "                metadata = metadata.drop(idx)\n",
    "                continue\n",
    "            \n",
    "            # Find the maximum amplitude and its position\n",
    "            max_amp_pos = np.argmax(envelope)\n",
    "            max_amp = envelope[max_amp_pos]\n",
    "            \n",
    "            # 3.1 Maximum derivative before the maximum amplitude\n",
    "            pre_max_deriv = 0\n",
    "            if max_amp_pos > 0:\n",
    "                pre_envelope = envelope[:max_amp_pos]\n",
    "                if len(pre_envelope) > 1:  # Need at least 2 points for diff\n",
    "                    pre_max_deriv = np.max(np.diff(pre_envelope))\n",
    "            \n",
    "            # 3.2 Derivative after the maximum amplitude\n",
    "            post_max_deriv = 0\n",
    "            if max_amp_pos < len(envelope)-1:\n",
    "                post_envelope = envelope[max_amp_pos:]\n",
    "                if len(post_envelope) > 1:  # Need at least 2 points for diff\n",
    "                    post_max_deriv = np.min(np.diff(post_envelope))\n",
    "            \n",
    "            # 3.3 Temporal centroid\n",
    "            times = np.arange(len(y))\n",
    "            # Avoid division by zero\n",
    "            env_sum = np.sum(envelope)\n",
    "            if env_sum > 0:\n",
    "                temporal_centroid = np.sum(times * envelope) / env_sum\n",
    "                temporal_centroid_ratio = temporal_centroid / len(y)\n",
    "            else:\n",
    "                temporal_centroid_ratio = 0.5  # Default to middle if envelope is all zeros\n",
    "            \n",
    "            # 3.4 Flatness coefficient (spectral flatness as a proxy)\n",
    "            # Handle potential warnings from librosa\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                flatness = librosa.feature.spectral_flatness(y=y)[0].mean()\n",
    "                flatness = 0.0 if np.isnan(flatness) else flatness\n",
    "            \n",
    "            # Combine all features\n",
    "            feature_vector = np.concatenate([\n",
    "                mfcc_mean,                    # 14 features\n",
    "                mfcc_delta_mean,              # 14 features\n",
    "                [pre_max_deriv,               # 1 feature\n",
    "                 post_max_deriv,              # 1 feature\n",
    "                 flatness,                    # 1 feature\n",
    "                 temporal_centroid_ratio]      # 1 feature\n",
    "            ])\n",
    "            \n",
    "            features.append(feature_vector)\n",
    "            labels.append(row['instrument_label'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['segment_path']}: {str(e)}\")\n",
    "            metadata = metadata.drop(idx)\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Print summary of processing\n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"Successfully processed: {len(features)} segments\")\n",
    "    print(f\"Failed/Skipped: {len(metadata.index) - len(features)} segments\")\n",
    "    \n",
    "    # Create features directory if it doesn't exist\n",
    "    # output_dir = Path('features')\n",
    "    # output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Save features and labels with directory-specific names\n",
    "    # dir_suffix = '_augmented' if segments_dir == 'augmentedSegments' else ''\n",
    "    # np.save(output_dir / f'mfcc_features_expanded{dir_suffix}.npy', X)\n",
    "    # np.save(output_dir / f'labels_expanded{dir_suffix}.npy', y)\n",
    "    \n",
    "    np.save(features_output_dir, X)\n",
    "    np.save(labels_output_dir, y)\n",
    "    \n",
    "    return X, y, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_info_path = '../../segment_info/segment_info.csv'\n",
    "augmented_segment_info_path = '../../segment_info/augmented_segment_info.csv'\n",
    "\n",
    "os.makedirs('../../extracted_features/features', exist_ok=True)\n",
    "os.makedirs('../../extracted_features/labels', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting expanded feature set from ../../segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5714/5714 [00:11<00:00, 517.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete:\n",
      "Successfully processed: 5714 segments\n",
      "Failed/Skipped: 0 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_mfcc_env, y_mfcc_env, metadata_mfcc_env = extract_mfcc_features_expanded(\n",
    "    segment_info_path,\n",
    "    segments_dir='../../segments',\n",
    "    features_output_dir='../../extracted_features/features/mfcc_env_features.npy',\n",
    "    labels_output_dir='../../extracted_features/labels/mfcc_env_labels.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting expanded feature set from ../../augmentedSegments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34284/34284 [01:04<00:00, 533.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete:\n",
      "Successfully processed: 34284 segments\n",
      "Failed/Skipped: 0 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_mfcc_env_aug, y_mfcc_env_aug, metadata_mfcc_env_aug = extract_mfcc_features_expanded(\n",
    "    augmented_segment_info_path,\n",
    "    segments_dir='../../augmentedSegments',\n",
    "    features_output_dir='../../extracted_features/features/mfcc_env_aug_features.npy',\n",
    "    labels_output_dir='../../extracted_features/labels/mfcc_env_aug_labels.npy'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
