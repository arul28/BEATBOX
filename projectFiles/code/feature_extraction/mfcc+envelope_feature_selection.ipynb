{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Picking the best features from the 32 features that are extracted in mfcc+envelope_extraction.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Important Features (Random Forest):\n",
      "     Feature  RF_Importance\n",
      "1     mfcc_2       0.086723\n",
      "15   delta_2       0.085599\n",
      "2     mfcc_3       0.068139\n",
      "3     mfcc_4       0.044547\n",
      "30  flatness       0.041299\n",
      "4     mfcc_5       0.036626\n",
      "6     mfcc_7       0.034778\n",
      "16   delta_3       0.034503\n",
      "9    mfcc_10       0.033933\n",
      "0     mfcc_1       0.030908\n",
      "\n",
      "PCA Explained Variance Ratio:\n",
      "First 5 components explain: 39.10% of variance\n",
      "\n",
      "Highly Correlated Feature Pairs:\n",
      "pre_max_deriv & post_max_deriv: -0.896\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def analyze_feature_importance(X, y):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using multiple methods\n",
    "    \"\"\"\n",
    "    # Create feature names for better interpretation\n",
    "    feature_names = (\n",
    "        [f'mfcc_{i+1}' for i in range(14)] +  # 14 MFCC features\n",
    "        [f'delta_{i+1}' for i in range(14)] +  # 14 delta features\n",
    "        ['pre_max_deriv', 'post_max_deriv', 'flatness', 'temporal_centroid']  # 4 envelope features\n",
    "    )\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # 1. PCA Analysis\n",
    "    pca = PCA()\n",
    "    pca.fit(X_scaled)\n",
    "    \n",
    "    # Calculate feature importance based on PCA components\n",
    "    feature_importance_pca = np.abs(pca.components_[0])  # Using first principal component\n",
    "    \n",
    "    # 2. Random Forest Feature Importance\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_scaled, y)\n",
    "    feature_importance_rf = rf.feature_importances_\n",
    "    \n",
    "    # 3. Correlation Analysis\n",
    "    correlation_matrix = np.corrcoef(X_scaled.T)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'PCA_Importance': feature_importance_pca,\n",
    "        'RF_Importance': feature_importance_rf\n",
    "    })\n",
    "    \n",
    "    # Sort by Random Forest importance\n",
    "    results_df = results_df.sort_values('RF_Importance', ascending=False)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Feature Importance\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.bar(range(len(feature_importance_rf)), results_df['RF_Importance'])\n",
    "    plt.xticks(range(len(feature_importance_rf)), results_df['Feature'], rotation=45, ha='right')\n",
    "    plt.title('Feature Importance (Random Forest)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Plot 2: Correlation Heatmap\n",
    "    plt.subplot(2, 1, 2)\n",
    "    sns.heatmap(correlation_matrix, xticklabels=feature_names, yticklabels=feature_names, \n",
    "                cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print top features\n",
    "    print(\"\\nTop 10 Most Important Features (Random Forest):\")\n",
    "    print(results_df[['Feature', 'RF_Importance']].head(10))\n",
    "    \n",
    "    print(\"\\nPCA Explained Variance Ratio:\")\n",
    "    print(f\"First 5 components explain: {pca.explained_variance_ratio_[:5].cumsum()[-1]:.2%} of variance\")\n",
    "    \n",
    "    # Identify highly correlated features\n",
    "    print(\"\\nHighly Correlated Feature Pairs:\")\n",
    "    for i in range(len(feature_names)):\n",
    "        for j in range(i+1, len(feature_names)):\n",
    "            if abs(correlation_matrix[i,j]) > 0.8:  # Threshold for high correlation\n",
    "                print(f\"{feature_names[i]} & {feature_names[j]}: {correlation_matrix[i,j]:.3f}\")\n",
    "    \n",
    "    return results_df, correlation_matrix, pca\n",
    "\n",
    "# Load the data\n",
    "X = np.load('../../extracted_features/features/mfcc_env_features.npy')\n",
    "y = np.load('../../extracted_features/labels/mfcc_env_labels.npy')\n",
    "\n",
    "# Run the analysis\n",
    "results_df, correlation_matrix, pca = analyze_feature_importance(X, y)\n",
    "\n",
    "os.makedirs('../../visualization', exist_ok=True)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('../../visualization/feature_extraction_analysis.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_features_expanded(segment_info_path, segments_dir='segments', n_mfcc=14, features_output_dir='../../extracted_features/features/mfcc_features.npy', labels_output_dir='../../extracted_features/labels/mfcc_labels.npy'):\n",
    "    \"\"\"\n",
    "    Extract expanded feature set including MFCCs, their deltas, and envelope descriptors.\n",
    "    Added robustness checks for empty or corrupted audio segments.\n",
    "    \"\"\"\n",
    "    # Load segment info\n",
    "    metadata = pd.read_csv(segment_info_path)\n",
    "    \n",
    "    # Update paths to use the specified segments directory\n",
    "    segments_path = Path(segments_dir)\n",
    "    metadata['segment_path'] = metadata['segment_path'].apply(\n",
    "        lambda x: str(segments_path / Path(x).name))\n",
    "    \n",
    "    # Initialize arrays to store features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"Extracting expanded feature set from {segments_dir}...\")\n",
    "    for idx, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "        try:\n",
    "            # Load audio segment\n",
    "            y, sr = librosa.load(row['segment_path'])\n",
    "            \n",
    "            # Check if the audio segment is valid\n",
    "            if len(y) == 0:\n",
    "                print(f\"Skipping empty audio file: {row['segment_path']}\")\n",
    "                metadata = metadata.drop(idx)\n",
    "                continue\n",
    "                \n",
    "            # 1. Extract MFCCs and their statistics\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "            mfcc_mean = np.mean(mfcc, axis=1)\n",
    "            \n",
    "            # 2. Compute MFCC deltas (first derivatives)\n",
    "            mfcc_delta = librosa.feature.delta(mfcc)\n",
    "            mfcc_delta_mean = np.mean(mfcc_delta, axis=1)\n",
    "            \n",
    "            # 3. Extract envelope-based descriptors\n",
    "            # Find the amplitude envelope\n",
    "            envelope = np.abs(y)\n",
    "            \n",
    "            # Safety check for empty envelope\n",
    "            if len(envelope) == 0:\n",
    "                print(f\"Skipping file with empty envelope: {row['segment_path']}\")\n",
    "                metadata = metadata.drop(idx)\n",
    "                continue\n",
    "            \n",
    "            # Find the maximum amplitude and its position\n",
    "            max_amp_pos = np.argmax(envelope)\n",
    "            max_amp = envelope[max_amp_pos]\n",
    "            \n",
    "            # 3.1 Maximum derivative before the maximum amplitude\n",
    "            pre_max_deriv = 0\n",
    "            if max_amp_pos > 0:\n",
    "                pre_envelope = envelope[:max_amp_pos]\n",
    "                if len(pre_envelope) > 1:  # Need at least 2 points for diff\n",
    "                    pre_max_deriv = np.max(np.diff(pre_envelope))\n",
    "            \n",
    "            # 3.2 Derivative after the maximum amplitude\n",
    "            post_max_deriv = 0\n",
    "            if max_amp_pos < len(envelope)-1:\n",
    "                post_envelope = envelope[max_amp_pos:]\n",
    "                if len(post_envelope) > 1:  # Need at least 2 points for diff\n",
    "                    post_max_deriv = np.min(np.diff(post_envelope))\n",
    "            \n",
    "            # 3.3 Temporal centroid\n",
    "            times = np.arange(len(y))\n",
    "            # Avoid division by zero\n",
    "            env_sum = np.sum(envelope)\n",
    "            if env_sum > 0:\n",
    "                temporal_centroid = np.sum(times * envelope) / env_sum\n",
    "                temporal_centroid_ratio = temporal_centroid / len(y)\n",
    "            else:\n",
    "                temporal_centroid_ratio = 0.5  # Default to middle if envelope is all zeros\n",
    "            \n",
    "            # 3.4 Flatness coefficient (spectral flatness as a proxy)\n",
    "            # Handle potential warnings from librosa\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                flatness = librosa.feature.spectral_flatness(y=y)[0].mean()\n",
    "                flatness = 0.0 if np.isnan(flatness) else flatness\n",
    "            \n",
    "            # Combine all features\n",
    "            feature_vector = np.concatenate([\n",
    "                mfcc_mean,                    # 14 features\n",
    "                mfcc_delta_mean,              # 14 features\n",
    "                [pre_max_deriv,               # 1 feature\n",
    "                 post_max_deriv,              # 1 feature\n",
    "                 flatness,                    # 1 feature\n",
    "                 temporal_centroid_ratio]      # 1 feature\n",
    "            ])\n",
    "            \n",
    "            features.append(feature_vector)\n",
    "            labels.append(row['instrument_label'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['segment_path']}: {str(e)}\")\n",
    "            metadata = metadata.drop(idx)\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Print summary of processing\n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"Successfully processed: {len(features)} segments\")\n",
    "    print(f\"Failed/Skipped: {len(metadata.index) - len(features)} segments\")\n",
    "    \n",
    "    # Create features directory if it doesn't exist\n",
    "    # output_dir = Path('features')\n",
    "    # output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Save features and labels with directory-specific names\n",
    "    # dir_suffix = '_augmented' if segments_dir == 'augmentedSegments' else ''\n",
    "    # np.save(output_dir / f'mfcc_features_expanded{dir_suffix}.npy', X)\n",
    "    # np.save(output_dir / f'labels_expanded{dir_suffix}.npy', y)\n",
    "    \n",
    "    np.save(features_output_dir, X)\n",
    "    np.save(labels_output_dir, y)\n",
    "    \n",
    "    return X, y, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_features_optimized(segment_info_path, segments_dir='segments', n_mfcc=14, features_output_dir='../../extracted_features/features/mfcc_features.npy', labels_output_dir='../../extracted_features/labels/mfcc_labels.npy'):\n",
    "    \"\"\"\n",
    "    Extract only the most important features based on our analysis.\n",
    "    \"\"\"\n",
    "    # First, get all features\n",
    "    X, y, metadata = extract_mfcc_features_expanded(\n",
    "        segment_info_path, \n",
    "        segments_dir, \n",
    "        n_mfcc,\n",
    "        features_output_dir,\n",
    "        labels_output_dir\n",
    "    )\n",
    "    \n",
    "    # Debug prints\n",
    "    print(\"Shape of X:\", X.shape)\n",
    "    print(\"Type of X:\", type(X))\n",
    "    print(\"First few elements of X:\", X[:5])\n",
    "    \n",
    "    # Load the saved features to verify they were saved correctly\n",
    "    X = np.load(features_output_dir)\n",
    "    print(\"Shape of loaded X:\", X.shape)\n",
    "    \n",
    "    # Make sure X is 2D\n",
    "    if len(X.shape) == 1:\n",
    "        print(\"Reshaping X...\")\n",
    "        X = X.reshape(-1, 34)  # 34 features: 14 MFCCs + 14 deltas + 4 envelope features\n",
    "        print(\"New shape of X:\", X.shape)\n",
    "    \n",
    "    # Define indices of important features based on analysis\n",
    "    important_feature_indices = [\n",
    "        1,  # mfcc_2\n",
    "        15, # delta_2\n",
    "        2,  # mfcc_3\n",
    "        3,  # mfcc_4\n",
    "        30, # flatness\n",
    "        4,  # mfcc_5\n",
    "        6,  # mfcc_7\n",
    "        16, # delta_3\n",
    "        9,  # mfcc_10\n",
    "        0,  # mfcc_1\n",
    "        29, # temporal_centroid\n",
    "        28  # pre_max_deriv\n",
    "    ]\n",
    "    \n",
    "    # Select only the important features\n",
    "    X_selected = X[:, important_feature_indices]\n",
    "    \n",
    "    # Save the selected features\n",
    "    np.save(features_output_dir, X_selected)\n",
    "    np.save(labels_output_dir, y)\n",
    "    \n",
    "    print(f\"Final shape of X_selected: {X_selected.shape}\")\n",
    "    return X_selected, y, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_info_path = '../../segment_info/segment_info.csv'\n",
    "augmented_segment_info_path = '../../segment_info/augmented_segment_info.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting expanded feature set from ../../segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5714/5714 [00:11<00:00, 482.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete:\n",
      "Successfully processed: 5714 segments\n",
      "Failed/Skipped: 0 segments\n",
      "Shape of X: (5714, 32)\n",
      "Type of X: <class 'numpy.ndarray'>\n",
      "First few elements of X: [[-3.68122345e+02  1.36077242e+01  5.19156027e+00  6.92084074e+00\n",
      "  -2.02500820e+01 -9.85776424e+00 -2.29979496e+01  6.71332121e+00\n",
      "   4.09922504e+00 -1.35043945e+01 -1.25964928e+01 -4.04153204e+00\n",
      "  -7.45831156e+00 -6.52279091e+00 -1.65836639e+01  8.35736942e+00\n",
      "   9.07826841e-01  1.26167294e-02  1.16370177e+00 -3.48212808e-01\n",
      "   3.92646998e-01  8.85657873e-03  1.27106011e-01  3.27873826e-01\n",
      "  -5.89539558e-02 -1.78510681e-01 -5.40644050e-01 -4.96054739e-01\n",
      "   1.26419216e-01 -1.32621095e-01  9.62158144e-02  2.86268803e-01]\n",
      " [-4.62650421e+02  6.26236954e+01  6.74965048e+00  6.19364166e+00\n",
      "  -1.31714792e+01 -1.31292267e+01 -2.00550766e+01  5.50945950e+00\n",
      "   6.36978912e+00 -8.63569736e+00 -1.24239960e+01 -5.82191420e+00\n",
      "  -1.06630611e+01 -7.98821878e+00 -2.25514851e+01  7.56658888e+00\n",
      "   9.36554670e-01  4.88572001e-01  1.70452797e+00 -2.29848728e-01\n",
      "   6.02070570e-01 -3.62888545e-01  8.79644573e-01  7.78507411e-01\n",
      "  -5.53027093e-01 -3.95722568e-01 -4.14513588e-01 -4.38183188e-01\n",
      "   1.18776865e-01 -1.39946654e-01  5.77729791e-02  1.51616666e-01]\n",
      " [-4.11087036e+02  3.47303429e+01  1.02768164e+01  1.34073524e+01\n",
      "  -1.06803026e+01 -1.11955175e+01 -1.76077271e+01  1.77509701e+00\n",
      "   1.26023924e+00 -7.44957876e+00 -1.05162497e+01 -3.22238016e+00\n",
      "  -7.67404318e+00 -6.44019365e+00  1.18289256e+00 -3.93496490e+00\n",
      "   1.04614305e+00 -1.42359242e-01  2.73255527e-01 -1.20327123e-01\n",
      "  -6.02358580e-02 -8.16165984e-01  6.08615458e-01  2.22875103e-01\n",
      "  -9.17098224e-02  3.29561919e-01 -7.46054277e-02 -3.28310341e-01\n",
      "   8.20593312e-02 -9.37118679e-02  7.06424862e-02  4.89170448e-01]\n",
      " [-3.70167755e+02  1.54944906e+01 -2.44734478e+00  7.66767740e+00\n",
      "  -1.91426926e+01 -1.15335379e+01 -2.48214169e+01  3.35252905e+00\n",
      "   5.44521713e+00 -5.22538042e+00 -9.28639030e+00  8.97286057e-01\n",
      "  -6.77998066e+00 -7.86240339e+00 -1.51631165e+01  9.41259575e+00\n",
      "   1.55733097e+00 -8.88226867e-01  1.36737072e+00  5.96732914e-01\n",
      "   5.92468500e-01  3.12253237e-01  6.11748695e-02  3.88881892e-01\n",
      "  -1.29354268e-01 -6.49390817e-01 -3.89873117e-01 -5.35585821e-01\n",
      "   1.28492489e-01 -1.00844964e-01  9.19543952e-02  2.87156437e-01]\n",
      " [-4.58662811e+02  6.51236649e+01  2.98494196e+00  6.23464632e+00\n",
      "  -1.05245752e+01 -1.10928888e+01 -2.03291645e+01  4.53546810e+00\n",
      "   7.61622477e+00 -3.23027492e+00 -1.03507347e+01 -3.70923471e+00\n",
      "  -7.06708479e+00 -8.00749016e+00 -2.06373196e+01  9.09885693e+00\n",
      "   1.54042041e+00  8.20233941e-01  1.24099267e+00 -9.00185525e-01\n",
      "   2.59450167e-01 -7.00834334e-01  8.95079076e-01  5.36457121e-01\n",
      "  -4.46379691e-01 -6.76072776e-01  1.48463249e-01  1.18710071e-01\n",
      "   1.28492475e-01 -1.00844957e-01  5.16823791e-02  1.67186623e-01]]\n",
      "Shape of loaded X: (5714, 32)\n",
      "Final shape of X_selected: (5714, 12)\n",
      "Extracting expanded feature set from ../../augmentedSegments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34284/34284 [01:06<00:00, 517.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete:\n",
      "Successfully processed: 34284 segments\n",
      "Failed/Skipped: 0 segments\n",
      "Shape of X: (34284, 32)\n",
      "Type of X: <class 'numpy.ndarray'>\n",
      "First few elements of X: [[-3.68122345e+02  1.36077242e+01  5.19156027e+00  6.92084074e+00\n",
      "  -2.02500820e+01 -9.85776424e+00 -2.29979496e+01  6.71332121e+00\n",
      "   4.09922504e+00 -1.35043945e+01 -1.25964928e+01 -4.04153204e+00\n",
      "  -7.45831156e+00 -6.52279091e+00 -1.65836639e+01  8.35736942e+00\n",
      "   9.07826841e-01  1.26167294e-02  1.16370177e+00 -3.48212808e-01\n",
      "   3.92646998e-01  8.85657873e-03  1.27106011e-01  3.27873826e-01\n",
      "  -5.89539558e-02 -1.78510681e-01 -5.40644050e-01 -4.96054739e-01\n",
      "   1.26419216e-01 -1.32621095e-01  9.62158144e-02  2.86268803e-01]\n",
      " [-4.62650421e+02  6.26236954e+01  6.74965048e+00  6.19364166e+00\n",
      "  -1.31714792e+01 -1.31292267e+01 -2.00550766e+01  5.50945950e+00\n",
      "   6.36978912e+00 -8.63569736e+00 -1.24239960e+01 -5.82191420e+00\n",
      "  -1.06630611e+01 -7.98821878e+00 -2.25514851e+01  7.56658888e+00\n",
      "   9.36554670e-01  4.88572001e-01  1.70452797e+00 -2.29848728e-01\n",
      "   6.02070570e-01 -3.62888545e-01  8.79644573e-01  7.78507411e-01\n",
      "  -5.53027093e-01 -3.95722568e-01 -4.14513588e-01 -4.38183188e-01\n",
      "   1.18776865e-01 -1.39946654e-01  5.77729791e-02  1.51616666e-01]\n",
      " [-4.11087036e+02  3.47303429e+01  1.02768164e+01  1.34073524e+01\n",
      "  -1.06803026e+01 -1.11955175e+01 -1.76077271e+01  1.77509701e+00\n",
      "   1.26023924e+00 -7.44957876e+00 -1.05162497e+01 -3.22238016e+00\n",
      "  -7.67404318e+00 -6.44019365e+00  1.18289256e+00 -3.93496490e+00\n",
      "   1.04614305e+00 -1.42359242e-01  2.73255527e-01 -1.20327123e-01\n",
      "  -6.02358580e-02 -8.16165984e-01  6.08615458e-01  2.22875103e-01\n",
      "  -9.17098224e-02  3.29561919e-01 -7.46054277e-02 -3.28310341e-01\n",
      "   8.20593312e-02 -9.37118679e-02  7.06424862e-02  4.89170448e-01]\n",
      " [-3.70167755e+02  1.54944906e+01 -2.44734478e+00  7.66767740e+00\n",
      "  -1.91426926e+01 -1.15335379e+01 -2.48214169e+01  3.35252905e+00\n",
      "   5.44521713e+00 -5.22538042e+00 -9.28639030e+00  8.97286057e-01\n",
      "  -6.77998066e+00 -7.86240339e+00 -1.51631165e+01  9.41259575e+00\n",
      "   1.55733097e+00 -8.88226867e-01  1.36737072e+00  5.96732914e-01\n",
      "   5.92468500e-01  3.12253237e-01  6.11748695e-02  3.88881892e-01\n",
      "  -1.29354268e-01 -6.49390817e-01 -3.89873117e-01 -5.35585821e-01\n",
      "   1.28492489e-01 -1.00844964e-01  9.19543952e-02  2.87156437e-01]\n",
      " [-4.58662811e+02  6.51236649e+01  2.98494196e+00  6.23464632e+00\n",
      "  -1.05245752e+01 -1.10928888e+01 -2.03291645e+01  4.53546810e+00\n",
      "   7.61622477e+00 -3.23027492e+00 -1.03507347e+01 -3.70923471e+00\n",
      "  -7.06708479e+00 -8.00749016e+00 -2.06373196e+01  9.09885693e+00\n",
      "   1.54042041e+00  8.20233941e-01  1.24099267e+00 -9.00185525e-01\n",
      "   2.59450167e-01 -7.00834334e-01  8.95079076e-01  5.36457121e-01\n",
      "  -4.46379691e-01 -6.76072776e-01  1.48463249e-01  1.18710071e-01\n",
      "   1.28492475e-01 -1.00844957e-01  5.16823791e-02  1.67186623e-01]]\n",
      "Shape of loaded X: (34284, 32)\n",
      "Final shape of X_selected: (34284, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_mfcc_extracted, y_mfcc_extracted, metadata_mfcc_extracted = extract_mfcc_features_optimized(\n",
    "    segment_info_path,\n",
    "    segments_dir='../../segments',\n",
    "    features_output_dir='../../extracted_features/features/mfcc_extracted_features.npy',\n",
    "    labels_output_dir='../../extracted_features/labels/mfcc_extracted_labels.npy'\n",
    ")\n",
    "\n",
    "X_mfcc_extracted_aug, y_mfcc_extracted_aug, metadata_mfcc_extracted_aug = extract_mfcc_features_optimized(\n",
    "    augmented_segment_info_path,\n",
    "    segments_dir='../../augmentedSegments',\n",
    "    features_output_dir='../../extracted_features/features/mfcc_extracted_aug_features.npy',\n",
    "    labels_output_dir='../../extracted_features/labels/mfcc_extracted_aug_labels.npy'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
