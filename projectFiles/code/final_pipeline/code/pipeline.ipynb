{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing User Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from scipy.signal import butter, filtfilt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(y, sr):\n",
    "    \"\"\"\n",
    "    Clean up the audio recording with minimal interference to the core sound.\n",
    "    \n",
    "    Args:\n",
    "        y: Audio signal\n",
    "        sr: Sample rate\n",
    "    Returns:\n",
    "        Cleaned audio signal\n",
    "    \"\"\"\n",
    "    # 1. High-pass filter to remove low frequency rumble/boom\n",
    "    nyq = sr / 2\n",
    "    cutoff = 60  # Hz - remove frequencies below 60Hz\n",
    "    order = 4\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    y_filtered = filtfilt(b, a, y)\n",
    "    \n",
    "    # 2. Normalize audio\n",
    "    y_normalized = librosa.util.normalize(y_filtered)\n",
    "    \n",
    "    # 3. Remove silence and very quiet parts\n",
    "    y_cleaned = librosa.effects.trim(\n",
    "        y_normalized,\n",
    "        top_db=30,  # adjust this value based on your needs\n",
    "        frame_length=2048,\n",
    "        hop_length=512\n",
    "    )[0]\n",
    "    \n",
    "    # 4. Subtle noise reduction using spectral gating\n",
    "    S = librosa.stft(y_cleaned)\n",
    "    mag = np.abs(S)\n",
    "    phase = np.angle(S)\n",
    "    \n",
    "    # Estimate noise floor\n",
    "    noise_floor = np.mean(np.min(mag, axis=1))\n",
    "    threshold = noise_floor * 2  # Adjust multiplier based on needs\n",
    "    \n",
    "    # Apply soft gate\n",
    "    mag_gated = np.maximum(mag - threshold, 0)\n",
    "    y_denoised = librosa.istft(mag_gated * np.exp(1j * phase))\n",
    "    \n",
    "    # 5. Final normalization\n",
    "    y_final = librosa.util.normalize(y_denoised)\n",
    "    \n",
    "    return y_final\n",
    "\n",
    "def segment_user_input(input_file, segments_output_dir, csv_output_dir, drum_type, segment_duration=0.5):\n",
    "    \"\"\"\n",
    "    Segment a user's drum recording based on onset detection.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to the input wav file (snare.wav or kick.wav)\n",
    "        segments_output_dir: Directory to save the segmented audio files\n",
    "        csv_output_dir: Directory to save the segment info CSV\n",
    "        drum_type: Type of drum ('sd' for snare, 'kd' for kick)\n",
    "        segment_duration: Duration of each segment in seconds\n",
    "    \"\"\"\n",
    "    # Create output directories\n",
    "    segments_dir = Path(segments_output_dir) / f\"{drum_type}_segments\"\n",
    "    segments_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    csv_dir = Path(csv_output_dir)\n",
    "    csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load and preprocess audio file\n",
    "    y, sr = librosa.load(input_file, sr=None)  # Keep original sample rate\n",
    "    \n",
    "    # Clean up the audio\n",
    "    print(\"Cleaning up audio...\")\n",
    "    y_cleaned = preprocess_audio(y, sr)\n",
    "    \n",
    "    # Save preprocessed full audio for reference\n",
    "    preprocessed_path = Path(input_file).parent / f\"{Path(input_file).stem}_cleaned.wav\"\n",
    "    sf.write(preprocessed_path, y_cleaned, sr)\n",
    "    print(f\"Saved cleaned audio to: {preprocessed_path}\")\n",
    "    \n",
    "    # Detect onsets on cleaned audio\n",
    "    onset_frames = librosa.onset.onset_detect(\n",
    "        y=y_cleaned, \n",
    "        sr=sr,\n",
    "        units='frames',\n",
    "        hop_length=512,\n",
    "        backtrack=True,\n",
    "        pre_max=20,\n",
    "        post_max=20,\n",
    "        pre_avg=100,\n",
    "        post_avg=100,\n",
    "        delta=0.2,\n",
    "        wait=30\n",
    "    )\n",
    "    \n",
    "    # Convert frames to time\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "    \n",
    "    segment_info = []\n",
    "    \n",
    "    # Process each onset\n",
    "    for i, onset_time in enumerate(onset_times):\n",
    "        start_sample = int(onset_time * sr)\n",
    "        end_sample = start_sample + int(segment_duration * sr)\n",
    "        \n",
    "        if start_sample < 0:\n",
    "            start_sample = 0\n",
    "        if end_sample > len(y_cleaned):\n",
    "            end_sample = len(y_cleaned)\n",
    "            \n",
    "        if end_sample > start_sample:\n",
    "            segment = y_cleaned[start_sample:end_sample]\n",
    "            \n",
    "            if len(segment) < int(segment_duration * sr):\n",
    "                segment = np.pad(\n",
    "                    segment,\n",
    "                    (0, int(segment_duration * sr) - len(segment)),\n",
    "                    mode='constant'\n",
    "                )\n",
    "            \n",
    "            segment_filename = f\"user_{drum_type}_{i:04d}.wav\"\n",
    "            segment_path = segments_dir / segment_filename\n",
    "            \n",
    "            sf.write(str(segment_path), segment, sr)\n",
    "            \n",
    "            segment_info.append({\n",
    "                'segment_path': str(segment_path),\n",
    "                'instrument_label': drum_type,\n",
    "                'onset_time': onset_time,\n",
    "                'original_wav': str(input_file)\n",
    "            })\n",
    "    \n",
    "    segment_df = pd.DataFrame(segment_info)\n",
    "    csv_path = csv_dir / f\"{drum_type}_segment_info.csv\"\n",
    "    segment_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\nSegmentation Summary for {drum_type}:\")\n",
    "    print(f\"Total segments extracted: {len(segment_df)}\")\n",
    "    print(f\"Segments saved in: {segments_dir}\")\n",
    "    print(f\"CSV saved as: {csv_path}\")\n",
    "    \n",
    "    return segment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up audio...\n",
      "Saved cleaned audio to: ../audio/sample1/vox/snare_cleaned.wav\n",
      "\n",
      "Segmentation Summary for sd:\n",
      "Total segments extracted: 8\n",
      "Segments saved in: ../audio/sample1/segments/sd_segments\n",
      "CSV saved as: ../csv_info/sd_segment_info.csv\n"
     ]
    }
   ],
   "source": [
    "snare_df = segment_user_input(\n",
    "    input_file=\"../audio/sample1/vox/snare.wav\",\n",
    "    segments_output_dir=\"../audio/sample1/segments\",\n",
    "    csv_output_dir=\"../csv_info\",\n",
    "    drum_type=\"sd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up audio...\n",
      "Saved cleaned audio to: ../audio/sample1/vox/kick_cleaned.wav\n",
      "\n",
      "Segmentation Summary for kd:\n",
      "Total segments extracted: 7\n",
      "Segments saved in: ../audio/sample1/segments/kd_segments\n",
      "CSV saved as: ../csv_info/kd_segment_info.csv\n"
     ]
    }
   ],
   "source": [
    "kick_df = segment_user_input(\n",
    "    input_file=\"../audio/sample1/vox/kick.wav\",\n",
    "    segments_output_dir=\"../audio/sample1/segments\",\n",
    "    csv_output_dir=\"../csv_info\",\n",
    "    drum_type=\"kd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_user_input(segment_info_path, segments_dir, n_mfcc=14, features_output_dir=None, labels_output_dir=None):\n",
    "    \"\"\"\n",
    "    Extract the same features as the original pipeline: MFCCs, deltas, and envelope descriptors.\n",
    "    Then select the most important features based on the original analysis.\n",
    "    \n",
    "    Args:\n",
    "        segment_info_path: Path to the CSV containing segment information\n",
    "        segments_dir: Directory containing the audio segments\n",
    "        n_mfcc: Number of MFCC coefficients (default 14 as in original)\n",
    "        features_output_dir: Where to save the features\n",
    "        labels_output_dir: Where to save the labels\n",
    "    \"\"\"\n",
    "    # Load segment info\n",
    "    metadata = pd.read_csv(segment_info_path)\n",
    "    \n",
    "    # Initialize arrays to store features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"Extracting expanded feature set from {segments_dir}...\")\n",
    "    for idx, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "        try:\n",
    "            # Load audio segment\n",
    "            y, sr = librosa.load(row['segment_path'])\n",
    "            \n",
    "            # Skip if empty\n",
    "            if len(y) == 0:\n",
    "                print(f\"Skipping empty audio file: {row['segment_path']}\")\n",
    "                continue\n",
    "                \n",
    "            # 1. Extract MFCCs and their statistics\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "            mfcc_mean = np.mean(mfcc, axis=1)\n",
    "            \n",
    "            # 2. Compute MFCC deltas (first derivatives)\n",
    "            mfcc_delta = librosa.feature.delta(mfcc)\n",
    "            mfcc_delta_mean = np.mean(mfcc_delta, axis=1)\n",
    "            \n",
    "            # 3. Extract envelope-based descriptors\n",
    "            envelope = np.abs(y)\n",
    "            \n",
    "            if len(envelope) == 0:\n",
    "                print(f\"Skipping file with empty envelope: {row['segment_path']}\")\n",
    "                continue\n",
    "            \n",
    "            # Find the maximum amplitude and its position\n",
    "            max_amp_pos = np.argmax(envelope)\n",
    "            \n",
    "            # 3.1 Maximum derivative before the maximum amplitude\n",
    "            pre_max_deriv = 0\n",
    "            if max_amp_pos > 0:\n",
    "                pre_envelope = envelope[:max_amp_pos]\n",
    "                if len(pre_envelope) > 1:\n",
    "                    pre_max_deriv = np.max(np.diff(pre_envelope))\n",
    "            \n",
    "            # 3.2 Derivative after the maximum amplitude\n",
    "            post_max_deriv = 0\n",
    "            if max_amp_pos < len(envelope)-1:\n",
    "                post_envelope = envelope[max_amp_pos:]\n",
    "                if len(post_envelope) > 1:\n",
    "                    post_max_deriv = np.min(np.diff(post_envelope))\n",
    "            \n",
    "            # 3.3 Temporal centroid\n",
    "            times = np.arange(len(y))\n",
    "            env_sum = np.sum(envelope)\n",
    "            if env_sum > 0:\n",
    "                temporal_centroid = np.sum(times * envelope) / env_sum\n",
    "                temporal_centroid_ratio = temporal_centroid / len(y)\n",
    "            else:\n",
    "                temporal_centroid_ratio = 0.5\n",
    "            \n",
    "            # 3.4 Flatness coefficient\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                flatness = librosa.feature.spectral_flatness(y=y)[0].mean()\n",
    "                flatness = 0.0 if np.isnan(flatness) else flatness\n",
    "            \n",
    "            # Combine all features in the same order as original pipeline\n",
    "            feature_vector = np.concatenate([\n",
    "                mfcc_mean,                # 14 features\n",
    "                mfcc_delta_mean,          # 14 features\n",
    "                [pre_max_deriv,           # 1 feature\n",
    "                 post_max_deriv,          # 1 feature\n",
    "                 flatness,                # 1 feature\n",
    "                 temporal_centroid_ratio]  # 1 feature\n",
    "            ])\n",
    "            \n",
    "            features.append(feature_vector)\n",
    "            labels.append(row['instrument_label'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['segment_path']}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Select only the important features (same as original pipeline)\n",
    "    important_feature_indices = [\n",
    "        1,   # mfcc_2\n",
    "        15,  # delta_2\n",
    "        2,   # mfcc_3\n",
    "        3,   # mfcc_4\n",
    "        30,  # flatness\n",
    "        4,   # mfcc_5\n",
    "        6,   # mfcc_7\n",
    "        16,  # delta_3\n",
    "        9,   # mfcc_10\n",
    "        0,   # mfcc_1\n",
    "        29,  # temporal_centroid\n",
    "        28   # pre_max_deriv\n",
    "    ]\n",
    "    \n",
    "    # Select the optimized feature set\n",
    "    X_selected = X[:, important_feature_indices]\n",
    "    \n",
    "    # Save features if paths provided\n",
    "    if features_output_dir and labels_output_dir:\n",
    "        np.save(features_output_dir, X_selected)\n",
    "        np.save(labels_output_dir, y)\n",
    "    \n",
    "    print(f\"\\nFeature extraction complete:\")\n",
    "    print(f\"Successfully processed: {len(features)} segments\")\n",
    "    print(f\"Feature vector shape: {X_selected.shape}\")\n",
    "    print(\"\\nFeature set includes:\")\n",
    "    print(\"- Selected MFCCs and their deltas\")\n",
    "    print(\"- Envelope descriptors (flatness, temporal centroid, pre-max derivative)\")\n",
    "    \n",
    "    return X_selected, y\n",
    "\n",
    "# Example usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting expanded feature set from ../audio/sample1/segments/sd_segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 163.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction complete:\n",
      "Successfully processed: 8 segments\n",
      "Feature vector shape: (8, 12)\n",
      "\n",
      "Feature set includes:\n",
      "- Selected MFCCs and their deltas\n",
      "- Envelope descriptors (flatness, temporal centroid, pre-max derivative)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "snare_features, snare_labels = extract_features_from_user_input(\n",
    "    segment_info_path='../csv_info/sd_segment_info.csv',\n",
    "    segments_dir='../audio/sample1/segments/sd_segments',\n",
    "    features_output_dir='../extracted_features/features/snare_features.npy',\n",
    "    labels_output_dir='../extracted_features/labels/snare_labels.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting expanded feature set from ../audio/sample1/segments/kd_segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 165.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction complete:\n",
      "Successfully processed: 7 segments\n",
      "Feature vector shape: (7, 12)\n",
      "\n",
      "Feature set includes:\n",
      "- Selected MFCCs and their deltas\n",
      "- Envelope descriptors (flatness, temporal centroid, pre-max derivative)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kick_features, kick_labels = extract_features_from_user_input(\n",
    "    segment_info_path='../csv_info/kd_segment_info.csv',\n",
    "    segments_dir='../audio/sample1/segments/kd_segments',\n",
    "    features_output_dir='../extracted_features/features/kick_features.npy',\n",
    "    labels_output_dir='../extracted_features/labels/kick_labels.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data split summary:\n",
      "Base training examples: 5714\n",
      "User adaptation examples: 7\n",
      "User validation examples: 8\n",
      "\n",
      "Adaptation set label distribution:\n",
      "sd    4\n",
      "kd    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set label distribution:\n",
      "kd    4\n",
      "sd    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Performing grid search for base model parameters...\n",
      "\n",
      "Best base model parameters: {'metric': 'euclidean', 'n_neighbors': 11, 'weights': 'uniform'}\n",
      "Best cross-validation score: 0.514\n",
      "\n",
      "Grid searching weight factors and regularization parameters...\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  1  3 11 10] with weights [0.08338947 0.09972653 0.11888635 0.15797155 0.16685551]\n",
      "Bottom 5 features: [9 4 8 2 5] with weights [0.01827592 0.04874903 0.04935377 0.05544346 0.05934138]\n",
      "Weight=2, C=0.3, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  8  9 11 10] with weights [0. 0. 0. 0. 1.]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2, C=0.3, Method=lasso: Score=0.250\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  1  3 10 11] with weights [0.10516378 0.11277461 0.12408819 0.13523104 0.15313917]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.01045882 0.04417683 0.04734822 0.04914097 0.05438834]\n",
      "Weight=2, C=0.6, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11  6 10] with weights [0.         0.         0.         0.49661423 0.50338577]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2, C=0.6, Method=lasso: Score=0.750\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [10  1  7  3 11] with weights [0.11218393 0.12313365 0.12541654 0.12776236 0.14760785]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.00394429 0.03058658 0.04663258 0.04792396 0.05305955]\n",
      "Weight=2, C=1.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.37548804 0.62451196]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2, C=1.0, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1  7 11] with weights [0.10348467 0.13006446 0.1306713  0.14123421 0.1422982 ]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.0009861  0.02012665 0.04598412 0.04631707 0.05178584]\n",
      "Weight=2, C=1.5, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.       0.       0.       0.334098 0.665902]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2, C=1.5, Method=lasso: Score=0.750\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1 11  7] with weights [0.10606725 0.13079086 0.13481456 0.13785473 0.15103053]\n",
      "Bottom 5 features: [9 6 8 4 2] with weights [0.00407636 0.0133148  0.04480263 0.04530001 0.05068502]\n",
      "Weight=2, C=2.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.31578106 0.68421894]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2, C=2.0, Method=lasso: Score=0.750\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  1  3 11 10] with weights [0.08338947 0.09972653 0.11888635 0.15797155 0.16685551]\n",
      "Bottom 5 features: [9 4 8 2 5] with weights [0.01827592 0.04874903 0.04935377 0.05544346 0.05934138]\n",
      "Weight=2.5, C=0.3, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  8  9 11 10] with weights [0. 0. 0. 0. 1.]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2.5, C=0.3, Method=lasso: Score=0.250\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  1  3 10 11] with weights [0.10516378 0.11277461 0.12408819 0.13523104 0.15313917]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.01045882 0.04417683 0.04734822 0.04914097 0.05438834]\n",
      "Weight=2.5, C=0.6, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11  6 10] with weights [0.         0.         0.         0.49687055 0.50312945]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2.5, C=0.6, Method=lasso: Score=0.750\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [10  1  7  3 11] with weights [0.11218393 0.12313365 0.12541654 0.12776236 0.14760785]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.00394429 0.03058658 0.04663258 0.04792396 0.05305955]\n",
      "Weight=2.5, C=1.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.37547932 0.62452068]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2.5, C=1.0, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1  7 11] with weights [0.10348467 0.13006446 0.1306713  0.14123421 0.1422982 ]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.0009861  0.02012665 0.04598412 0.04631707 0.05178584]\n",
      "Weight=2.5, C=1.5, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.33397279 0.66602721]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2.5, C=1.5, Method=lasso: Score=0.750\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1 11  7] with weights [0.10606725 0.13079086 0.13481456 0.13785473 0.15103053]\n",
      "Bottom 5 features: [9 6 8 4 2] with weights [0.00407636 0.0133148  0.04480263 0.04530001 0.05068502]\n",
      "Weight=2.5, C=2.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.31584406 0.68415594]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2.5, C=2.0, Method=lasso: Score=0.750\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  1  3 11 10] with weights [0.08338947 0.09972653 0.11888635 0.15797155 0.16685551]\n",
      "Bottom 5 features: [9 4 8 2 5] with weights [0.01827592 0.04874903 0.04935377 0.05544346 0.05934138]\n",
      "Weight=3, C=0.3, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  8  9 11 10] with weights [0. 0. 0. 0. 1.]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3, C=0.3, Method=lasso: Score=0.250\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  1  3 10 11] with weights [0.10516378 0.11277461 0.12408819 0.13523104 0.15313917]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.01045882 0.04417683 0.04734822 0.04914097 0.05438834]\n",
      "Weight=3, C=0.6, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11  6 10] with weights [0.         0.         0.         0.49655864 0.50344136]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3, C=0.6, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [10  1  7  3 11] with weights [0.11218393 0.12313365 0.12541654 0.12776236 0.14760785]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.00394429 0.03058658 0.04663258 0.04792396 0.05305955]\n",
      "Weight=3, C=1.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.37522876 0.62477124]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3, C=1.0, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1  7 11] with weights [0.10348467 0.13006446 0.1306713  0.14123421 0.1422982 ]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.0009861  0.02012665 0.04598412 0.04631707 0.05178584]\n",
      "Weight=3, C=1.5, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.33397121 0.66602879]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3, C=1.5, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1 11  7] with weights [0.10606725 0.13079086 0.13481456 0.13785473 0.15103053]\n",
      "Bottom 5 features: [9 6 8 4 2] with weights [0.00407636 0.0133148  0.04480263 0.04530001 0.05068502]\n",
      "Weight=3, C=2.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.31577017 0.68422983]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3, C=2.0, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  1  3 11 10] with weights [0.08338947 0.09972653 0.11888635 0.15797155 0.16685551]\n",
      "Bottom 5 features: [9 4 8 2 5] with weights [0.01827592 0.04874903 0.04935377 0.05544346 0.05934138]\n",
      "Weight=3.5, C=0.3, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  8  9 11 10] with weights [0. 0. 0. 0. 1.]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3.5, C=0.3, Method=lasso: Score=0.250\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  1  3 10 11] with weights [0.10516378 0.11277461 0.12408819 0.13523104 0.15313917]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.01045882 0.04417683 0.04734822 0.04914097 0.05438834]\n",
      "Weight=3.5, C=0.6, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11  6 10] with weights [0.         0.         0.         0.49662244 0.50337756]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3.5, C=0.6, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [10  1  7  3 11] with weights [0.11218393 0.12313365 0.12541654 0.12776236 0.14760785]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.00394429 0.03058658 0.04663258 0.04792396 0.05305955]\n",
      "Weight=3.5, C=1.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.37522199 0.62477801]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3.5, C=1.0, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1  7 11] with weights [0.10348467 0.13006446 0.1306713  0.14123421 0.1422982 ]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.0009861  0.02012665 0.04598412 0.04631707 0.05178584]\n",
      "Weight=3.5, C=1.5, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.33397077 0.66602923]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3.5, C=1.5, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1 11  7] with weights [0.10606725 0.13079086 0.13481456 0.13785473 0.15103053]\n",
      "Bottom 5 features: [9 6 8 4 2] with weights [0.00407636 0.0133148  0.04480263 0.04530001 0.05068502]\n",
      "Weight=3.5, C=2.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.31580506 0.68419494]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3.5, C=2.0, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  1  3 11 10] with weights [0.08338947 0.09972653 0.11888635 0.15797155 0.16685551]\n",
      "Bottom 5 features: [9 4 8 2 5] with weights [0.01827592 0.04874903 0.04935377 0.05544346 0.05934138]\n",
      "Weight=4, C=0.3, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  8  9 11 10] with weights [0. 0. 0. 0. 1.]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=4, C=0.3, Method=lasso: Score=0.250\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  1  3 10 11] with weights [0.10516378 0.11277461 0.12408819 0.13523104 0.15313917]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.01045882 0.04417683 0.04734822 0.04914097 0.05438834]\n",
      "Weight=4, C=0.6, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11  6 10] with weights [0.         0.         0.         0.49685146 0.50314854]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=4, C=0.6, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [10  1  7  3 11] with weights [0.11218393 0.12313365 0.12541654 0.12776236 0.14760785]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.00394429 0.03058658 0.04663258 0.04792396 0.05305955]\n",
      "Weight=4, C=1.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.37549448 0.62450552]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=4, C=1.0, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1  7 11] with weights [0.10348467 0.13006446 0.1306713  0.14123421 0.1422982 ]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.0009861  0.02012665 0.04598412 0.04631707 0.05178584]\n",
      "Weight=4, C=1.5, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.33399959 0.66600041]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=4, C=1.5, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1 11  7] with weights [0.10606725 0.13079086 0.13481456 0.13785473 0.15103053]\n",
      "Bottom 5 features: [9 6 8 4 2] with weights [0.00407636 0.0133148  0.04480263 0.04530001 0.05068502]\n",
      "Weight=4, C=2.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.31584667 0.68415333]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=4, C=2.0, Method=lasso: Score=0.875\n",
      "\n",
      "Top 5 parameter combinations:\n",
      "1. Weight=2, C=0.3, Method=ridge: Score=0.875\n",
      "2. Weight=2, C=0.6, Method=ridge: Score=0.875\n",
      "3. Weight=2, C=1.0, Method=ridge: Score=0.875\n",
      "4. Weight=2, C=1.0, Method=lasso: Score=0.875\n",
      "5. Weight=2, C=1.5, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 6  1  7 11 10] with weights [0.10835225 0.1141869  0.11970669 0.12216692 0.13781145]\n",
      "Bottom 5 features: [4 9 8 3 2] with weights [0.00085934 0.00259358 0.03182316 0.07284187 0.08986453]\n",
      "\n",
      "Final Model Summary:\n",
      "Base training examples: 5714\n",
      "Total user examples: 15\n",
      "\n",
      "Best parameters:\n",
      "Base KNN parameters: {'metric': 'euclidean', 'n_neighbors': 11, 'weights': 'uniform'}\n",
      "Feature weighting parameters: {'weight_factor': 2, 'C': 0.3, 'method': 'ridge'}\n",
      "\n",
      "Model is ready to use!\n",
      "Class labels: ['hhc' 'hho' 'kd' 'sd']\n"
     ]
    }
   ],
   "source": [
    "def create_feature_weighted_knn(X_train, y_train, X_user, y_user, weight_factor=3.0, \n",
    "                              feature_weight_method='ridge', C=1.0, **knn_params):\n",
    "    \"\"\"\n",
    "    Create a KNN model with learned feature weights from user examples.\n",
    "    \"\"\"\n",
    "    # Learn feature weights from user examples\n",
    "    if feature_weight_method == 'ridge':\n",
    "        model = RidgeClassifier(alpha=1.0/C)\n",
    "    elif feature_weight_method == 'lasso':\n",
    "        model = LogisticRegression(penalty='l1', C=C, solver='liblinear')\n",
    "    else:\n",
    "        model = LogisticRegression(penalty='l1', C=C, solver='liblinear')\n",
    "    \n",
    "    # Fit the model to user examples\n",
    "    model.fit(X_user, y_user)\n",
    "    \n",
    "    # Get feature weights\n",
    "    if hasattr(model, 'coef_'):\n",
    "        if len(model.coef_.shape) == 2:\n",
    "            feature_weights = np.mean(np.abs(model.coef_), axis=0)\n",
    "        else:\n",
    "            feature_weights = np.abs(model.coef_)\n",
    "    else:\n",
    "        feature_weights = np.ones(X_user.shape[1])\n",
    "    \n",
    "    # Normalize weights to sum to 1\n",
    "    feature_weights = feature_weights / np.sum(feature_weights)\n",
    "    \n",
    "    # Print top and bottom features\n",
    "    top_indices = np.argsort(feature_weights)[-5:]\n",
    "    bottom_indices = np.argsort(feature_weights)[:5]\n",
    "    \n",
    "    print(\"\\nFeature weights learned from user examples:\")\n",
    "    print(f\"Top 5 features: {top_indices} with weights {feature_weights[top_indices]}\")\n",
    "    print(f\"Bottom 5 features: {bottom_indices} with weights {feature_weights[bottom_indices]}\")\n",
    "    \n",
    "    # Repeat user examples\n",
    "    n_repeats = int(weight_factor)\n",
    "    X_user_repeated = np.repeat(X_user, n_repeats, axis=0)\n",
    "    y_user_repeated = np.repeat(y_user, n_repeats)\n",
    "    \n",
    "    # Combine with training data\n",
    "    X_combined = np.vstack([X_train, X_user_repeated])\n",
    "    y_combined = np.concatenate([y_train, y_user_repeated])\n",
    "    \n",
    "    # Define weighted distance function\n",
    "    def weighted_euclidean(x, y):\n",
    "        return np.sqrt(np.sum(feature_weights * ((x - y) ** 2)))\n",
    "    \n",
    "    # Create KNN with custom metric\n",
    "    params = knn_params.copy()\n",
    "    params['metric'] = 'pyfunc'\n",
    "    params['metric_params'] = {'func': weighted_euclidean}\n",
    "    \n",
    "    knn = KNeighborsClassifier(**params)\n",
    "    knn.fit(X_combined, y_combined)\n",
    "    \n",
    "    return knn, feature_weights\n",
    "\n",
    "def grid_search_hyperparameters(X_train, y_train, X_user, y_user, X_val, y_val, base_params):\n",
    "    \"\"\"\n",
    "    Grid search for weight factors and regularization parameters.\n",
    "    Validation is done only on user validation set.\n",
    "    \"\"\"\n",
    "    weight_factors = [2, 2.5, 3, 3.5, 4]\n",
    "    C_values = [0.3, 0.6, 1.0, 1.5, 2.0]\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\nGrid searching weight factors and regularization parameters...\")\n",
    "    for weight_factor in weight_factors:\n",
    "        for C in C_values:\n",
    "            for method in ['ridge', 'lasso']:\n",
    "                try:\n",
    "                    model, _ = create_feature_weighted_knn(\n",
    "                        X_train, y_train, X_user, y_user,\n",
    "                        weight_factor=weight_factor,\n",
    "                        feature_weight_method=method,\n",
    "                        C=C,\n",
    "                        **base_params\n",
    "                    )\n",
    "                    \n",
    "                    # Evaluate on validation set\n",
    "                    score = model.score(X_val, y_val)\n",
    "                    results.append({\n",
    "                        'weight_factor': weight_factor,\n",
    "                        'C': C,\n",
    "                        'method': method,\n",
    "                        'score': score\n",
    "                    })\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_params = {\n",
    "                            'weight_factor': weight_factor,\n",
    "                            'C': C,\n",
    "                            'method': method\n",
    "                        }\n",
    "                    \n",
    "                    print(f\"Weight={weight_factor}, C={C}, Method={method}: Score={score:.3f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed for weight={weight_factor}, C={C}, Method={method}: {str(e)}\")\n",
    "    \n",
    "    # Sort and print results\n",
    "    results.sort(key=lambda x: x['score'], reverse=True)\n",
    "    print(\"\\nTop 5 parameter combinations:\")\n",
    "    for i, result in enumerate(results[:5], 1):\n",
    "        print(f\"{i}. Weight={result['weight_factor']}, C={result['C']}, \"\n",
    "              f\"Method={result['method']}: Score={result['score']:.3f}\")\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "def create_hybrid_knn_model(base_features_path, base_labels_path, user_features_paths, user_labels_paths):\n",
    "    \"\"\"\n",
    "    Create a hybrid KNN model using validation strategy from knn_hybrid.py\n",
    "    \"\"\"\n",
    "    # Load base training data\n",
    "    X_train = np.load(base_features_path)\n",
    "    y_train = np.load(base_labels_path)\n",
    "    \n",
    "    # Load user examples and their labels\n",
    "    X_user_list = []\n",
    "    y_user_list = []\n",
    "    for feature_path, label_path in zip(user_features_paths.values(), user_labels_paths.values()):\n",
    "        features = np.load(feature_path)\n",
    "        labels = np.load(label_path)\n",
    "        X_user_list.append(features)\n",
    "        y_user_list.append(labels)\n",
    "    \n",
    "    X_user = np.vstack(X_user_list)\n",
    "    y_user = np.concatenate(y_user_list)\n",
    "    \n",
    "    # Split user examples into adaptation and validation sets\n",
    "    examples_per_class = 5\n",
    "    adapt_indices = []\n",
    "    val_indices = []\n",
    "    \n",
    "    for label in np.unique(y_user):\n",
    "        label_indices = np.where(y_user == label)[0]\n",
    "        if len(label_indices) <= examples_per_class * 2:\n",
    "            n_adapt = len(label_indices) // 2\n",
    "        else:\n",
    "            n_adapt = examples_per_class\n",
    "            \n",
    "        np.random.seed(42)\n",
    "        selected_indices = np.random.choice(label_indices, n_adapt, replace=False)\n",
    "        adapt_indices.extend(selected_indices)\n",
    "        val_indices.extend([idx for idx in label_indices if idx not in selected_indices])\n",
    "    \n",
    "    # Split user data into adaptation and validation sets\n",
    "    X_adapt = X_user[adapt_indices]\n",
    "    y_adapt = y_user[adapt_indices]\n",
    "    X_user_val = X_user[val_indices]\n",
    "    y_user_val = y_user[val_indices]\n",
    "    \n",
    "    print(\"\\nData split summary:\")\n",
    "    print(f\"Base training examples: {len(X_train)}\")\n",
    "    print(f\"User adaptation examples: {len(X_adapt)}\")\n",
    "    print(f\"User validation examples: {len(X_user_val)}\")\n",
    "    \n",
    "    print(\"\\nAdaptation set label distribution:\")\n",
    "    print(pd.Series(y_adapt).value_counts())\n",
    "    print(\"\\nValidation set label distribution:\")\n",
    "    print(pd.Series(y_user_val).value_counts())\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_adapt_scaled = scaler.transform(X_adapt)\n",
    "    X_user_val_scaled = scaler.transform(X_user_val)\n",
    "    X_user_scaled = scaler.transform(X_user)  # Scale all user data for final model\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_train_enc = le.fit_transform(y_train)\n",
    "    y_adapt_enc = le.transform(y_adapt)\n",
    "    y_user_val_enc = le.transform(y_user_val)\n",
    "    y_user_enc = le.transform(y_user)  # Encode all user labels for final model\n",
    "    \n",
    "    # First find best base model parameters\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "    \n",
    "    print(\"\\nPerforming grid search for base model parameters...\")\n",
    "    base_knn = KNeighborsClassifier()\n",
    "    grid_search = GridSearchCV(\n",
    "        base_knn,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train_scaled, y_train_enc)\n",
    "    \n",
    "    print(\"\\nBest base model parameters:\", grid_search.best_params_)\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\n",
    "    \n",
    "    # Grid search for feature weighting parameters\n",
    "    best_params, best_score = grid_search_hyperparameters(\n",
    "        X_train_scaled, y_train_enc,\n",
    "        X_adapt_scaled, y_adapt_enc,\n",
    "        X_user_val_scaled, y_user_val_enc,\n",
    "        grid_search.best_params_\n",
    "    )\n",
    "    \n",
    "    # Create final model with best parameters using ALL data\n",
    "    final_model, feature_weights = create_feature_weighted_knn(\n",
    "        X_train_scaled, y_train_enc,\n",
    "        X_user_scaled, y_user_enc,  # Use all user examples\n",
    "        weight_factor=best_params['weight_factor'],\n",
    "        feature_weight_method=best_params['method'],\n",
    "        C=best_params['C'],\n",
    "        **grid_search.best_params_\n",
    "    )\n",
    "    \n",
    "    # Visualize feature weights\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(feature_weights)), feature_weights)\n",
    "    plt.title('Feature Weights in Final Model')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Weight')\n",
    "    \n",
    "    # Save visualization\n",
    "    vis_dir = \"/Users/arul/ML/BEATBOX/projectFiles/visualization/hybridknn\"\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(vis_dir, 'final_feature_weights.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\nFinal Model Summary:\")\n",
    "    print(f\"Base training examples: {len(X_train)}\")\n",
    "    print(f\"Total user examples: {len(X_user)}\")\n",
    "    print(\"\\nBest parameters:\")\n",
    "    print(f\"Base KNN parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Feature weighting parameters: {best_params}\")\n",
    "    \n",
    "    return final_model, scaler, le, grid_search.best_params_, best_params, feature_weights\n",
    "\n",
    "def predict_drum_type(model, scaler, label_encoder, features):\n",
    "    \"\"\"\n",
    "    Predict drum types for new features using the hybrid model.\n",
    "    Returns both predictions and probabilities.\n",
    "    \"\"\"\n",
    "    features_scaled = scaler.transform(features)\n",
    "    predictions_enc = model.predict(features_scaled)\n",
    "    predictions = label_encoder.inverse_transform(predictions_enc)\n",
    "    probabilities = model.predict_proba(features_scaled)\n",
    "    \n",
    "    return predictions, probabilities\n",
    "\n",
    "def main():\n",
    "    base_features_path = \"/Users/arul/ML/BEATBOX/projectFiles/extracted_features/features/mfcc_extracted_features.npy\"\n",
    "    base_labels_path = \"/Users/arul/ML/BEATBOX/projectFiles/extracted_features/labels/mfcc_extracted_labels.npy\"\n",
    "    \n",
    "    # Paths for both features and labels\n",
    "    user_features_paths = {\n",
    "        'kick': '/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/extracted_features/features/kick_features.npy',\n",
    "        'snare': '/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/extracted_features/features/snare_features.npy'\n",
    "    }\n",
    "    \n",
    "    user_labels_paths = {\n",
    "        'kick': '/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/extracted_features/labels/kick_labels.npy',\n",
    "        'snare': '/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/extracted_features/labels/snare_labels.npy'\n",
    "    }\n",
    "    \n",
    "    model, scaler, label_encoder, base_params, feature_params, feature_weights = create_hybrid_knn_model(\n",
    "        base_features_path=base_features_path,\n",
    "        base_labels_path=base_labels_path,\n",
    "        user_features_paths=user_features_paths,\n",
    "        user_labels_paths=user_labels_paths\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel is ready to use!\")\n",
    "    print(\"Class labels:\", label_encoder.classes_)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segmenting audio file: /Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/audio/sample1/vox/full_vocal.wav\n",
      "Found 22 onsets\n",
      "\n",
      "Processing complete:\n",
      "Total segments created: 22\n",
      "Segments saved in: /Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/audio/sample1/segments/full_segments\n",
      "CSV file saved as: /Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/csv_info/full_segment_info.csv\n",
      "Onset detection plot saved as: /Users/arul/ML/BEATBOX/projectFiles/visualization/final_pipeline/onset_detection.png\n",
      "\n",
      "Please manually fill in the 'instrument_label' column with:\n",
      "- 'kd' for kick drum\n",
      "- 'sd' for snare drum\n",
      "- 'hhc' for closed hi-hat\n",
      "- 'hho' for open hi-hat\n"
     ]
    }
   ],
   "source": [
    "def segment_full_recording(input_file, output_dir, segment_info_path):\n",
    "    \"\"\"\n",
    "    Segment a full vocal percussion recording with parameters optimized for detecting rapid successive onsets.\n",
    "    \"\"\"\n",
    "    # Create output directories if they don't exist\n",
    "    segments_dir = Path(output_dir)  # Now using the exact path specified\n",
    "    segments_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create visualization directory\n",
    "    vis_dir = Path(\"/Users/arul/ML/BEATBOX/projectFiles/visualization/final_pipeline\")\n",
    "    vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(input_file)\n",
    "    \n",
    "    # Parameters optimized for detecting close successive onsets\n",
    "    onset_frames = librosa.onset.onset_detect(\n",
    "        y=y, \n",
    "        sr=sr,\n",
    "        units='frames',\n",
    "        hop_length=256,\n",
    "        backtrack=True,\n",
    "        pre_max=3,\n",
    "        post_max=3,\n",
    "        pre_avg=15,\n",
    "        post_avg=15,\n",
    "        delta=0.07,\n",
    "        wait=5\n",
    "    )\n",
    "    \n",
    "    # Visualize the onset detection\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot waveform\n",
    "    times = np.arange(len(y)) / sr\n",
    "    plt.plot(times, y, alpha=0.6, label='Waveform')\n",
    "    \n",
    "    # Plot onset markers\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr=sr, hop_length=256)\n",
    "    plt.vlines(onset_times, -1, 1, color='r', label='Onsets')\n",
    "    \n",
    "    plt.title('Waveform and Detected Onsets')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot in the visualization directory\n",
    "    plt.savefig(vis_dir / 'onset_detection.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Convert frames to samples\n",
    "    onset_samples = librosa.frames_to_samples(onset_frames, hop_length=256)\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr=sr, hop_length=256)\n",
    "    \n",
    "    # Initialize lists for segment info\n",
    "    segment_paths = []\n",
    "    start_times = []\n",
    "    end_times = []\n",
    "    \n",
    "    # Segment duration in seconds\n",
    "    segment_duration = 0.25\n",
    "    samples_per_segment = int(segment_duration * sr)\n",
    "    \n",
    "    print(f\"\\nSegmenting audio file: {input_file}\")\n",
    "    print(f\"Found {len(onset_frames)} onsets\")\n",
    "    \n",
    "    # Process each onset\n",
    "    for i, (onset_sample, onset_time) in enumerate(zip(onset_samples, onset_times)):\n",
    "        # Calculate segment boundaries\n",
    "        start_sample = max(0, onset_sample - int(0.05 * sr))\n",
    "        end_sample = min(len(y), start_sample + samples_per_segment)\n",
    "        \n",
    "        # Extract segment\n",
    "        segment = y[start_sample:end_sample]\n",
    "        \n",
    "        # Pad if necessary\n",
    "        if len(segment) < samples_per_segment:\n",
    "            segment = np.pad(segment, (0, samples_per_segment - len(segment)))\n",
    "        \n",
    "        # Generate segment filename\n",
    "        segment_filename = f'segment_{i:04d}.wav'\n",
    "        segment_path = segments_dir / segment_filename  # Save directly in output_dir\n",
    "        \n",
    "        # Save segment\n",
    "        sf.write(segment_path, segment, sr)\n",
    "        \n",
    "        # Store segment information\n",
    "        segment_paths.append(str(segment_path))\n",
    "        start_times.append(onset_time)\n",
    "        end_times.append(onset_time + segment_duration)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    segment_info = pd.DataFrame({\n",
    "        'segment_path': segment_paths,\n",
    "        'start_time': start_times,\n",
    "        'end_time': end_times,\n",
    "        'instrument_label': ''\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    segment_info.to_csv(segment_info_path, index=False)\n",
    "    \n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"Total segments created: {len(segment_paths)}\")\n",
    "    print(f\"Segments saved in: {segments_dir}\")\n",
    "    print(f\"CSV file saved as: {segment_info_path}\")\n",
    "    print(f\"Onset detection plot saved as: {vis_dir}/onset_detection.png\")\n",
    "    print(\"\\nPlease manually fill in the 'instrument_label' column with:\")\n",
    "    print(\"- 'kd' for kick drum\")\n",
    "    print(\"- 'sd' for snare drum\")\n",
    "    print(\"- 'hhc' for closed hi-hat\")\n",
    "    print(\"- 'hho' for open hi-hat\")\n",
    "    \n",
    "    return segment_info\n",
    "\n",
    "# Example usage\n",
    "input_file = \"/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/audio/sample1/vox/full_vocal.wav\"\n",
    "output_dir = \"/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/audio/sample1/segments/full_segments\"\n",
    "segment_info_path = \"/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/csv_info/full_segment_info.csv\"\n",
    "\n",
    "segment_info = segment_full_recording(input_file, output_dir, segment_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting expanded feature set from ../audio/sample1/segments/full_segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 157.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction complete:\n",
      "Successfully processed: 22 segments\n",
      "Feature vector shape: (22, 12)\n",
      "\n",
      "Feature set includes:\n",
      "- Selected MFCCs and their deltas\n",
      "- Envelope descriptors (flatness, temporal centroid, pre-max derivative)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_features, full_labels = extract_features_from_user_input(\n",
    "    segment_info_path='../csv_info/full_segment_info.csv',\n",
    "    segments_dir='../audio/sample1/segments/full_segments',\n",
    "    features_output_dir='../extracted_features/features/full_features.npy',\n",
    "    labels_output_dir='../extracted_features/labels/full_labels.npy'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
