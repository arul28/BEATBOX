{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing User Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from scipy.signal import butter, filtfilt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(y, sr):\n",
    "    nyq = sr / 2\n",
    "    cutoff = 60\n",
    "    order = 4\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    y_filtered = filtfilt(b, a, y)\n",
    "    \n",
    "    y_normalized = librosa.util.normalize(y_filtered)\n",
    "    \n",
    "    y_cleaned = librosa.effects.trim(\n",
    "        y_normalized,\n",
    "        top_db=30,\n",
    "        frame_length=2048,\n",
    "        hop_length=512\n",
    "    )[0]\n",
    "    \n",
    "    S = librosa.stft(y_cleaned)\n",
    "    mag = np.abs(S)\n",
    "    phase = np.angle(S)\n",
    "\n",
    "    noise_floor = np.mean(np.min(mag, axis=1))\n",
    "    threshold = noise_floor * 2\n",
    "    \n",
    "    mag_gated = np.maximum(mag - threshold, 0)\n",
    "    y_denoised = librosa.istft(mag_gated * np.exp(1j * phase))\n",
    "    \n",
    "    y_final = librosa.util.normalize(y_denoised)\n",
    "    \n",
    "    return y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def segment_user_input(input_file, segments_output_dir, csv_output_dir, drum_type, segment_duration=0.5):\n",
    "    \"\"\"\n",
    "    Segment a user's drum recording based on onset detection.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to the input wav file (snare.wav or kick.wav)\n",
    "        segments_output_dir: Directory to save the segmented audio files\n",
    "        csv_output_dir: Directory to save the segment info CSV\n",
    "        drum_type: Type of drum ('sd' for snare, 'kd' for kick)\n",
    "        segment_duration: Duration of each segment in seconds\n",
    "    \"\"\"\n",
    "    # Create output directories\n",
    "    segments_dir = Path(segments_output_dir) / f\"{drum_type}_segments\"\n",
    "    segments_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    csv_dir = Path(csv_output_dir)\n",
    "    csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load and preprocess audio file\n",
    "    y, sr = librosa.load(input_file, sr=None)  # Keep original sample rate\n",
    "    \n",
    "    # Clean up the audio\n",
    "    print(\"Cleaning up audio...\")\n",
    "    y_cleaned = preprocess_audio(y, sr)\n",
    "    \n",
    "    # Save preprocessed full audio for reference\n",
    "    preprocessed_path = Path(input_file).parent / f\"{Path(input_file).stem}_cleaned.wav\"\n",
    "    sf.write(preprocessed_path, y_cleaned, sr)\n",
    "    print(f\"Saved cleaned audio to: {preprocessed_path}\")\n",
    "    \n",
    "    # Detect onsets on cleaned audio\n",
    "    onset_frames = librosa.onset.onset_detect(\n",
    "        y=y_cleaned, \n",
    "        sr=sr,\n",
    "        units='frames',\n",
    "        hop_length=512,\n",
    "        backtrack=True,\n",
    "        pre_max=20,\n",
    "        post_max=20,\n",
    "        pre_avg=100,\n",
    "        post_avg=100,\n",
    "        delta=0.2,\n",
    "        wait=30\n",
    "    )\n",
    "    \n",
    "    # Convert frames to time\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "    \n",
    "    segment_info = []\n",
    "    \n",
    "    # Process each onset\n",
    "    for i, onset_time in enumerate(onset_times):\n",
    "        start_sample = int(onset_time * sr)\n",
    "        end_sample = start_sample + int(segment_duration * sr)\n",
    "        \n",
    "        if start_sample < 0:\n",
    "            start_sample = 0\n",
    "        if end_sample > len(y_cleaned):\n",
    "            end_sample = len(y_cleaned)\n",
    "            \n",
    "        if end_sample > start_sample:\n",
    "            segment = y_cleaned[start_sample:end_sample]\n",
    "            \n",
    "            if len(segment) < int(segment_duration * sr):\n",
    "                segment = np.pad(\n",
    "                    segment,\n",
    "                    (0, int(segment_duration * sr) - len(segment)),\n",
    "                    mode='constant'\n",
    "                )\n",
    "            \n",
    "            segment_filename = f\"user_{drum_type}_{i:04d}.wav\"\n",
    "            segment_path = segments_dir / segment_filename\n",
    "            \n",
    "            sf.write(str(segment_path), segment, sr)\n",
    "            \n",
    "            segment_info.append({\n",
    "                'segment_path': str(segment_path),\n",
    "                'instrument_label': drum_type,\n",
    "                'onset_time': onset_time,\n",
    "                'original_wav': str(input_file)\n",
    "            })\n",
    "    \n",
    "    segment_df = pd.DataFrame(segment_info)\n",
    "    csv_path = csv_dir / f\"{drum_type}_segment_info.csv\"\n",
    "    segment_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"\\nSegmentation Summary for {drum_type}:\")\n",
    "    print(f\"Total segments extracted: {len(segment_df)}\")\n",
    "    print(f\"Segments saved in: {segments_dir}\")\n",
    "    print(f\"CSV saved as: {csv_path}\")\n",
    "    \n",
    "    return segment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up audio...\n",
      "Saved cleaned audio to: ../audio/sample1/vox/snare_cleaned.wav\n",
      "\n",
      "Segmentation Summary for sd:\n",
      "Total segments extracted: 8\n",
      "Segments saved in: ../audio/sample1/segments/sd_segments\n",
      "CSV saved as: ../csv_info/sd_segment_info.csv\n"
     ]
    }
   ],
   "source": [
    "snare_df = segment_user_input(\n",
    "    input_file=\"../audio/sample1/vox/snare.wav\",\n",
    "    segments_output_dir=\"../audio/sample1/segments\",\n",
    "    csv_output_dir=\"../csv_info\",\n",
    "    drum_type=\"sd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up audio...\n",
      "Saved cleaned audio to: ../audio/sample1/vox/kick_cleaned.wav\n",
      "\n",
      "Segmentation Summary for kd:\n",
      "Total segments extracted: 7\n",
      "Segments saved in: ../audio/sample1/segments/kd_segments\n",
      "CSV saved as: ../csv_info/kd_segment_info.csv\n"
     ]
    }
   ],
   "source": [
    "kick_df = segment_user_input(\n",
    "    input_file=\"../audio/sample1/vox/kick.wav\",\n",
    "    segments_output_dir=\"../audio/sample1/segments\",\n",
    "    csv_output_dir=\"../csv_info\",\n",
    "    drum_type=\"kd\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_user_input(segment_info_path, segments_dir, n_mfcc=14, features_output_dir=None, labels_output_dir=None):\n",
    "    \"\"\"\n",
    "    Extract the same features as the original pipeline: MFCCs, deltas, and envelope descriptors.\n",
    "    Then select the most important features based on the original analysis.\n",
    "    \n",
    "    Args:\n",
    "        segment_info_path: Path to the CSV containing segment information\n",
    "        segments_dir: Directory containing the audio segments\n",
    "        n_mfcc: Number of MFCC coefficients (default 14 as in original)\n",
    "        features_output_dir: Where to save the features\n",
    "        labels_output_dir: Where to save the labels\n",
    "    \"\"\"\n",
    "    # Load segment info\n",
    "    metadata = pd.read_csv(segment_info_path)\n",
    "    \n",
    "    # Initialize arrays to store features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"Extracting expanded feature set from {segments_dir}...\")\n",
    "    for idx, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "        try:\n",
    "            # Load audio segment\n",
    "            y, sr = librosa.load(row['segment_path'])\n",
    "            \n",
    "            # Skip if empty\n",
    "            if len(y) == 0:\n",
    "                print(f\"Skipping empty audio file: {row['segment_path']}\")\n",
    "                continue\n",
    "                \n",
    "            # 1. Extract MFCCs and their statistics\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "            mfcc_mean = np.mean(mfcc, axis=1)\n",
    "            \n",
    "            # 2. Compute MFCC deltas (first derivatives)\n",
    "            mfcc_delta = librosa.feature.delta(mfcc)\n",
    "            mfcc_delta_mean = np.mean(mfcc_delta, axis=1)\n",
    "            \n",
    "            # 3. Extract envelope-based descriptors\n",
    "            envelope = np.abs(y)\n",
    "            \n",
    "            if len(envelope) == 0:\n",
    "                print(f\"Skipping file with empty envelope: {row['segment_path']}\")\n",
    "                continue\n",
    "            \n",
    "            # Find the maximum amplitude and its position\n",
    "            max_amp_pos = np.argmax(envelope)\n",
    "            \n",
    "            # 3.1 Maximum derivative before the maximum amplitude\n",
    "            pre_max_deriv = 0\n",
    "            if max_amp_pos > 0:\n",
    "                pre_envelope = envelope[:max_amp_pos]\n",
    "                if len(pre_envelope) > 1:\n",
    "                    pre_max_deriv = np.max(np.diff(pre_envelope))\n",
    "            \n",
    "            # 3.2 Derivative after the maximum amplitude\n",
    "            post_max_deriv = 0\n",
    "            if max_amp_pos < len(envelope)-1:\n",
    "                post_envelope = envelope[max_amp_pos:]\n",
    "                if len(post_envelope) > 1:\n",
    "                    post_max_deriv = np.min(np.diff(post_envelope))\n",
    "            \n",
    "            # 3.3 Temporal centroid\n",
    "            times = np.arange(len(y))\n",
    "            env_sum = np.sum(envelope)\n",
    "            if env_sum > 0:\n",
    "                temporal_centroid = np.sum(times * envelope) / env_sum\n",
    "                temporal_centroid_ratio = temporal_centroid / len(y)\n",
    "            else:\n",
    "                temporal_centroid_ratio = 0.5\n",
    "            \n",
    "            # 3.4 Flatness coefficient\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                flatness = librosa.feature.spectral_flatness(y=y)[0].mean()\n",
    "                flatness = 0.0 if np.isnan(flatness) else flatness\n",
    "            \n",
    "            # Combine all features in the same order as original pipeline\n",
    "            feature_vector = np.concatenate([\n",
    "                mfcc_mean,                # 14 features\n",
    "                mfcc_delta_mean,          # 14 features\n",
    "                [pre_max_deriv,           # 1 feature\n",
    "                 post_max_deriv,          # 1 feature\n",
    "                 flatness,                # 1 feature\n",
    "                 temporal_centroid_ratio]  # 1 feature\n",
    "            ])\n",
    "            \n",
    "            features.append(feature_vector)\n",
    "            labels.append(row['instrument_label'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['segment_path']}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Select only the important features (same as original pipeline)\n",
    "    important_feature_indices = [\n",
    "        1,   # mfcc_2\n",
    "        15,  # delta_2\n",
    "        2,   # mfcc_3\n",
    "        3,   # mfcc_4\n",
    "        30,  # flatness\n",
    "        4,   # mfcc_5\n",
    "        6,   # mfcc_7\n",
    "        16,  # delta_3\n",
    "        9,   # mfcc_10\n",
    "        0,   # mfcc_1\n",
    "        29,  # temporal_centroid\n",
    "        28   # pre_max_deriv\n",
    "    ]\n",
    "    \n",
    "    # Select the optimized feature set\n",
    "    X_selected = X[:, important_feature_indices]\n",
    "    \n",
    "    # Save features if paths provided\n",
    "    if features_output_dir and labels_output_dir:\n",
    "        np.save(features_output_dir, X_selected)\n",
    "        np.save(labels_output_dir, y)\n",
    "    \n",
    "    print(f\"\\nFeature extraction complete:\")\n",
    "    print(f\"Successfully processed: {len(features)} segments\")\n",
    "    print(f\"Feature vector shape: {X_selected.shape}\")\n",
    "    print(\"\\nFeature set includes:\")\n",
    "    print(\"- Selected MFCCs and their deltas\")\n",
    "    print(\"- Envelope descriptors (flatness, temporal centroid, pre-max derivative)\")\n",
    "    \n",
    "    return X_selected, y\n",
    "\n",
    "# Example usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting expanded feature set from ../audio/sample1/segments/sd_segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 163.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction complete:\n",
      "Successfully processed: 8 segments\n",
      "Feature vector shape: (8, 12)\n",
      "\n",
      "Feature set includes:\n",
      "- Selected MFCCs and their deltas\n",
      "- Envelope descriptors (flatness, temporal centroid, pre-max derivative)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "snare_features, snare_labels = extract_features_from_user_input(\n",
    "    segment_info_path='../csv_info/sd_segment_info.csv',\n",
    "    segments_dir='../audio/sample1/segments/sd_segments',\n",
    "    features_output_dir='../extracted_features/features/snare_features.npy',\n",
    "    labels_output_dir='../extracted_features/labels/snare_labels.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting expanded feature set from ../audio/sample1/segments/kd_segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 165.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction complete:\n",
      "Successfully processed: 7 segments\n",
      "Feature vector shape: (7, 12)\n",
      "\n",
      "Feature set includes:\n",
      "- Selected MFCCs and their deltas\n",
      "- Envelope descriptors (flatness, temporal centroid, pre-max derivative)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "kick_features, kick_labels = extract_features_from_user_input(\n",
    "    segment_info_path='../csv_info/kd_segment_info.csv',\n",
    "    segments_dir='../audio/sample1/segments/kd_segments',\n",
    "    features_output_dir='../extracted_features/features/kick_features.npy',\n",
    "    labels_output_dir='../extracted_features/labels/kick_labels.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Saving Hybrid KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data split summary:\n",
      "Base training examples: 5714\n",
      "User adaptation examples: 7\n",
      "User validation examples: 8\n",
      "\n",
      "Adaptation set label distribution:\n",
      "sd    4\n",
      "kd    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set label distribution:\n",
      "kd    4\n",
      "sd    4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Performing grid search for base model parameters...\n",
      "\n",
      "Best base model parameters: {'metric': 'euclidean', 'n_neighbors': 11, 'weights': 'uniform'}\n",
      "Best cross-validation score: 0.514\n",
      "\n",
      "Grid searching weight factors and regularization parameters...\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  1  3 11 10] with weights [0.08338947 0.09972653 0.11888635 0.15797155 0.16685551]\n",
      "Bottom 5 features: [9 4 8 2 5] with weights [0.01827592 0.04874903 0.04935377 0.05544346 0.05934138]\n",
      "Weight=2, C=0.3, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  8  9 11 10] with weights [0. 0. 0. 0. 1.]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2, C=0.3, Method=lasso: Score=0.250\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  1  3 10 11] with weights [0.10516378 0.11277461 0.12408819 0.13523104 0.15313917]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.01045882 0.04417683 0.04734822 0.04914097 0.05438834]\n",
      "Weight=2, C=0.6, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11  6 10] with weights [0.         0.         0.         0.49661423 0.50338577]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2, C=0.6, Method=lasso: Score=0.750\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [10  1  7  3 11] with weights [0.11218393 0.12313365 0.12541654 0.12776236 0.14760785]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.00394429 0.03058658 0.04663258 0.04792396 0.05305955]\n",
      "Weight=2, C=1.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.37548804 0.62451196]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2, C=1.0, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1  7 11] with weights [0.10348467 0.13006446 0.1306713  0.14123421 0.1422982 ]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.0009861  0.02012665 0.04598412 0.04631707 0.05178584]\n",
      "Weight=2, C=1.5, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.       0.       0.       0.334098 0.665902]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2, C=1.5, Method=lasso: Score=0.750\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1 11  7] with weights [0.10606725 0.13079086 0.13481456 0.13785473 0.15103053]\n",
      "Bottom 5 features: [9 6 8 4 2] with weights [0.00407636 0.0133148  0.04480263 0.04530001 0.05068502]\n",
      "Weight=2, C=2.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.31578106 0.68421894]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2, C=2.0, Method=lasso: Score=0.750\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  1  3 11 10] with weights [0.08338947 0.09972653 0.11888635 0.15797155 0.16685551]\n",
      "Bottom 5 features: [9 4 8 2 5] with weights [0.01827592 0.04874903 0.04935377 0.05544346 0.05934138]\n",
      "Weight=2.5, C=0.3, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  8  9 11 10] with weights [0. 0. 0. 0. 1.]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2.5, C=0.3, Method=lasso: Score=0.250\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  1  3 10 11] with weights [0.10516378 0.11277461 0.12408819 0.13523104 0.15313917]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.01045882 0.04417683 0.04734822 0.04914097 0.05438834]\n",
      "Weight=2.5, C=0.6, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11  6 10] with weights [0.         0.         0.         0.49687055 0.50312945]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2.5, C=0.6, Method=lasso: Score=0.750\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [10  1  7  3 11] with weights [0.11218393 0.12313365 0.12541654 0.12776236 0.14760785]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.00394429 0.03058658 0.04663258 0.04792396 0.05305955]\n",
      "Weight=2.5, C=1.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.37547932 0.62452068]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2.5, C=1.0, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1  7 11] with weights [0.10348467 0.13006446 0.1306713  0.14123421 0.1422982 ]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.0009861  0.02012665 0.04598412 0.04631707 0.05178584]\n",
      "Weight=2.5, C=1.5, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.33397279 0.66602721]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2.5, C=1.5, Method=lasso: Score=0.750\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1 11  7] with weights [0.10606725 0.13079086 0.13481456 0.13785473 0.15103053]\n",
      "Bottom 5 features: [9 6 8 4 2] with weights [0.00407636 0.0133148  0.04480263 0.04530001 0.05068502]\n",
      "Weight=2.5, C=2.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.31584406 0.68415594]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=2.5, C=2.0, Method=lasso: Score=0.750\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  1  3 11 10] with weights [0.08338947 0.09972653 0.11888635 0.15797155 0.16685551]\n",
      "Bottom 5 features: [9 4 8 2 5] with weights [0.01827592 0.04874903 0.04935377 0.05544346 0.05934138]\n",
      "Weight=3, C=0.3, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  8  9 11 10] with weights [0. 0. 0. 0. 1.]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3, C=0.3, Method=lasso: Score=0.250\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  1  3 10 11] with weights [0.10516378 0.11277461 0.12408819 0.13523104 0.15313917]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.01045882 0.04417683 0.04734822 0.04914097 0.05438834]\n",
      "Weight=3, C=0.6, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11  6 10] with weights [0.         0.         0.         0.49655864 0.50344136]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3, C=0.6, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [10  1  7  3 11] with weights [0.11218393 0.12313365 0.12541654 0.12776236 0.14760785]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.00394429 0.03058658 0.04663258 0.04792396 0.05305955]\n",
      "Weight=3, C=1.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.37522876 0.62477124]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3, C=1.0, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1  7 11] with weights [0.10348467 0.13006446 0.1306713  0.14123421 0.1422982 ]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.0009861  0.02012665 0.04598412 0.04631707 0.05178584]\n",
      "Weight=3, C=1.5, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.33397121 0.66602879]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3, C=1.5, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1 11  7] with weights [0.10606725 0.13079086 0.13481456 0.13785473 0.15103053]\n",
      "Bottom 5 features: [9 6 8 4 2] with weights [0.00407636 0.0133148  0.04480263 0.04530001 0.05068502]\n",
      "Weight=3, C=2.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.31577017 0.68422983]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3, C=2.0, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  1  3 11 10] with weights [0.08338947 0.09972653 0.11888635 0.15797155 0.16685551]\n",
      "Bottom 5 features: [9 4 8 2 5] with weights [0.01827592 0.04874903 0.04935377 0.05544346 0.05934138]\n",
      "Weight=3.5, C=0.3, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  8  9 11 10] with weights [0. 0. 0. 0. 1.]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3.5, C=0.3, Method=lasso: Score=0.250\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  1  3 10 11] with weights [0.10516378 0.11277461 0.12408819 0.13523104 0.15313917]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.01045882 0.04417683 0.04734822 0.04914097 0.05438834]\n",
      "Weight=3.5, C=0.6, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11  6 10] with weights [0.         0.         0.         0.49662244 0.50337756]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3.5, C=0.6, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [10  1  7  3 11] with weights [0.11218393 0.12313365 0.12541654 0.12776236 0.14760785]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.00394429 0.03058658 0.04663258 0.04792396 0.05305955]\n",
      "Weight=3.5, C=1.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.37522199 0.62477801]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3.5, C=1.0, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1  7 11] with weights [0.10348467 0.13006446 0.1306713  0.14123421 0.1422982 ]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.0009861  0.02012665 0.04598412 0.04631707 0.05178584]\n",
      "Weight=3.5, C=1.5, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.33397077 0.66602923]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3.5, C=1.5, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1 11  7] with weights [0.10606725 0.13079086 0.13481456 0.13785473 0.15103053]\n",
      "Bottom 5 features: [9 6 8 4 2] with weights [0.00407636 0.0133148  0.04480263 0.04530001 0.05068502]\n",
      "Weight=3.5, C=2.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.31580506 0.68419494]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=3.5, C=2.0, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  1  3 11 10] with weights [0.08338947 0.09972653 0.11888635 0.15797155 0.16685551]\n",
      "Bottom 5 features: [9 4 8 2 5] with weights [0.01827592 0.04874903 0.04935377 0.05544346 0.05934138]\n",
      "Weight=4, C=0.3, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  8  9 11 10] with weights [0. 0. 0. 0. 1.]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=4, C=0.3, Method=lasso: Score=0.250\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 7  1  3 10 11] with weights [0.10516378 0.11277461 0.12408819 0.13523104 0.15313917]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.01045882 0.04417683 0.04734822 0.04914097 0.05438834]\n",
      "Weight=4, C=0.6, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11  6 10] with weights [0.         0.         0.         0.49685146 0.50314854]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=4, C=0.6, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [10  1  7  3 11] with weights [0.11218393 0.12313365 0.12541654 0.12776236 0.14760785]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.00394429 0.03058658 0.04663258 0.04792396 0.05305955]\n",
      "Weight=4, C=1.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.37549448 0.62450552]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=4, C=1.0, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1  7 11] with weights [0.10348467 0.13006446 0.1306713  0.14123421 0.1422982 ]\n",
      "Bottom 5 features: [9 6 4 8 2] with weights [0.0009861  0.02012665 0.04598412 0.04631707 0.05178584]\n",
      "Weight=4, C=1.5, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.33399959 0.66600041]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=4, C=1.5, Method=lasso: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 0  3  1 11  7] with weights [0.10606725 0.13079086 0.13481456 0.13785473 0.15103053]\n",
      "Bottom 5 features: [9 6 8 4 2] with weights [0.00407636 0.0133148  0.04480263 0.04530001 0.05068502]\n",
      "Weight=4, C=2.0, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 8  9 11 10  6] with weights [0.         0.         0.         0.31584667 0.68415333]\n",
      "Bottom 5 features: [0 1 2 3 4] with weights [0. 0. 0. 0. 0.]\n",
      "Weight=4, C=2.0, Method=lasso: Score=0.875\n",
      "\n",
      "Top 5 parameter combinations:\n",
      "1. Weight=2, C=0.3, Method=ridge: Score=0.875\n",
      "2. Weight=2, C=0.6, Method=ridge: Score=0.875\n",
      "3. Weight=2, C=1.0, Method=ridge: Score=0.875\n",
      "4. Weight=2, C=1.0, Method=lasso: Score=0.875\n",
      "5. Weight=2, C=1.5, Method=ridge: Score=0.875\n",
      "\n",
      "Feature weights learned from user examples:\n",
      "Top 5 features: [ 6  1  7 11 10] with weights [0.10835225 0.1141869  0.11970669 0.12216692 0.13781145]\n",
      "Bottom 5 features: [4 9 8 3 2] with weights [0.00085934 0.00259358 0.03182316 0.07284187 0.08986453]\n",
      "\n",
      "Model components saved:\n",
      "- Model saved to: /Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/models/hybrid_knn_model.joblib\n",
      "- Scaler saved to: /Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/models/scaler.joblib\n",
      "- Label encoder saved to: /Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/models/label_encoder.joblib\n",
      "- Parameters saved to: /Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/models/model_params.joblib\n",
      "\n",
      "Model is ready to use!\n",
      "Class labels: ['hhc' 'hho' 'kd' 'sd']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from scipy.signal import butter, filtfilt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# First, define the weighted euclidean function at module level\n",
    "def weighted_euclidean(x, y, weights):\n",
    "    \"\"\"\n",
    "    Compute weighted euclidean distance between two vectors.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(weights * ((x - y) ** 2)))\n",
    "\n",
    "class WeightedKNN(KNeighborsClassifier):\n",
    "    \"\"\"\n",
    "    Custom KNN classifier that uses weighted euclidean distance.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_weights, **kwargs):\n",
    "        self.feature_weights = feature_weights\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self._fit_X = X\n",
    "        self._fit_y = y\n",
    "        return self\n",
    "        \n",
    "    def _get_weights(self, dist):\n",
    "        \"\"\"Get weights based on distances\"\"\"\n",
    "        if self.weights == 'uniform':\n",
    "            return np.ones(dist.shape)\n",
    "        elif self.weights == 'distance':\n",
    "            return 1.0 / dist\n",
    "        else:\n",
    "            raise ValueError(\"Weights must be 'uniform' or 'distance'\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distances = np.zeros((X.shape[0], self._fit_X.shape[0]))\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(self._fit_X.shape[0]):\n",
    "                distances[i, j] = weighted_euclidean(X[i], self._fit_X[j], self.feature_weights)\n",
    "        \n",
    "        neigh_ind = distances.argsort(axis=1)[:, :self.n_neighbors]\n",
    "        neigh_labels = self._fit_y[neigh_ind]\n",
    "        \n",
    "        if self.weights == 'distance':\n",
    "            weights = self._get_weights(distances[np.arange(len(X))[:, np.newaxis], neigh_ind])\n",
    "            weights = weights / weights.sum(axis=1)[:, np.newaxis]\n",
    "        else:\n",
    "            weights = np.ones((len(X), self.n_neighbors)) / self.n_neighbors\n",
    "        \n",
    "        votes = np.zeros((len(X), len(np.unique(self._fit_y))))\n",
    "        for i in range(len(X)):\n",
    "            np.add.at(votes[i], neigh_labels[i], weights[i])\n",
    "        \n",
    "        return np.argmax(votes, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        distances = np.zeros((X.shape[0], self._fit_X.shape[0]))\n",
    "        for i in range(X.shape[0]):\n",
    "            for j in range(self._fit_X.shape[0]):\n",
    "                distances[i, j] = weighted_euclidean(X[i], self._fit_X[j], self.feature_weights)\n",
    "        \n",
    "        neigh_ind = distances.argsort(axis=1)[:, :self.n_neighbors]\n",
    "        neigh_labels = self._fit_y[neigh_ind]\n",
    "        \n",
    "        if self.weights == 'distance':\n",
    "            weights = self._get_weights(distances[np.arange(len(X))[:, np.newaxis], neigh_ind])\n",
    "            weights = weights / weights.sum(axis=1)[:, np.newaxis]\n",
    "        else:\n",
    "            weights = np.ones((len(X), self.n_neighbors)) / self.n_neighbors\n",
    "        \n",
    "        proba = np.zeros((len(X), len(np.unique(self._fit_y))))\n",
    "        for i in range(len(X)):\n",
    "            np.add.at(proba[i], neigh_labels[i], weights[i])\n",
    "        \n",
    "        proba = proba / proba.sum(axis=1)[:, np.newaxis]\n",
    "        return proba\n",
    "\n",
    "def create_feature_weighted_knn(X_train, y_train, X_user, y_user, weight_factor=3.0, \n",
    "                              feature_weight_method='ridge', C=1.0, **knn_params):\n",
    "    \"\"\"\n",
    "    Create a KNN model with learned feature weights from user examples.\n",
    "    \"\"\"\n",
    "    # Learn feature weights from user examples\n",
    "    if feature_weight_method == 'ridge':\n",
    "        model = RidgeClassifier(alpha=1.0/C)\n",
    "    elif feature_weight_method == 'lasso':\n",
    "        model = LogisticRegression(penalty='l1', C=C, solver='liblinear')\n",
    "    else:\n",
    "        model = LogisticRegression(penalty='l1', C=C, solver='liblinear')\n",
    "    \n",
    "    model.fit(X_user, y_user)\n",
    "    \n",
    "    if hasattr(model, 'coef_'):\n",
    "        if len(model.coef_.shape) == 2:\n",
    "            feature_weights = np.mean(np.abs(model.coef_), axis=0)\n",
    "        else:\n",
    "            feature_weights = np.abs(model.coef_)\n",
    "    else:\n",
    "        feature_weights = np.ones(X_user.shape[1])\n",
    "    \n",
    "    feature_weights = feature_weights / np.sum(feature_weights)\n",
    "    \n",
    "    top_indices = np.argsort(feature_weights)[-5:]\n",
    "    bottom_indices = np.argsort(feature_weights)[:5]\n",
    "    \n",
    "    print(\"\\nFeature weights learned from user examples:\")\n",
    "    print(f\"Top 5 features: {top_indices} with weights {feature_weights[top_indices]}\")\n",
    "    print(f\"Bottom 5 features: {bottom_indices} with weights {feature_weights[bottom_indices]}\")\n",
    "    \n",
    "    n_repeats = int(weight_factor)\n",
    "    X_user_repeated = np.repeat(X_user, n_repeats, axis=0)\n",
    "    y_user_repeated = np.repeat(y_user, n_repeats)\n",
    "    \n",
    "    X_combined = np.vstack([X_train, X_user_repeated])\n",
    "    y_combined = np.concatenate([y_train, y_user_repeated])\n",
    "    \n",
    "    knn_params.pop('metric', None)\n",
    "    knn_params.pop('metric_params', None)\n",
    "    knn = WeightedKNN(feature_weights=feature_weights, **knn_params)\n",
    "    knn.fit(X_combined, y_combined)\n",
    "    \n",
    "    return knn, feature_weights\n",
    "\n",
    "def grid_search_hyperparameters(X_train, y_train, X_user, y_user, X_val, y_val, base_params):\n",
    "    \"\"\"\n",
    "    Grid search for weight factors and regularization parameters.\n",
    "    \"\"\"\n",
    "    weight_factors = [2, 2.5, 3, 3.5, 4]\n",
    "    C_values = [0.3, 0.6, 1.0, 1.5, 2.0]\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\nGrid searching weight factors and regularization parameters...\")\n",
    "    for weight_factor in weight_factors:\n",
    "        for C in C_values:\n",
    "            for method in ['ridge', 'lasso']:\n",
    "                try:\n",
    "                    model, _ = create_feature_weighted_knn(\n",
    "                        X_train, y_train, X_user, y_user,\n",
    "                        weight_factor=weight_factor,\n",
    "                        feature_weight_method=method,\n",
    "                        C=C,\n",
    "                        **base_params\n",
    "                    )\n",
    "                    \n",
    "                    score = model.score(X_val, y_val)\n",
    "                    results.append({\n",
    "                        'weight_factor': weight_factor,\n",
    "                        'C': C,\n",
    "                        'method': method,\n",
    "                        'score': score\n",
    "                    })\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_params = {\n",
    "                            'weight_factor': weight_factor,\n",
    "                            'C': C,\n",
    "                            'method': method\n",
    "                        }\n",
    "                    \n",
    "                    print(f\"Weight={weight_factor}, C={C}, Method={method}: Score={score:.3f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed for weight={weight_factor}, C={C}, Method={method}: {str(e)}\")\n",
    "    \n",
    "    results.sort(key=lambda x: x['score'], reverse=True)\n",
    "    print(\"\\nTop 5 parameter combinations:\")\n",
    "    for i, result in enumerate(results[:5], 1):\n",
    "        print(f\"{i}. Weight={result['weight_factor']}, C={result['C']}, \"\n",
    "              f\"Method={result['method']}: Score={result['score']:.3f}\")\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "def create_hybrid_knn_model(base_features_path, base_labels_path, user_features_paths, user_labels_paths):\n",
    "    \"\"\"\n",
    "    Create a hybrid KNN model using validation strategy from knn_hybrid.py\n",
    "    \"\"\"\n",
    "    # Load base training data\n",
    "    X_train = np.load(base_features_path)\n",
    "    y_train = np.load(base_labels_path)\n",
    "    \n",
    "    # Load user examples\n",
    "    X_user_list = []\n",
    "    y_user_list = []\n",
    "    for feature_path, label_path in zip(user_features_paths.values(), user_labels_paths.values()):\n",
    "        features = np.load(feature_path)\n",
    "        labels = np.load(label_path)\n",
    "        X_user_list.append(features)\n",
    "        y_user_list.append(labels)\n",
    "    \n",
    "    X_user = np.vstack(X_user_list)\n",
    "    y_user = np.concatenate(y_user_list)\n",
    "    \n",
    "    # Split user examples\n",
    "    examples_per_class = 5\n",
    "    adapt_indices = []\n",
    "    val_indices = []\n",
    "    \n",
    "    for label in np.unique(y_user):\n",
    "        label_indices = np.where(y_user == label)[0]\n",
    "        if len(label_indices) <= examples_per_class * 2:\n",
    "            n_adapt = len(label_indices) // 2\n",
    "        else:\n",
    "            n_adapt = examples_per_class\n",
    "            \n",
    "        np.random.seed(42)\n",
    "        selected_indices = np.random.choice(label_indices, n_adapt, replace=False)\n",
    "        adapt_indices.extend(selected_indices)\n",
    "        val_indices.extend([idx for idx in label_indices if idx not in selected_indices])\n",
    "    \n",
    "    X_adapt = X_user[adapt_indices]\n",
    "    y_adapt = y_user[adapt_indices]\n",
    "    X_user_val = X_user[val_indices]\n",
    "    y_user_val = y_user[val_indices]\n",
    "    \n",
    "    print(\"\\nData split summary:\")\n",
    "    print(f\"Base training examples: {len(X_train)}\")\n",
    "    print(f\"User adaptation examples: {len(X_adapt)}\")\n",
    "    print(f\"User validation examples: {len(X_user_val)}\")\n",
    "    \n",
    "    print(\"\\nAdaptation set label distribution:\")\n",
    "    print(pd.Series(y_adapt).value_counts())\n",
    "    print(\"\\nValidation set label distribution:\")\n",
    "    print(pd.Series(y_user_val).value_counts())\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_adapt_scaled = scaler.transform(X_adapt)\n",
    "    X_user_val_scaled = scaler.transform(X_user_val)\n",
    "    X_user_scaled = scaler.transform(X_user)\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_train_enc = le.fit_transform(y_train)\n",
    "    y_adapt_enc = le.transform(y_adapt)\n",
    "    y_user_val_enc = le.transform(y_user_val)\n",
    "    y_user_enc = le.transform(y_user)\n",
    "    \n",
    "    # Find best base model parameters\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "    \n",
    "    print(\"\\nPerforming grid search for base model parameters...\")\n",
    "    base_knn = KNeighborsClassifier()\n",
    "    grid_search = GridSearchCV(base_knn, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train_enc)\n",
    "    \n",
    "    print(\"\\nBest base model parameters:\", grid_search.best_params_)\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\n",
    "    \n",
    "    # Grid search for feature weighting parameters\n",
    "    best_params, best_score = grid_search_hyperparameters(\n",
    "        X_train_scaled, y_train_enc,\n",
    "        X_adapt_scaled, y_adapt_enc,\n",
    "        X_user_val_scaled, y_user_val_enc,\n",
    "        grid_search.best_params_\n",
    "    )\n",
    "    \n",
    "    # Create final model with best parameters using ALL data\n",
    "    final_model, feature_weights = create_feature_weighted_knn(\n",
    "        X_train_scaled, y_train_enc,\n",
    "        X_user_scaled, y_user_enc,\n",
    "        weight_factor=best_params['weight_factor'],\n",
    "        feature_weight_method=best_params['method'],\n",
    "        C=best_params['C'],\n",
    "        **grid_search.best_params_\n",
    "    )\n",
    "    \n",
    "    # Visualize feature weights\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(feature_weights)), feature_weights)\n",
    "    plt.title('Feature Weights in Final Model')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Weight')\n",
    "    \n",
    "    vis_dir = \"/Users/arul/ML/BEATBOX/projectFiles/visualization/hybridknn\"\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(vis_dir, 'final_feature_weights.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return final_model, scaler, le, grid_search.best_params_, best_params, feature_weights\n",
    "\n",
    "def save_model_components(model, scaler, label_encoder, base_params, feature_params, feature_weights, output_dir):\n",
    "    \"\"\"\n",
    "    Save all model components to disk.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    joblib.dump(model, os.path.join(output_dir, 'hybrid_knn_model.joblib'))\n",
    "    joblib.dump(scaler, os.path.join(output_dir, 'scaler.joblib'))\n",
    "    joblib.dump(label_encoder, os.path.join(output_dir, 'label_encoder.joblib'))\n",
    "    \n",
    "    joblib.dump({\n",
    "        'base_params': base_params,\n",
    "        'feature_params': feature_params,\n",
    "        'feature_weights': feature_weights\n",
    "    }, os.path.join(output_dir, 'model_params.joblib'))\n",
    "    \n",
    "    print(\"\\nModel components saved:\")\n",
    "    print(f\"- Model saved to: {output_dir}/hybrid_knn_model.joblib\")\n",
    "    print(f\"- Scaler saved to: {output_dir}/scaler.joblib\")\n",
    "    print(f\"- Label encoder saved to: {output_dir}/label_encoder.joblib\")\n",
    "    print(f\"- Parameters saved to: {output_dir}/model_params.joblib\")\n",
    "\n",
    "def main():\n",
    "    base_features_path = \"/Users/arul/ML/BEATBOX/projectFiles/extracted_features/features/mfcc_extracted_features.npy\"\n",
    "    base_labels_path = \"/Users/arul/ML/BEATBOX/projectFiles/extracted_features/labels/mfcc_extracted_labels.npy\"\n",
    "    \n",
    "    user_features_paths = {\n",
    "        'kick': '/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/extracted_features/features/kick_features.npy',\n",
    "        'snare': '/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/extracted_features/features/snare_features.npy'\n",
    "    }\n",
    "    \n",
    "    user_labels_paths = {\n",
    "        'kick': '/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/extracted_features/labels/kick_labels.npy',\n",
    "        'snare': '/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/extracted_features/labels/snare_labels.npy'\n",
    "    }\n",
    "    \n",
    "    model, scaler, label_encoder, base_params, feature_params, feature_weights = create_hybrid_knn_model(\n",
    "        base_features_path=base_features_path,\n",
    "        base_labels_path=base_labels_path,\n",
    "        user_features_paths=user_features_paths,\n",
    "        user_labels_paths=user_labels_paths\n",
    "    )\n",
    "    \n",
    "    # Save model components\n",
    "    model_dir = \"/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/models\"\n",
    "    save_model_components(\n",
    "        model=model,\n",
    "        scaler=scaler,\n",
    "        label_encoder=label_encoder,\n",
    "        base_params=base_params,\n",
    "        feature_params=feature_params,\n",
    "        feature_weights=feature_weights,\n",
    "        output_dir=model_dir\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel is ready to use!\")\n",
    "    print(\"Class labels:\", label_encoder.classes_)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segmenting audio file: /Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/audio/sample1/vox/full_vocal.wav\n",
      "Found 22 onsets\n",
      "\n",
      "Processing complete:\n",
      "Total segments created: 22\n",
      "Segments saved in: /Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/audio/sample1/segments/full_segments\n",
      "CSV file saved as: /Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/csv_info/full_segment_info.csv\n",
      "Onset detection plot saved as: /Users/arul/ML/BEATBOX/projectFiles/visualization/final_pipeline/onset_detection.png\n",
      "\n",
      "Please manually fill in the 'instrument_label' column with:\n",
      "- 'kd' for kick drum\n",
      "- 'sd' for snare drum\n",
      "- 'hhc' for closed hi-hat\n",
      "- 'hho' for open hi-hat\n"
     ]
    }
   ],
   "source": [
    "def segment_full_recording(input_file, output_dir, segment_info_path):\n",
    "    \"\"\"\n",
    "    Segment a full vocal percussion recording with parameters optimized for detecting rapid successive onsets.\n",
    "    \"\"\"\n",
    "    # Create output directories if they don't exist\n",
    "    segments_dir = Path(output_dir)  # Now using the exact path specified\n",
    "    segments_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create visualization directory\n",
    "    vis_dir = Path(\"/Users/arul/ML/BEATBOX/projectFiles/visualization/final_pipeline\")\n",
    "    vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(input_file)\n",
    "    \n",
    "    # Parameters optimized for detecting close successive onsets\n",
    "    onset_frames = librosa.onset.onset_detect(\n",
    "        y=y, \n",
    "        sr=sr,\n",
    "        units='frames',\n",
    "        hop_length=256,\n",
    "        backtrack=True,\n",
    "        pre_max=3,\n",
    "        post_max=3,\n",
    "        pre_avg=15,\n",
    "        post_avg=15,\n",
    "        delta=0.07,\n",
    "        wait=5\n",
    "    )\n",
    "    \n",
    "    # Visualize the onset detection\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot waveform\n",
    "    times = np.arange(len(y)) / sr\n",
    "    plt.plot(times, y, alpha=0.6, label='Waveform')\n",
    "    \n",
    "    # Plot onset markers\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr=sr, hop_length=256)\n",
    "    plt.vlines(onset_times, -1, 1, color='r', label='Onsets')\n",
    "    \n",
    "    plt.title('Waveform and Detected Onsets')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot in the visualization directory\n",
    "    plt.savefig(vis_dir / 'onset_detection.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Convert frames to samples\n",
    "    onset_samples = librosa.frames_to_samples(onset_frames, hop_length=256)\n",
    "    onset_times = librosa.frames_to_time(onset_frames, sr=sr, hop_length=256)\n",
    "    \n",
    "    # Initialize lists for segment info\n",
    "    segment_paths = []\n",
    "    start_times = []\n",
    "    end_times = []\n",
    "    \n",
    "    # Segment duration in seconds\n",
    "    segment_duration = 0.25\n",
    "    samples_per_segment = int(segment_duration * sr)\n",
    "    \n",
    "    print(f\"\\nSegmenting audio file: {input_file}\")\n",
    "    print(f\"Found {len(onset_frames)} onsets\")\n",
    "    \n",
    "    # Process each onset\n",
    "    for i, (onset_sample, onset_time) in enumerate(zip(onset_samples, onset_times)):\n",
    "        # Calculate segment boundaries\n",
    "        start_sample = max(0, onset_sample - int(0.05 * sr))\n",
    "        end_sample = min(len(y), start_sample + samples_per_segment)\n",
    "        \n",
    "        # Extract segment\n",
    "        segment = y[start_sample:end_sample]\n",
    "        \n",
    "        # Pad if necessary\n",
    "        if len(segment) < samples_per_segment:\n",
    "            segment = np.pad(segment, (0, samples_per_segment - len(segment)))\n",
    "        \n",
    "        # Generate segment filename\n",
    "        segment_filename = f'segment_{i:04d}.wav'\n",
    "        segment_path = segments_dir / segment_filename  # Save directly in output_dir\n",
    "        \n",
    "        # Save segment\n",
    "        sf.write(segment_path, segment, sr)\n",
    "        \n",
    "        # Store segment information\n",
    "        segment_paths.append(str(segment_path))\n",
    "        start_times.append(onset_time)\n",
    "        end_times.append(onset_time + segment_duration)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    segment_info = pd.DataFrame({\n",
    "        'segment_path': segment_paths,\n",
    "        'start_time': start_times,\n",
    "        'end_time': end_times,\n",
    "        'instrument_label': ''\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    segment_info.to_csv(segment_info_path, index=False)\n",
    "    \n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"Total segments created: {len(segment_paths)}\")\n",
    "    print(f\"Segments saved in: {segments_dir}\")\n",
    "    print(f\"CSV file saved as: {segment_info_path}\")\n",
    "    print(f\"Onset detection plot saved as: {vis_dir}/onset_detection.png\")\n",
    "    print(\"\\nPlease manually fill in the 'instrument_label' column with:\")\n",
    "    print(\"- 'kd' for kick drum\")\n",
    "    print(\"- 'sd' for snare drum\")\n",
    "    print(\"- 'hhc' for closed hi-hat\")\n",
    "    print(\"- 'hho' for open hi-hat\")\n",
    "    \n",
    "    return segment_info\n",
    "\n",
    "# Example usage\n",
    "input_file = \"/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/audio/sample1/vox/full_vocal.wav\"\n",
    "output_dir = \"/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/audio/sample1/segments/full_segments\"\n",
    "segment_info_path = \"/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/csv_info/full_segment_info.csv\"\n",
    "\n",
    "segment_info = segment_full_recording(input_file, output_dir, segment_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting expanded feature set from ../audio/sample1/segments/full_segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 157.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature extraction complete:\n",
      "Successfully processed: 22 segments\n",
      "Feature vector shape: (22, 12)\n",
      "\n",
      "Feature set includes:\n",
      "- Selected MFCCs and their deltas\n",
      "- Envelope descriptors (flatness, temporal centroid, pre-max derivative)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_features, full_labels = extract_features_from_user_input(\n",
    "    segment_info_path='../csv_info/full_segment_info.csv',\n",
    "    segments_dir='../audio/sample1/segments/full_segments',\n",
    "    features_output_dir='../extracted_features/features/full_features.npy',\n",
    "    labels_output_dir='../extracted_features/labels/full_labels.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def load_model_components(model_dir):\n",
    "    \"\"\"\n",
    "    Load all model components from disk.\n",
    "    \"\"\"\n",
    "    model = joblib.load(os.path.join(model_dir, 'hybrid_knn_model.joblib'))\n",
    "    scaler = joblib.load(os.path.join(model_dir, 'scaler.joblib'))\n",
    "    label_encoder = joblib.load(os.path.join(model_dir, 'label_encoder.joblib'))\n",
    "    params = joblib.load(os.path.join(model_dir, 'model_params.joblib'))\n",
    "    \n",
    "    return model, scaler, label_encoder, params\n",
    "\n",
    "def analyze_full_recording(model, scaler, label_encoder, features, segment_info_path, vis_dir, csv_output_path):\n",
    "    \"\"\"\n",
    "    Analyze the full recording and generate predictions with visualizations.\n",
    "    Compare with true labels if available.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded hybrid KNN model\n",
    "        scaler: Fitted StandardScaler\n",
    "        label_encoder: Fitted LabelEncoder\n",
    "        features: Features extracted from segments\n",
    "        segment_info_path: Path to CSV containing segment information\n",
    "        vis_dir: Directory to save visualizations\n",
    "        csv_output_path: Path to save the predictions CSV file\n",
    "    \"\"\"\n",
    "    # Load segment info for timestamps\n",
    "    segment_info = pd.read_csv(segment_info_path)\n",
    "    has_true_labels = 'instrument_label' in segment_info.columns and not segment_info['instrument_label'].isna().all()\n",
    "    \n",
    "    # Scale features\n",
    "    features_scaled = scaler.transform(features)\n",
    "    \n",
    "    # Get predictions and probabilities\n",
    "    predictions = model.predict(features_scaled)\n",
    "    probabilities = model.predict_proba(features_scaled)\n",
    "    \n",
    "    # Convert numeric predictions to labels\n",
    "    predicted_labels = label_encoder.inverse_transform(predictions)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Segment': segment_info['segment_path'],\n",
    "        'Start_Time': segment_info['start_time'],\n",
    "        'Predicted_Label': predicted_labels,\n",
    "        'Confidence': np.max(probabilities, axis=1)\n",
    "    })\n",
    "    \n",
    "    if has_true_labels:\n",
    "        results_df['True_Label'] = segment_info['instrument_label']\n",
    "        results_df['Correct'] = results_df['True_Label'] == results_df['Predicted_Label']\n",
    "    \n",
    "    # Save results to specified CSV path\n",
    "    os.makedirs(os.path.dirname(csv_output_path), exist_ok=True)\n",
    "    results_df.to_csv(csv_output_path, index=False)\n",
    "    \n",
    "    # [Rest of the visualization code remains the same...]\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nPrediction Summary:\")\n",
    "    print(\"-----------------\")\n",
    "    print(f\"\\nResults saved to: {csv_output_path}\")\n",
    "    print(\"\\nTotal segments analyzed:\", len(results_df))\n",
    "    print(\"\\nPredicted label distribution:\")\n",
    "    print(results_df['Predicted_Label'].value_counts())\n",
    "    \n",
    "    if has_true_labels:\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(results_df['True_Label'], results_df['Predicted_Label']))\n",
    "        print(f\"\\nOverall Accuracy: {accuracy_score(results_df['True_Label'], results_df['Predicted_Label']):.3f}\")\n",
    "    \n",
    "    print(\"\\nConfidence statistics:\")\n",
    "    print(results_df.groupby('Predicted_Label')['Confidence'].describe())\n",
    "    \n",
    "    # List high confidence predictions\n",
    "    high_confidence = results_df[results_df['Confidence'] > 0.8]\n",
    "    print(\"\\nHigh confidence predictions (>80%):\")\n",
    "    print(f\"Found {len(high_confidence)} high confidence predictions\")\n",
    "    if has_true_labels:\n",
    "        print(high_confidence[['Start_Time', 'True_Label', 'Predicted_Label', 'Confidence', 'Correct']])\n",
    "    else:\n",
    "        print(high_confidence[['Start_Time', 'Predicted_Label', 'Confidence']])\n",
    "    \n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Summary:\n",
      "-----------------\n",
      "\n",
      "Results saved to: /Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/csv_info/sample1_predictions.csv\n",
      "\n",
      "Total segments analyzed: 22\n",
      "\n",
      "Predicted label distribution:\n",
      "Predicted_Label\n",
      "kd     13\n",
      "sd      8\n",
      "hhc     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         hhc       0.00      0.00      0.00         0\n",
      "          kd       0.92      0.80      0.86        15\n",
      "          sd       0.75      0.86      0.80         7\n",
      "\n",
      "    accuracy                           0.82        22\n",
      "   macro avg       0.56      0.55      0.55        22\n",
      "weighted avg       0.87      0.82      0.84        22\n",
      "\n",
      "\n",
      "Overall Accuracy: 0.818\n",
      "\n",
      "Confidence statistics:\n",
      "                 count      mean       std       min       25%       50%  \\\n",
      "Predicted_Label                                                            \n",
      "hhc                1.0  0.363636       NaN  0.363636  0.363636  0.363636   \n",
      "kd                13.0  0.818182  0.174078  0.454545  0.727273  0.818182   \n",
      "sd                 8.0  0.761364  0.205804  0.454545  0.613636  0.772727   \n",
      "\n",
      "                      75%       max  \n",
      "Predicted_Label                      \n",
      "hhc              0.363636  0.363636  \n",
      "kd               1.000000  1.000000  \n",
      "sd               0.931818  1.000000  \n",
      "\n",
      "High confidence predictions (>80%):\n",
      "Found 12 high confidence predictions\n",
      "    Start_Time True_Label Predicted_Label  Confidence  Correct\n",
      "3     2.496145         kd              sd    1.000000    False\n",
      "6     3.297234         sd              kd    0.909091    False\n",
      "9     4.783311         sd              sd    0.909091     True\n",
      "10    5.387029         kd              kd    1.000000     True\n",
      "11    5.746939         kd              kd    1.000000     True\n",
      "12    6.060408         kd              kd    0.818182     True\n",
      "13    6.188118         sd              sd    0.818182     True\n",
      "15    6.977596         kd              kd    1.000000     True\n",
      "16    7.534875         kd              kd    0.909091     True\n",
      "19    8.579773         kd              kd    1.000000     True\n",
      "20    8.893243         kd              kd    0.818182     True\n",
      "21    9.090612         sd              sd    1.000000     True\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/models\"\n",
    "model, scaler, label_encoder, params = load_model_components(model_dir)\n",
    "\n",
    "# Paths for full recording analysis\n",
    "full_features = np.load(\"/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/extracted_features/features/full_features.npy\")\n",
    "segment_info_path = \"/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/csv_info/full_segment_info.csv\"\n",
    "vis_dir = \"/Users/arul/ML/BEATBOX/projectFiles/visualization/final_pipeline\"\n",
    "csv_output_path = \"/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/csv_info/sample1_predictions.csv\"  # New parameter\n",
    "\n",
    "# Analyze the full recording\n",
    "results = analyze_full_recording(\n",
    "    model=model,\n",
    "    scaler=scaler,\n",
    "    label_encoder=label_encoder,\n",
    "    features=full_features,\n",
    "    segment_info_path=segment_info_path,\n",
    "    vis_dir=vis_dir,\n",
    "    csv_output_path=csv_output_path  # New parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def create_drum_track(predictions_csv, real_drums_dir, output_path, duration=None):\n",
    "    \"\"\"\n",
    "    Create a new audio track using real drum samples based on model predictions.\n",
    "    \n",
    "    Args:\n",
    "        predictions_csv: Path to CSV containing predictions and timing\n",
    "        real_drums_dir: Directory containing real drum samples\n",
    "        output_path: Where to save the final audio file\n",
    "        duration: Optional duration to extend the audio (in seconds)\n",
    "    \"\"\"\n",
    "    # Load predictions\n",
    "    predictions = pd.read_csv(predictions_csv)\n",
    "    \n",
    "    # Load drum samples\n",
    "    drum_samples = {}\n",
    "    sample_rates = {}\n",
    "    for drum_type in ['kd', 'sd', 'hhc', 'hho']:\n",
    "        sample_path = os.path.join(real_drums_dir, f\"{drum_type}.wav\")\n",
    "        if os.path.exists(sample_path):\n",
    "            audio, sr = librosa.load(sample_path, sr=None)\n",
    "            drum_samples[drum_type] = audio\n",
    "            sample_rates[drum_type] = sr\n",
    "    \n",
    "    # Use the first sample rate as our target sample rate\n",
    "    target_sr = list(sample_rates.values())[0]\n",
    "    \n",
    "    # Determine output duration\n",
    "    if duration is None:\n",
    "        duration = predictions['Start_Time'].max() + 1.0  # Add 1 second padding\n",
    "    \n",
    "    # Create empty output array\n",
    "    output_samples = int(duration * target_sr)\n",
    "    output = np.zeros(output_samples)\n",
    "    \n",
    "    # Place samples at predicted times\n",
    "    for _, row in predictions.iterrows():\n",
    "        drum_type = row['Predicted_Label'].lower()\n",
    "        if drum_type in drum_samples:\n",
    "            # Convert time to sample position\n",
    "            start_sample = int(row['Start_Time'] * target_sr)\n",
    "            \n",
    "            # Get the drum sample\n",
    "            sample = drum_samples[drum_type]\n",
    "            \n",
    "            # Adjust gain based on confidence (optional)\n",
    "            gain = 0.8 + (0.2 * row['Confidence'])  # Scale between 0.8 and 1.0\n",
    "            \n",
    "            # Add the sample to the output\n",
    "            end_sample = start_sample + len(sample)\n",
    "            if end_sample > len(output):\n",
    "                # Truncate sample if it would exceed output length\n",
    "                sample = sample[:len(output) - start_sample]\n",
    "                end_sample = len(output)\n",
    "            \n",
    "            # Add sample with gain adjustment\n",
    "            output[start_sample:end_sample] += sample * gain\n",
    "    \n",
    "    # Normalize the output to prevent clipping\n",
    "    output = librosa.util.normalize(output) * 0.9\n",
    "    \n",
    "    # Save the result\n",
    "    sf.write(output_path, output, target_sr)\n",
    "    \n",
    "    print(f\"\\nDrum track created:\")\n",
    "    print(f\"- Duration: {duration:.2f} seconds\")\n",
    "    print(f\"- Sample rate: {target_sr} Hz\")\n",
    "    print(f\"- Output saved to: {output_path}\")\n",
    "    \n",
    "    return output, target_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Drum track created:\n",
      "- Duration: 10.09 seconds\n",
      "- Sample rate: 44100 Hz\n",
      "- Output saved to: /Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/audio/sample1/vox/synthesized_drums.wav\n"
     ]
    }
   ],
   "source": [
    "predictions_path = \"/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/csv_info/sample1_predictions.csv\"\n",
    "real_drums_dir = \"/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/audio/real_drums\"\n",
    "output_path = \"/Users/arul/ML/BEATBOX/projectFiles/code/final_pipeline/audio/sample1/vox/synthesized_drums.wav\"\n",
    "\n",
    "# Create the drum track\n",
    "audio, sr = create_drum_track(\n",
    "    predictions_csv=predictions_path,\n",
    "    real_drums_dir=real_drums_dir,\n",
    "    output_path=output_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
