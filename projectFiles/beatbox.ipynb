{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting and Organizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to make a master dataframe with all the relevant data. This master dataframe will contain an entry for every single onset, for every single wav file in the audio file. If an audio file is multiple drum sounds, then there is a single onset for each drum sound, and an single audio file will contirnbute to multiple entries in the dataset. We will have to parse AVP and LVT seperately and then combine them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse AVP csv, get the onset time, instrument label, onset phoneme, coda phoneme, dataset, participant id, subset, csv file path, wav file path\n",
    "def parse_avp_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Parses an AVP CSV with no header, returning a list of dicts.\n",
    "    Each dict has:\n",
    "      - onset_time (float)\n",
    "      - instrument_label (str)\n",
    "      - onset_phoneme (str)\n",
    "      - coda_phoneme (str)\n",
    "      - dataset (str) = \"AVP\"\n",
    "      - participant_id (str)\n",
    "      - subset (str) = \"personal\"\n",
    "      - csv_file_path (str)\n",
    "      - wav_file_path (str)\n",
    "    \"\"\"\n",
    "    # 1) Extract some metadata from the file path\n",
    "    csv_dir = os.path.dirname(csv_path)             # e.g., \"AVP_Dataset/Personal/Participant_1\"\n",
    "    csv_file_name = os.path.basename(csv_path)      # e.g., \"P1_kd.csv\" or \"Participant_1_kd.csv\"\n",
    "    base_name, _ = os.path.splitext(csv_file_name)  # e.g., \"P1_kd\" or \"Participant_1_kd\"\n",
    "    \n",
    "    # 2) Determine participant_id from the file name\n",
    "    #    Suppose your file name is \"P1_kd.csv\", so the participant part is \"P1\".\n",
    "    #    If it’s \"Participant_1_kd.csv\", you might want the first two segments. Adapt as needed.\n",
    "    #    Here's a simple approach:\n",
    "    parts = base_name.split(\"_\")  # e.g. [\"P1\", \"kd\"] or [\"Participant\", \"1\", \"kd\"]\n",
    "    participant_id = parts[0]     # e.g. \"P1\" or \"Participant\"\n",
    "    # If your actual naming is more complicated, tweak the logic. \n",
    "    # For instance, if \"Participant_1\" is always 2 segments, you might do participant_id = \"_\".join(parts[:2])\n",
    "    \n",
    "    # 3) Build the wav path. If the CSV is \"P1_kd.csv\", the wav is \"P1_kd.wav\" in the same folder.\n",
    "    wav_file_name = base_name + \".wav\"\n",
    "    wav_file_path = os.path.join(csv_dir, wav_file_name)\n",
    "    \n",
    "    # 4) We'll fix \"dataset\" = \"AVP\" and \"subset\" = \"personal\" \n",
    "    #    (since that's the arrangement you described for the AVP dataset).\n",
    "    dataset = \"AVP\"\n",
    "    subset = \"personal\"\n",
    "    \n",
    "    # 5) Parse each row of the CSV\n",
    "    data = []\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            # Expecting: [onset_time, instrument_label, onset_phoneme, coda_phoneme]\n",
    "            if len(row) < 2:\n",
    "                continue  # skip empty or malformed lines\n",
    "            onset_time = float(row[0])\n",
    "            instrument_label = row[1]\n",
    "            onset_phoneme = row[2] if len(row) > 2 else ''\n",
    "            coda_phoneme = row[3] if len(row) > 3 else ''\n",
    "            \n",
    "            entry = {\n",
    "                'onset_time': onset_time,\n",
    "                'instrument_label': instrument_label,\n",
    "                'onset_phoneme': onset_phoneme,\n",
    "                'coda_phoneme': coda_phoneme,\n",
    "                'dataset': dataset,\n",
    "                'participant_id': participant_id,\n",
    "                'subset': subset,\n",
    "                'csv_file_path': csv_path,\n",
    "                'wav_file_path': wav_file_path\n",
    "            }\n",
    "            data.append(entry)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to collect all AVP data, takes a root directory as input, walks through the AVP dataset directory and collects all CSV data into a master DataFrame\n",
    "def collect_all_avp_data(root_dir):\n",
    "    \"\"\"\n",
    "    Walks through the AVP dataset directory and collects all CSV data into a master DataFrame,\n",
    "    maintaining the grouping of entries from the same CSV file\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    personal_dir = os.path.join(root_dir, \"Personal\")\n",
    "    \n",
    "    # Walk through all participant directories in sorted order\n",
    "    for participant_dir in sorted(os.listdir(personal_dir)):\n",
    "        participant_path = os.path.join(personal_dir, participant_dir)\n",
    "        \n",
    "        # Skip if not a directory or hidden files\n",
    "        if not os.path.isdir(participant_path) or participant_dir.startswith('.'):\n",
    "            continue\n",
    "            \n",
    "        # Process CSV files in sorted order\n",
    "        for file_name in sorted(os.listdir(participant_path)):\n",
    "            if file_name.endswith('.csv'):\n",
    "                csv_path = os.path.join(participant_path, file_name)\n",
    "                \n",
    "                try:\n",
    "                    parsed_data = parse_avp_csv(csv_path)\n",
    "                    # Add the source filename as a field for sorting\n",
    "                    for entry in parsed_data:\n",
    "                        entry['source_file'] = file_name\n",
    "                    all_data.extend(parsed_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {csv_path}: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Sort to maintain grouping:\n",
    "    # First by participant_id, then by source file, then by onset_time\n",
    "    df = df.sort_values(['participant_id', 'source_file', 'onset_time'])\n",
    "    \n",
    "    # Optionally remove the temporary source_file column if you don't need it\n",
    "    df = df.drop('source_file', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_standardized_phoneme(simple_phoneme, is_onset=True):\n",
    "#     \"\"\"\n",
    "#     Converts LVT phoneme notation to match AVP's IPA notation\n",
    "    \n",
    "#     Args:\n",
    "#         simple_phoneme: The phoneme from LVT dataset\n",
    "#         is_onset: Boolean indicating if this is an onset (True) or coda (False) phoneme\n",
    "#     \"\"\"\n",
    "#     # Onset phonemes (consonants at start)\n",
    "#     onset_map = {\n",
    "#         '!': '!',        # keep as is (appears in both)\n",
    "#         'k': 'k',        # keep as is\n",
    "#         'p': 'p',        # keep as is\n",
    "#         's': 's',        # keep as is\n",
    "#         't': 't',        # keep as is\n",
    "#         'ts': 'ts',      # keep as is\n",
    "#         'tʃ': 'tʃ',      # keep as is\n",
    "#         'ʔ': 'tʃ',       # map glottal stop to tʃ\n",
    "#         'ʡʢ': 'ʡʢ'       # keep as is (appears in both)\n",
    "#     }\n",
    "    \n",
    "#     # Coda phonemes (vowels/endings)\n",
    "#     coda_map = {\n",
    "#         'a': 'æ',        # map 'a' to 'æ' as it's more common in AVP\n",
    "#         'h': 'h',        # keep as is\n",
    "#         'u': 'u',        # keep as is\n",
    "#         'x': 'x',        # keep as is\n",
    "#         'ʊ': 'ʊ'         # keep as is\n",
    "#     }\n",
    "    \n",
    "#     # Choose which mapping to use\n",
    "#     phoneme_map = onset_map if is_onset else coda_map\n",
    "    \n",
    "#     # Return mapped phoneme if it exists, otherwise return original\n",
    "#     return phoneme_map.get(simple_phoneme, simple_phoneme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functiuon to parse LVT csv, get the onset time, instrument label, onset phoneme, coda phoneme, dataset, participant id, subset, csv file path, wav file path\n",
    "def parse_lvt_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Parses an LVT CSV with no header, returning a list of dicts.\n",
    "    Similar to parse_avp_csv but handles LVT-specific formatting.\n",
    "    \"\"\"\n",
    "    # Extract metadata from the file path\n",
    "    csv_dir = os.path.dirname(csv_path)             \n",
    "    csv_file_name = os.path.basename(csv_path)      # e.g., \"AFRP.csv\"\n",
    "    base_name, _ = os.path.splitext(csv_file_name)  # e.g., \"AFRP\"\n",
    "    \n",
    "    # Determine if this is from Frase or Improviso folder\n",
    "    subset = \"Frase\" if \"Frase\" in csv_dir else \"Improviso\"\n",
    "    \n",
    "    # Participant ID is the filename without extension\n",
    "    participant_id = base_name\n",
    "    \n",
    "    # Build the wav path (add \"3\" before .wav)\n",
    "    wav_file_name = base_name + \"3.wav\"\n",
    "    wav_file_path = os.path.join(csv_dir, wav_file_name)\n",
    "    \n",
    "    # Mapping for instrument labels\n",
    "    instrument_map = {\n",
    "        \"Kick\": \"kd\",\n",
    "        \"Snare\": \"sd\",\n",
    "        \"HH\": \"hhc\"  # Assuming all HH in LVT are closed hi-hats\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue  # skip empty or malformed lines\n",
    "                \n",
    "            onset_time = float(row[0])\n",
    "            original_label = row[1]\n",
    "            instrument_label = instrument_map.get(original_label, original_label)\n",
    "            onset_phoneme = row[2] if len(row) > 2 else ''\n",
    "            coda_phoneme = row[3] if len(row) > 3 else ''\n",
    "            \n",
    "            # onset_phoneme = get_standardized_phoneme(row[2], is_onset=True)   # converts 'ts' if needed\n",
    "            # coda_phoneme = get_standardized_phoneme(row[3], is_onset=False)   # converts 'x' if needed\n",
    "            \n",
    "            entry = {\n",
    "                'onset_time': onset_time,\n",
    "                'instrument_label': instrument_label,\n",
    "                'onset_phoneme': onset_phoneme,\n",
    "                'coda_phoneme': coda_phoneme,\n",
    "                'dataset': \"LVT\",\n",
    "                'participant_id': participant_id,\n",
    "                'subset': subset,\n",
    "                'csv_file_path': csv_path,\n",
    "                'wav_file_path': wav_file_path\n",
    "            }\n",
    "            data.append(entry)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to collect all LVT data, takes a root directory as input, walks through the LVT dataset directory and collects all CSV data into a master DataFrame\n",
    "def collect_all_lvt_data(root_dir):\n",
    "    \"\"\"\n",
    "    Walks through the LVT dataset directory and collects all CSV data into a master DataFrame\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # Process both Frase and Improviso folders\n",
    "    for subset_dir in [\"Frase\", \"Improviso\"]:\n",
    "        subset_path = os.path.join(root_dir, subset_dir)\n",
    "        \n",
    "        # Skip if directory doesn't exist\n",
    "        if not os.path.isdir(subset_path):\n",
    "            continue\n",
    "            \n",
    "        # Process CSV files in sorted order\n",
    "        for file_name in sorted(os.listdir(subset_path)):\n",
    "            if file_name.endswith('.csv') and not file_name.startswith('.'):\n",
    "                csv_path = os.path.join(subset_path, file_name)\n",
    "                \n",
    "                try:\n",
    "                    parsed_data = parse_lvt_csv(csv_path)\n",
    "                    # Add source file for sorting\n",
    "                    for entry in parsed_data:\n",
    "                        entry['source_file'] = file_name\n",
    "                    all_data.extend(parsed_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {csv_path}: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Sort to maintain grouping\n",
    "    df = df.sort_values(['subset', 'participant_id', 'source_file', 'onset_time'])\n",
    "    \n",
    "    # Remove temporary sorting column\n",
    "    df = df.drop('source_file', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create all datasets, calls the functions to collect AVP and LVT data and then combines them into a master dataframe\n",
    "def create_all_datasets():\n",
    "    avp_dataset_path = \"../AVP-LVT_Dataset/AVP_Dataset\"\n",
    "    lvt_dataset_path = \"../AVP-LVT_Dataset/LVT_Dataset\"\n",
    "    \n",
    "    # Collect data from both datasets\n",
    "    print(\"Processing AVP dataset...\")\n",
    "    avp_df = collect_all_avp_data(avp_dataset_path)\n",
    "    \n",
    "    print(\"Processing LVT dataset...\")\n",
    "    lvt_df = collect_all_lvt_data(lvt_dataset_path)\n",
    "    \n",
    "    # Save individual datasets\n",
    "    print(\"\\nSaving individual datasets...\")\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs('EDA', exist_ok=True)\n",
    "    \n",
    "    avp_df.to_csv('EDA/avp_dataset.csv', index=False)\n",
    "    lvt_df.to_csv('EDA/lvt_dataset.csv', index=False)\n",
    "    \n",
    "    # Combine and save master dataset\n",
    "    print(\"Creating and saving master dataset...\")\n",
    "    master_df = pd.concat([avp_df, lvt_df], ignore_index=True)\n",
    "    master_df.to_csv('EDA/master_dataset.csv', index=False)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nDataset Summaries:\")\n",
    "    print(f\"AVP Dataset: {len(avp_df)} events\")\n",
    "    print(\"\\nAVP participants:\", len(avp_df['participant_id'].unique()))\n",
    "    print(\"AVP instrument distribution:\")\n",
    "    print(avp_df['instrument_label'].value_counts())\n",
    "    \n",
    "    print(f\"\\nLVT Dataset: {len(lvt_df)} events\")\n",
    "    print(\"LVT subsets:\", lvt_df['subset'].unique())\n",
    "    print(\"LVT participants:\", len(lvt_df['participant_id'].unique()))\n",
    "    print(\"LVT instrument distribution:\")\n",
    "    print(lvt_df['instrument_label'].value_counts())\n",
    "    \n",
    "    print(f\"\\nMaster Dataset: {len(master_df)} total events\")\n",
    "    print(\"Distribution by dataset:\")\n",
    "    print(master_df['dataset'].value_counts())\n",
    "    \n",
    "    return avp_df, lvt_df, master_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AVP dataset...\n",
      "Processing LVT dataset...\n",
      "\n",
      "Saving individual datasets...\n",
      "Creating and saving master dataset...\n",
      "\n",
      "Dataset Summaries:\n",
      "AVP Dataset: 4873 events\n",
      "\n",
      "AVP participants: 28\n",
      "AVP instrument distribution:\n",
      "instrument_label\n",
      "kd     1447\n",
      "sd     1253\n",
      "hhc    1164\n",
      "hho    1009\n",
      "Name: count, dtype: int64\n",
      "\n",
      "LVT Dataset: 841 events\n",
      "LVT subsets: ['Frase' 'Improviso']\n",
      "LVT participants: 40\n",
      "LVT instrument distribution:\n",
      "instrument_label\n",
      "hhc    334\n",
      "kd     329\n",
      "sd     178\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Master Dataset: 5714 total events\n",
      "Distribution by dataset:\n",
      "dataset\n",
      "AVP    4873\n",
      "LVT     841\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "avp_df, lvt_df, master_df = create_all_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have master_dataset.csv, and master_df, both of which contain the info for every single onset for every single sound in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVP Unique Onset Phonemes:\n",
      "['!', 'dʒ', 'k', 'kg', 'kʃ', 'p', 's', 't', 'ts', 'tɕ', 'tʃ', 'tʒ', 'ʡʢ']\n",
      "\n",
      "AVP Unique Coda Phonemes:\n",
      "['I', 'a', 'e', 'h', 'i', 'o', 'u', 'x', 'æ', 'œ', 'ɐ', 'ɘ', 'ə', 'ɪ', 'ɯ', 'ʊ', 'ʌ']\n",
      "\n",
      "LVT Unique Onset Phonemes:\n",
      "['!', 'k', 'p', 's', 't', 'ts', 'tʃ', 'ʔ', 'ʡʢ']\n",
      "\n",
      "LVT Unique Coda Phonemes:\n",
      "['a', 'h', 'u', 'x', 'ʊ']\n"
     ]
    }
   ],
   "source": [
    "def analyze_phonemes():\n",
    "    \"\"\"\n",
    "    Analyze and compare phonemes between AVP and LVT datasets\n",
    "    \"\"\"\n",
    "    # Read both datasets\n",
    "    avp_df = pd.read_csv('EDA/avp_dataset.csv')\n",
    "    lvt_df = pd.read_csv('EDA/lvt_dataset.csv')\n",
    "    \n",
    "    print(\"AVP Unique Onset Phonemes:\")\n",
    "    print(sorted(avp_df['onset_phoneme'].unique()))\n",
    "    print(\"\\nAVP Unique Coda Phonemes:\")\n",
    "    print(sorted(avp_df['coda_phoneme'].unique()))\n",
    "    \n",
    "    print(\"\\nLVT Unique Onset Phonemes:\")\n",
    "    print(sorted(lvt_df['onset_phoneme'].unique()))\n",
    "    print(\"\\nLVT Unique Coda Phonemes:\")\n",
    "    print(sorted(lvt_df['coda_phoneme'].unique()))\n",
    "\n",
    "# Run the analysis\n",
    "analyze_phonemes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Segmentation\n",
    "\n",
    "Using the master dataset's onset times, cut each continuous audio recording into individual \"boxemes\" (isolated vocal percussion sounds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def segment_audio(master_df, output_dir, segment_duration=0.5):\n",
    "    \"\"\"\n",
    "    Segments audio files and saves them to disk.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    segment_info = []\n",
    "    \n",
    "    # Group by wav_file_path\n",
    "    for wav_path, group in tqdm(master_df.groupby('wav_file_path')):\n",
    "        print(f\"\\nProcessing {wav_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Use soundfile instead of librosa.load\n",
    "            y, sr = sf.read(wav_path)\n",
    "            \n",
    "            # Process each onset in this file\n",
    "            for idx, row in group.iterrows():\n",
    "                start_sample = int(row['onset_time'] * sr)\n",
    "                end_sample = start_sample + int(segment_duration * sr)\n",
    "                \n",
    "                # Handle edge cases\n",
    "                if start_sample < 0:\n",
    "                    start_sample = 0\n",
    "                if end_sample > len(y):\n",
    "                    end_sample = len(y)\n",
    "                \n",
    "                if end_sample > start_sample:\n",
    "                    segment = y[start_sample:end_sample]\n",
    "                    \n",
    "                    # Pad if needed\n",
    "                    if len(segment) < int(segment_duration * sr):\n",
    "                        segment = np.pad(segment, \n",
    "                                      (0, int(segment_duration * sr) - len(segment)),\n",
    "                                      mode='constant')\n",
    "                    \n",
    "                    segment_filename = (f\"{row['dataset']}_{row['participant_id']}_\"\n",
    "                                     f\"{row['instrument_label']}_{idx:04d}.wav\")\n",
    "                    \n",
    "                    segment_path = output_dir / segment_filename\n",
    "                    sf.write(str(segment_path), segment, sr)\n",
    "                    \n",
    "                    segment_info.append({\n",
    "                        'segment_path': str(segment_path),\n",
    "                        'instrument_label': row['instrument_label'],\n",
    "                        'participant_id': row['participant_id'],\n",
    "                        'dataset': row['dataset'],\n",
    "                        'original_wav': wav_path,\n",
    "                        'onset_time': row['onset_time']\n",
    "                    })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {wav_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    segment_df = pd.DataFrame(segment_info)\n",
    "    segment_df.to_csv('EDA/segment_info.csv', index=False)\n",
    "    \n",
    "    return segment_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_csv('EDA/master_dataset.csv')\n",
    "    \n",
    "# Segment all audio files\n",
    "print(\"Starting audio segmentation...\")\n",
    "segment_df = segment_audio(master_df, output_dir='segments')\n",
    "    \n",
    "# Print summary\n",
    "print(\"\\nSegmentation Summary:\")\n",
    "print(f\"Total segments extracted: {len(segment_df)}\")\n",
    "print(\"\\nInstrument distribution:\")\n",
    "print(segment_df['instrument_label'].value_counts())\n",
    "print(\"\\nDataset distribution:\")\n",
    "print(segment_df['dataset'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment Segments via techniques in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def augment_segments(input_dir='segments', output_dir='augmentedSegments', num_augmentations=5):\n",
    "    \"\"\"\n",
    "    Augment audio segments using pitch shifting and time stretching.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Directory containing original segments\n",
    "        output_dir (str): Directory to save augmented segments\n",
    "        num_augmentations (int): Number of augmented versions to create per segment\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Load original segment info\n",
    "    original_info = pd.read_csv('EDA/segment_info.csv')\n",
    "    augmented_info = []\n",
    "    \n",
    "    # Copy original segments and their info\n",
    "    print(\"Copying original segments...\")\n",
    "    for _, row in tqdm(original_info.iterrows()):\n",
    "        orig_path = Path(row['segment_path'])\n",
    "        new_path = output_path / orig_path.name\n",
    "        \n",
    "        # Copy the audio file\n",
    "        y, sr = sf.read(orig_path)\n",
    "        sf.write(str(new_path), y, sr)\n",
    "        \n",
    "        # Add original file info to augmented dataset\n",
    "        augmented_info.append({\n",
    "            'segment_path': str(new_path),\n",
    "            'instrument_label': row['instrument_label'],\n",
    "            'participant_id': row['participant_id'],\n",
    "            'dataset': row['dataset'],\n",
    "            'original_wav': row['original_wav'],\n",
    "            'onset_time': row['onset_time']\n",
    "        })\n",
    "    \n",
    "    # Parameters for augmentation\n",
    "    pitch_shift_range = (-1.5, 1.5)  # semitones\n",
    "    time_stretch_range = (0.8, 1.2)   # rate\n",
    "    \n",
    "    # Process each file\n",
    "    print(\"\\nGenerating augmented segments...\")\n",
    "    for _, row in tqdm(original_info.iterrows()):\n",
    "        orig_path = Path(row['segment_path'])\n",
    "        y, sr = librosa.load(orig_path)\n",
    "        \n",
    "        # Create multiple augmented versions\n",
    "        for i in range(num_augmentations):\n",
    "            # Randomly choose augmentation order\n",
    "            if np.random.random() > 0.5:\n",
    "                # Pitch shift then time stretch\n",
    "                pitch_shift = np.random.uniform(*pitch_shift_range)\n",
    "                y_aug = librosa.effects.pitch_shift(y, sr=sr, n_steps=pitch_shift)\n",
    "                \n",
    "                time_stretch = np.random.uniform(*time_stretch_range)\n",
    "                y_aug = librosa.effects.time_stretch(y_aug, rate=time_stretch)\n",
    "            else:\n",
    "                # Time stretch then pitch shift\n",
    "                time_stretch = np.random.uniform(*time_stretch_range)\n",
    "                y_aug = librosa.effects.time_stretch(y, rate=time_stretch)\n",
    "                \n",
    "                pitch_shift = np.random.uniform(*pitch_shift_range)\n",
    "                y_aug = librosa.effects.pitch_shift(y_aug, sr=sr, n_steps=pitch_shift)\n",
    "            \n",
    "            # Generate augmented filename\n",
    "            aug_name = f\"{orig_path.stem}_aug{i+1}.wav\"\n",
    "            aug_path = output_path / aug_name\n",
    "            \n",
    "            # Save augmented audio\n",
    "            sf.write(str(aug_path), y_aug, sr)\n",
    "            \n",
    "            # Add augmented file info\n",
    "            augmented_info.append({\n",
    "                'segment_path': str(aug_path),\n",
    "                'instrument_label': row['instrument_label'],\n",
    "                'participant_id': row['participant_id'],\n",
    "                'dataset': row['dataset'],\n",
    "                'original_wav': row['original_wav'],\n",
    "                'onset_time': row['onset_time']\n",
    "            })\n",
    "    \n",
    "    # Create and save augmented segment info\n",
    "    augmented_df = pd.DataFrame(augmented_info)\n",
    "    augmented_df.to_csv('EDA/segment_info_augmented.csv', index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    total_files = len(augmented_info)\n",
    "    original_count = len(original_info)\n",
    "    print(f\"\\nAugmentation complete!\")\n",
    "    print(f\"Original segments: {original_count}\")\n",
    "    print(f\"Total segments after augmentation: {total_files}\")\n",
    "    print(f\"New segments added: {total_files - original_count}\")\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying original segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5714it [00:02, 2126.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating augmented segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5714it [01:44, 54.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmentation complete!\n",
      "Original segments: 5714\n",
      "Total segments after augmentation: 34284\n",
      "New segments added: 28570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_dir = augment_segments(\"segments\", \"augmentedSegments\", num_augmentations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract MFCC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_features(segment_info_path, segments_dir='segments', n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Extract MFCC features from segmented audio files.\n",
    "    \n",
    "    Args:\n",
    "        segment_info_path: Path to segment_info.csv\n",
    "        segments_dir: Directory containing the audio segments (either 'segments' or 'augmentedSegments')\n",
    "        n_mfcc: Number of MFCC coefficients to compute\n",
    "    \"\"\"\n",
    "    # Load segment info\n",
    "    metadata = pd.read_csv(segment_info_path)\n",
    "    \n",
    "    # Update paths to use the specified segments directory\n",
    "    segments_path = Path(segments_dir)\n",
    "    metadata['segment_path'] = metadata['segment_path'].apply(\n",
    "        lambda x: str(segments_path / Path(x).name))\n",
    "    \n",
    "    # Initialize arrays to store features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"Extracting MFCC features from {segments_dir}...\")\n",
    "    for idx, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "        try:\n",
    "            # Load audio segment\n",
    "            y, sr = librosa.load(row['segment_path'])\n",
    "            \n",
    "            # Extract MFCCs\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "            \n",
    "            # Take mean of each coefficient over time\n",
    "            mfcc_mean = np.mean(mfcc, axis=1)\n",
    "            \n",
    "            features.append(mfcc_mean)\n",
    "            labels.append(row['instrument_label'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['segment_path']}: {str(e)}\")\n",
    "            metadata = metadata.drop(idx)\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Create features directory if it doesn't exist\n",
    "    output_dir = Path('features')\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Save features and labels with directory-specific names\n",
    "    dir_suffix = '_augmented' if segments_dir == 'augmentedSegments' else ''\n",
    "    np.save(output_dir / f'mfcc_features{dir_suffix}.npy', X)\n",
    "    np.save(output_dir / f'labels{dir_suffix}.npy', y)\n",
    "    \n",
    "    return X, y, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MFCC features from segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5714/5714 [00:08<00:00, 704.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MFCC features from augmentedSegments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34284/34284 [00:47<00:00, 723.93it/s]\n"
     ]
    }
   ],
   "source": [
    "segment_info_path = 'EDA/segment_info.csv'\n",
    "augmented_segment_info_path = 'EDA/segment_info_augmented.csv'\n",
    "    \n",
    "    \n",
    "X_orig, y_orig, metadata_orig = extract_mfcc_features(\n",
    "    segment_info_path,\n",
    "    segments_dir='segments'\n",
    ")\n",
    "\n",
    "X_aug, y_aug, metadata_aug = extract_mfcc_features(\n",
    "    augmented_segment_info_path,\n",
    "    segments_dir='augmentedSegments'\n",
    ")\n",
    "\n",
    "    \n",
    "# # Show sample of the features\n",
    "# print(\"\\nSample MFCC features (first 3 segments):\")\n",
    "# print(X[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def cluster_sounds(n_clusters=4, features_path='features'):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering on MFCC features\n",
    "    \"\"\"\n",
    "    # Load the MFCC features\n",
    "    # features_path = Path('../projectFiles/features')\n",
    "    # X = np.load(features_path / 'mfcc_features.npy')\n",
    "    \n",
    "    X = np.load(features_path)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    print(f\"Performing K-means clustering with {n_clusters} clusters...\")\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Save cluster assignments\n",
    "    if features_path == 'features/mfcc_features_augmented.npy':\n",
    "        np.save('features/cluster_labels_augmented.npy', cluster_labels)\n",
    "    else:\n",
    "        np.save('features/cluster_labels.npy', cluster_labels)\n",
    "    \n",
    "    # Print cluster sizes\n",
    "    print(\"\\nCluster sizes:\")\n",
    "    for i in range(n_clusters):\n",
    "        print(f\"Cluster {i}: {np.sum(cluster_labels == i)} sounds\")\n",
    "    \n",
    "    return cluster_labels, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing K-means clustering with 4 clusters...\n",
      "\n",
      "Cluster sizes:\n",
      "Cluster 0: 9371 sounds\n",
      "Cluster 1: 5474 sounds\n",
      "Cluster 2: 11101 sounds\n",
      "Cluster 3: 8338 sounds\n",
      "Performing K-means clustering with 4 clusters...\n",
      "\n",
      "Cluster sizes:\n",
      "Cluster 0: 1026 sounds\n",
      "Cluster 1: 1522 sounds\n",
      "Cluster 2: 1992 sounds\n",
      "Cluster 3: 1174 sounds\n"
     ]
    }
   ],
   "source": [
    "# First run clustering\n",
    "cluster_labels_aug, kmeans_modelaug = cluster_sounds(n_clusters=4, features_path='features/mfcc_features_augmented.npy')\n",
    "\n",
    "cluster_labels_orig, kmeans_model_orig = cluster_sounds(n_clusters=4, features_path='features/mfcc_features.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_clustering(features_path='features', labels_path='labels.npy', cluster_labels_path='cluster_labels.npy'):\n",
    "    \"\"\"\n",
    "    Evaluate clustering results using both internal and external metrics\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    # features_path = Path('../projectFiles/features')\n",
    "    X = np.load(features_path)\n",
    "    y_true = np.load(labels_path)\n",
    "    cluster_labels = np.load(cluster_labels_path)\n",
    "    \n",
    "    # Standardize features (same as in clustering)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # 1. Internal Metrics\n",
    "    silhouette = silhouette_score(X_scaled, cluster_labels)\n",
    "    davies_bouldin = davies_bouldin_score(X_scaled, cluster_labels)\n",
    "    calinski_harabasz = calinski_harabasz_score(X_scaled, cluster_labels)\n",
    "    \n",
    "    print(\"Internal Metrics:\")\n",
    "    print(f\"Silhouette Score: {silhouette:.3f} (ranges from -1 to 1, higher is better)\")\n",
    "    print(f\"Davies-Bouldin Index: {davies_bouldin:.3f} (lower is better)\")\n",
    "    print(f\"Calinski-Harabasz Index: {calinski_harabasz:.3f} (higher is better)\")\n",
    "    \n",
    "    # 2. External Metrics\n",
    "    ari = adjusted_rand_score(y_true, cluster_labels)\n",
    "    nmi = normalized_mutual_info_score(y_true, cluster_labels)\n",
    "    \n",
    "    print(\"\\nExternal Metrics:\")\n",
    "    print(f\"Adjusted Rand Index: {ari:.3f} (ranges from -1 to 1, higher is better)\")\n",
    "    print(f\"Normalized Mutual Information: {nmi:.3f} (ranges from 0 to 1, higher is better)\")\n",
    "    \n",
    "    # 3. Create confusion matrix\n",
    "    unique_labels = np.unique(y_true)\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    confusion_matrix = np.zeros((len(unique_labels), len(unique_clusters)))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        for j, cluster in enumerate(unique_clusters):\n",
    "            confusion_matrix[i, j] = np.sum((y_true == label) & (cluster_labels == cluster))\n",
    "    \n",
    "    # Normalize by row (true labels)\n",
    "    confusion_matrix_normalized = confusion_matrix / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(confusion_matrix_normalized, \n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                xticklabels=[f'Cluster {i}' for i in range(len(unique_clusters))],\n",
    "                yticklabels=unique_labels,\n",
    "                cmap='YlOrRd')\n",
    "    plt.title('Normalized Confusion Matrix:\\nTrue Labels vs Cluster Assignments')\n",
    "    plt.xlabel('Predicted Cluster')\n",
    "    plt.ylabel('True Label')\n",
    "    \n",
    "    # Save confusion matrix\n",
    "    viz_dir = Path(features_path).parent.parent / 'visualization'\n",
    "    viz_dir.mkdir(exist_ok=True, parents=True)\n",
    "    if features_path == 'features/mfcc_features_augmented.npy':\n",
    "        plt.savefig(viz_dir / 'confusion_matrix_augmented.png', dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(viz_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Print cluster composition\n",
    "    print(\"\\nCluster Composition:\")\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_mask = cluster_labels == cluster\n",
    "        print(f\"\\nCluster {cluster}:\")\n",
    "        for label in unique_labels:\n",
    "            count = np.sum((y_true == label) & cluster_mask)\n",
    "            percentage = (count / np.sum(cluster_mask)) * 100\n",
    "            print(f\"{label}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "    return {\n",
    "        'silhouette': silhouette,\n",
    "        'davies_bouldin': davies_bouldin,\n",
    "        'calinski_harabasz': calinski_harabasz,\n",
    "        'ari': ari,\n",
    "        'nmi': nmi,\n",
    "        'confusion_matrix': confusion_matrix,\n",
    "        'confusion_matrix_normalized': confusion_matrix_normalized\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Metrics:\n",
      "Silhouette Score: 0.120 (ranges from -1 to 1, higher is better)\n",
      "Davies-Bouldin Index: 2.136 (lower is better)\n",
      "Calinski-Harabasz Index: 611.699 (higher is better)\n",
      "\n",
      "External Metrics:\n",
      "Adjusted Rand Index: 0.108 (ranges from -1 to 1, higher is better)\n",
      "Normalized Mutual Information: 0.115 (ranges from 0 to 1, higher is better)\n",
      "\n",
      "Cluster Composition:\n",
      "\n",
      "Cluster 0:\n",
      "hhc: 174 samples (17.0%)\n",
      "hho: 312 samples (30.4%)\n",
      "kd: 125 samples (12.2%)\n",
      "sd: 415 samples (40.4%)\n",
      "\n",
      "Cluster 1:\n",
      "hhc: 288 samples (18.9%)\n",
      "hho: 115 samples (7.6%)\n",
      "kd: 951 samples (62.5%)\n",
      "sd: 168 samples (11.0%)\n",
      "\n",
      "Cluster 2:\n",
      "hhc: 519 samples (26.1%)\n",
      "hho: 184 samples (9.2%)\n",
      "kd: 596 samples (29.9%)\n",
      "sd: 693 samples (34.8%)\n",
      "\n",
      "Cluster 3:\n",
      "hhc: 517 samples (44.0%)\n",
      "hho: 398 samples (33.9%)\n",
      "kd: 104 samples (8.9%)\n",
      "sd: 155 samples (13.2%)\n"
     ]
    }
   ],
   "source": [
    "evaluation_results_orig = evaluate_clustering(features_path='features/mfcc_features.npy', labels_path='features/labels.npy', cluster_labels_path='features/cluster_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Metrics:\n",
      "Silhouette Score: 0.118 (ranges from -1 to 1, higher is better)\n",
      "Davies-Bouldin Index: 2.219 (lower is better)\n",
      "Calinski-Harabasz Index: 3998.029 (higher is better)\n",
      "\n",
      "External Metrics:\n",
      "Adjusted Rand Index: 0.076 (ranges from -1 to 1, higher is better)\n",
      "Normalized Mutual Information: 0.076 (ranges from 0 to 1, higher is better)\n",
      "\n",
      "Cluster Composition:\n",
      "\n",
      "Cluster 0:\n",
      "hhc: 3199 samples (34.1%)\n",
      "hho: 2739 samples (29.2%)\n",
      "kd: 1133 samples (12.1%)\n",
      "sd: 2300 samples (24.5%)\n",
      "\n",
      "Cluster 1:\n",
      "hhc: 1704 samples (31.1%)\n",
      "hho: 1342 samples (24.5%)\n",
      "kd: 1078 samples (19.7%)\n",
      "sd: 1350 samples (24.7%)\n",
      "\n",
      "Cluster 2:\n",
      "hhc: 2670 samples (24.1%)\n",
      "hho: 1238 samples (11.2%)\n",
      "kd: 3193 samples (28.8%)\n",
      "sd: 4000 samples (36.0%)\n",
      "\n",
      "Cluster 3:\n",
      "hhc: 1415 samples (17.0%)\n",
      "hho: 735 samples (8.8%)\n",
      "kd: 5252 samples (63.0%)\n",
      "sd: 936 samples (11.2%)\n"
     ]
    }
   ],
   "source": [
    "evaluation_results_aug = evaluate_clustering(features_path='features/mfcc_features_augmented.npy', labels_path='features/labels_augmented.npy', cluster_labels_path='features/cluster_labels_augmented.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Clustering Results from K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_3d_visualization():\n",
    "    \"\"\"\n",
    "    Create 3D visualizations of both true labels and cluster assignments\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    features_path = Path('../projectFiles/features')\n",
    "    X = np.load(features_path / 'mfcc_features.npy')\n",
    "    y = np.load(features_path / 'labels.npy')\n",
    "    cluster_labels = np.load(features_path / 'cluster_labels.npy')\n",
    "    \n",
    "    # Standardize features (same as in clustering)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Reduce to 3 dimensions using PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create visualization directories\n",
    "    viz_dir = features_path.parent / 'visualization'\n",
    "    viz_3d_dir = viz_dir / '3d'\n",
    "    viz_3d_clusters = viz_3d_dir / 'clusters'\n",
    "    viz_3d_true = viz_3d_dir / 'true'\n",
    "    \n",
    "    # Create all directories\n",
    "    for dir_path in [viz_dir, viz_3d_dir, viz_3d_clusters, viz_3d_true]:\n",
    "        dir_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Plot 1: True Labels\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot each instrument type\n",
    "    for label in np.unique(y):\n",
    "        mask = y == label\n",
    "        ax.scatter(X_pca[mask, 0], X_pca[mask, 1], X_pca[mask, 2],\n",
    "                  label=label, alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "    ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%})')\n",
    "    ax.set_title('Drum Sounds with True Labels (3D PCA)')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Save with multiple angles\n",
    "    for angle in [0, 45, 90, 135]:\n",
    "        ax.view_init(elev=20, azim=angle)\n",
    "        plt.savefig(viz_3d_true / f'true_labels_angle_{angle}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 2: Cluster Assignments\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
    "                        c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "    ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%})')\n",
    "    ax.set_title('Drum Sound Clusters (3D PCA)')\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    \n",
    "    # Save with multiple angles\n",
    "    for angle in [0, 45, 90, 135]:\n",
    "        ax.view_init(elev=20, azim=angle)\n",
    "        plt.savefig(viz_3d_clusters / f'clusters_angle_{angle}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    # Print explained variance\n",
    "    total_var = sum(pca.explained_variance_ratio_[:3])\n",
    "    print(f\"\\nTotal variance explained by first 3 PCs: {total_var:.1%}\")\n",
    "    print(\"Individual contributions:\")\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_[:3]):\n",
    "        print(f\"PC{i+1}: {var:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_visualization():\n",
    "    \"\"\"\n",
    "    Create 2D visualizations of both true labels and cluster assignments\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    features_path = Path('../projectFiles/features')\n",
    "    X = np.load(features_path / 'mfcc_features.npy')\n",
    "    cluster_labels = np.load(features_path / 'cluster_labels.npy')\n",
    "    \n",
    "    # Standardize features (same as in clustering)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Use PCA to reduce to 2 dimensions for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create visualization directories\n",
    "    viz_dir = features_path.parent / 'visualization'\n",
    "    viz_2d_dir = viz_dir / '2d'\n",
    "    viz_2d_clusters = viz_2d_dir / 'clusters'\n",
    "    viz_2d_true = viz_2d_dir / 'true'\n",
    "    \n",
    "    # Create all directories\n",
    "    for dir_path in [viz_dir, viz_2d_dir, viz_2d_clusters, viz_2d_true]:\n",
    "        dir_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Create scatter plot for clusters\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                         c=cluster_labels, \n",
    "                         cmap='viridis', \n",
    "                         alpha=0.6)\n",
    "    plt.title('Drum Sound Clusters (PCA Visualization)')\n",
    "    plt.xlabel(f'First Principal Component\\nExplained Variance: {pca.explained_variance_ratio_[0]:.2%}')\n",
    "    plt.ylabel(f'Second Principal Component\\nExplained Variance: {pca.explained_variance_ratio_[1]:.2%}')\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    \n",
    "    # Add cluster centers if kmeans model is available\n",
    "    try:\n",
    "        kmeans = KMeans(n_clusters=len(np.unique(cluster_labels)), random_state=42)\n",
    "        kmeans.fit(X_scaled)\n",
    "        centers_pca = pca.transform(kmeans.cluster_centers_)\n",
    "        plt.scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
    "                   c='red', marker='x', s=200, linewidths=3, \n",
    "                   label='Cluster Centers')\n",
    "        plt.legend()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot cluster centers: {e}\")\n",
    "    \n",
    "    # Save the cluster plot\n",
    "    plt.savefig(viz_2d_clusters / 'cluster_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Optional: Plot true labels if available\n",
    "    try:\n",
    "        y = np.load(features_path / 'labels.npy')\n",
    "        \n",
    "        # Create a second plot with true labels\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for label in np.unique(y):\n",
    "            mask = y == label\n",
    "            plt.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                       label=label, alpha=0.6)\n",
    "        plt.title('Drum Sounds with True Labels (PCA Visualization)')\n",
    "        plt.xlabel(f'First Principal Component\\nExplained Variance: {pca.explained_variance_ratio_[0]:.2%}')\n",
    "        plt.ylabel(f'Second Principal Component\\nExplained Variance: {pca.explained_variance_ratio_[1]:.2%}')\n",
    "        plt.legend()\n",
    "        plt.savefig(viz_2d_true / 'true_labels_visualization.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"No labels file found - skipping true label visualization\")\n",
    "    \n",
    "    # Print explained variance\n",
    "    print(f\"\\nTotal variance explained by first 2 PCs: {sum(pca.explained_variance_ratio_[:2]):.1%}\")\n",
    "    print(\"Individual contributions:\")\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_[:2]):\n",
    "        print(f\"PC{i+1}: {var:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
