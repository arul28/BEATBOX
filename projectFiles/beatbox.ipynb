{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting and Organizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to make a master dataframe with all the relevant data. This master dataframe will contain an entry for every single onset, for every single wav file in the audio file. If an audio file is multiple drum sounds, then there is a single onset for each drum sound, and an single audio file will contirnbute to multiple entries in the dataset. We will have to parse AVP and LVT seperately and then combine them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse AVP csv, get the onset time, instrument label, onset phoneme, coda phoneme, dataset, participant id, subset, csv file path, wav file path\n",
    "def parse_avp_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Parses an AVP CSV with no header, returning a list of dicts.\n",
    "    Each dict has:\n",
    "      - onset_time (float)\n",
    "      - instrument_label (str)\n",
    "      - onset_phoneme (str)\n",
    "      - coda_phoneme (str)\n",
    "      - dataset (str) = \"AVP\"\n",
    "      - participant_id (str)\n",
    "      - subset (str) = \"personal\"\n",
    "      - csv_file_path (str)\n",
    "      - wav_file_path (str)\n",
    "    \"\"\"\n",
    "    # 1) Extract some metadata from the file path\n",
    "    csv_dir = os.path.dirname(csv_path)             # e.g., \"AVP_Dataset/Personal/Participant_1\"\n",
    "    csv_file_name = os.path.basename(csv_path)      # e.g., \"P1_kd.csv\" or \"Participant_1_kd.csv\"\n",
    "    base_name, _ = os.path.splitext(csv_file_name)  # e.g., \"P1_kd\" or \"Participant_1_kd\"\n",
    "    \n",
    "    # 2) Determine participant_id from the file name\n",
    "    #    Suppose your file name is \"P1_kd.csv\", so the participant part is \"P1\".\n",
    "    #    If it’s \"Participant_1_kd.csv\", you might want the first two segments. Adapt as needed.\n",
    "    #    Here's a simple approach:\n",
    "    parts = base_name.split(\"_\")  # e.g. [\"P1\", \"kd\"] or [\"Participant\", \"1\", \"kd\"]\n",
    "    participant_id = parts[0]     # e.g. \"P1\" or \"Participant\"\n",
    "    # If your actual naming is more complicated, tweak the logic. \n",
    "    # For instance, if \"Participant_1\" is always 2 segments, you might do participant_id = \"_\".join(parts[:2])\n",
    "    \n",
    "    # 3) Build the wav path. If the CSV is \"P1_kd.csv\", the wav is \"P1_kd.wav\" in the same folder.\n",
    "    wav_file_name = base_name + \".wav\"\n",
    "    wav_file_path = os.path.join(csv_dir, wav_file_name)\n",
    "    \n",
    "    # 4) We'll fix \"dataset\" = \"AVP\" and \"subset\" = \"personal\" \n",
    "    #    (since that's the arrangement you described for the AVP dataset).\n",
    "    dataset = \"AVP\"\n",
    "    subset = \"personal\"\n",
    "    \n",
    "    # 5) Parse each row of the CSV\n",
    "    data = []\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            # Expecting: [onset_time, instrument_label, onset_phoneme, coda_phoneme]\n",
    "            if len(row) < 2:\n",
    "                continue  # skip empty or malformed lines\n",
    "            onset_time = float(row[0])\n",
    "            instrument_label = row[1]\n",
    "            onset_phoneme = row[2] if len(row) > 2 else ''\n",
    "            coda_phoneme = row[3] if len(row) > 3 else ''\n",
    "            \n",
    "            entry = {\n",
    "                'onset_time': onset_time,\n",
    "                'instrument_label': instrument_label,\n",
    "                'onset_phoneme': onset_phoneme,\n",
    "                'coda_phoneme': coda_phoneme,\n",
    "                'dataset': dataset,\n",
    "                'participant_id': participant_id,\n",
    "                'subset': subset,\n",
    "                'csv_file_path': csv_path,\n",
    "                'wav_file_path': wav_file_path\n",
    "            }\n",
    "            data.append(entry)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to collect all AVP data, takes a root directory as input, walks through the AVP dataset directory and collects all CSV data into a master DataFrame\n",
    "def collect_all_avp_data(root_dir):\n",
    "    \"\"\"\n",
    "    Walks through the AVP dataset directory and collects all CSV data into a master DataFrame,\n",
    "    maintaining the grouping of entries from the same CSV file\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    personal_dir = os.path.join(root_dir, \"Personal\")\n",
    "    \n",
    "    # Walk through all participant directories in sorted order\n",
    "    for participant_dir in sorted(os.listdir(personal_dir)):\n",
    "        participant_path = os.path.join(personal_dir, participant_dir)\n",
    "        \n",
    "        # Skip if not a directory or hidden files\n",
    "        if not os.path.isdir(participant_path) or participant_dir.startswith('.'):\n",
    "            continue\n",
    "            \n",
    "        # Process CSV files in sorted order\n",
    "        for file_name in sorted(os.listdir(participant_path)):\n",
    "            if file_name.endswith('.csv'):\n",
    "                csv_path = os.path.join(participant_path, file_name)\n",
    "                \n",
    "                try:\n",
    "                    parsed_data = parse_avp_csv(csv_path)\n",
    "                    # Add the source filename as a field for sorting\n",
    "                    for entry in parsed_data:\n",
    "                        entry['source_file'] = file_name\n",
    "                    all_data.extend(parsed_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {csv_path}: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Sort to maintain grouping:\n",
    "    # First by participant_id, then by source file, then by onset_time\n",
    "    df = df.sort_values(['participant_id', 'source_file', 'onset_time'])\n",
    "    \n",
    "    # Optionally remove the temporary source_file column if you don't need it\n",
    "    df = df.drop('source_file', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_standardized_phoneme(simple_phoneme, is_onset=True):\n",
    "#     \"\"\"\n",
    "#     Converts LVT phoneme notation to match AVP's IPA notation\n",
    "    \n",
    "#     Args:\n",
    "#         simple_phoneme: The phoneme from LVT dataset\n",
    "#         is_onset: Boolean indicating if this is an onset (True) or coda (False) phoneme\n",
    "#     \"\"\"\n",
    "#     # Onset phonemes (consonants at start)\n",
    "#     onset_map = {\n",
    "#         '!': '!',        # keep as is (appears in both)\n",
    "#         'k': 'k',        # keep as is\n",
    "#         'p': 'p',        # keep as is\n",
    "#         's': 's',        # keep as is\n",
    "#         't': 't',        # keep as is\n",
    "#         'ts': 'ts',      # keep as is\n",
    "#         'tʃ': 'tʃ',      # keep as is\n",
    "#         'ʔ': 'tʃ',       # map glottal stop to tʃ\n",
    "#         'ʡʢ': 'ʡʢ'       # keep as is (appears in both)\n",
    "#     }\n",
    "    \n",
    "#     # Coda phonemes (vowels/endings)\n",
    "#     coda_map = {\n",
    "#         'a': 'æ',        # map 'a' to 'æ' as it's more common in AVP\n",
    "#         'h': 'h',        # keep as is\n",
    "#         'u': 'u',        # keep as is\n",
    "#         'x': 'x',        # keep as is\n",
    "#         'ʊ': 'ʊ'         # keep as is\n",
    "#     }\n",
    "    \n",
    "#     # Choose which mapping to use\n",
    "#     phoneme_map = onset_map if is_onset else coda_map\n",
    "    \n",
    "#     # Return mapped phoneme if it exists, otherwise return original\n",
    "#     return phoneme_map.get(simple_phoneme, simple_phoneme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functiuon to parse LVT csv, get the onset time, instrument label, onset phoneme, coda phoneme, dataset, participant id, subset, csv file path, wav file path\n",
    "def parse_lvt_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Parses an LVT CSV with no header, returning a list of dicts.\n",
    "    Similar to parse_avp_csv but handles LVT-specific formatting.\n",
    "    \"\"\"\n",
    "    # Extract metadata from the file path\n",
    "    csv_dir = os.path.dirname(csv_path)             \n",
    "    csv_file_name = os.path.basename(csv_path)      # e.g., \"AFRP.csv\"\n",
    "    base_name, _ = os.path.splitext(csv_file_name)  # e.g., \"AFRP\"\n",
    "    \n",
    "    # Determine if this is from Frase or Improviso folder\n",
    "    subset = \"Frase\" if \"Frase\" in csv_dir else \"Improviso\"\n",
    "    \n",
    "    # Participant ID is the filename without extension\n",
    "    participant_id = base_name\n",
    "    \n",
    "    # Build the wav path (add \"3\" before .wav)\n",
    "    wav_file_name = base_name + \"3.wav\"\n",
    "    wav_file_path = os.path.join(csv_dir, wav_file_name)\n",
    "    \n",
    "    # Mapping for instrument labels\n",
    "    instrument_map = {\n",
    "        \"Kick\": \"kd\",\n",
    "        \"Snare\": \"sd\",\n",
    "        \"HH\": \"hhc\"  # Assuming all HH in LVT are closed hi-hats\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue  # skip empty or malformed lines\n",
    "                \n",
    "            onset_time = float(row[0])\n",
    "            original_label = row[1]\n",
    "            instrument_label = instrument_map.get(original_label, original_label)\n",
    "            onset_phoneme = row[2] if len(row) > 2 else ''\n",
    "            coda_phoneme = row[3] if len(row) > 3 else ''\n",
    "            \n",
    "            # onset_phoneme = get_standardized_phoneme(row[2], is_onset=True)   # converts 'ts' if needed\n",
    "            # coda_phoneme = get_standardized_phoneme(row[3], is_onset=False)   # converts 'x' if needed\n",
    "            \n",
    "            entry = {\n",
    "                'onset_time': onset_time,\n",
    "                'instrument_label': instrument_label,\n",
    "                'onset_phoneme': onset_phoneme,\n",
    "                'coda_phoneme': coda_phoneme,\n",
    "                'dataset': \"LVT\",\n",
    "                'participant_id': participant_id,\n",
    "                'subset': subset,\n",
    "                'csv_file_path': csv_path,\n",
    "                'wav_file_path': wav_file_path\n",
    "            }\n",
    "            data.append(entry)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to collect all LVT data, takes a root directory as input, walks through the LVT dataset directory and collects all CSV data into a master DataFrame\n",
    "def collect_all_lvt_data(root_dir):\n",
    "    \"\"\"\n",
    "    Walks through the LVT dataset directory and collects all CSV data into a master DataFrame\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # Process both Frase and Improviso folders\n",
    "    for subset_dir in [\"Frase\", \"Improviso\"]:\n",
    "        subset_path = os.path.join(root_dir, subset_dir)\n",
    "        \n",
    "        # Skip if directory doesn't exist\n",
    "        if not os.path.isdir(subset_path):\n",
    "            continue\n",
    "            \n",
    "        # Process CSV files in sorted order\n",
    "        for file_name in sorted(os.listdir(subset_path)):\n",
    "            if file_name.endswith('.csv') and not file_name.startswith('.'):\n",
    "                csv_path = os.path.join(subset_path, file_name)\n",
    "                \n",
    "                try:\n",
    "                    parsed_data = parse_lvt_csv(csv_path)\n",
    "                    # Add source file for sorting\n",
    "                    for entry in parsed_data:\n",
    "                        entry['source_file'] = file_name\n",
    "                    all_data.extend(parsed_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {csv_path}: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Sort to maintain grouping\n",
    "    df = df.sort_values(['subset', 'participant_id', 'source_file', 'onset_time'])\n",
    "    \n",
    "    # Remove temporary sorting column\n",
    "    df = df.drop('source_file', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create all datasets, calls the functions to collect AVP and LVT data and then combines them into a master dataframe\n",
    "def create_all_datasets():\n",
    "    avp_dataset_path = \"../AVP-LVT_Dataset/AVP_Dataset\"\n",
    "    lvt_dataset_path = \"../AVP-LVT_Dataset/LVT_Dataset\"\n",
    "    \n",
    "    # Collect data from both datasets\n",
    "    print(\"Processing AVP dataset...\")\n",
    "    avp_df = collect_all_avp_data(avp_dataset_path)\n",
    "    \n",
    "    print(\"Processing LVT dataset...\")\n",
    "    lvt_df = collect_all_lvt_data(lvt_dataset_path)\n",
    "    \n",
    "    # Save individual datasets\n",
    "    print(\"\\nSaving individual datasets...\")\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs('EDA', exist_ok=True)\n",
    "    \n",
    "    avp_df.to_csv('EDA/avp_dataset.csv', index=False)\n",
    "    lvt_df.to_csv('EDA/lvt_dataset.csv', index=False)\n",
    "    \n",
    "    # Combine and save master dataset\n",
    "    print(\"Creating and saving master dataset...\")\n",
    "    master_df = pd.concat([avp_df, lvt_df], ignore_index=True)\n",
    "    master_df.to_csv('EDA/master_dataset.csv', index=False)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nDataset Summaries:\")\n",
    "    print(f\"AVP Dataset: {len(avp_df)} events\")\n",
    "    print(\"\\nAVP participants:\", len(avp_df['participant_id'].unique()))\n",
    "    print(\"AVP instrument distribution:\")\n",
    "    print(avp_df['instrument_label'].value_counts())\n",
    "    \n",
    "    print(f\"\\nLVT Dataset: {len(lvt_df)} events\")\n",
    "    print(\"LVT subsets:\", lvt_df['subset'].unique())\n",
    "    print(\"LVT participants:\", len(lvt_df['participant_id'].unique()))\n",
    "    print(\"LVT instrument distribution:\")\n",
    "    print(lvt_df['instrument_label'].value_counts())\n",
    "    \n",
    "    print(f\"\\nMaster Dataset: {len(master_df)} total events\")\n",
    "    print(\"Distribution by dataset:\")\n",
    "    print(master_df['dataset'].value_counts())\n",
    "    \n",
    "    return avp_df, lvt_df, master_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AVP dataset...\n",
      "Processing LVT dataset...\n",
      "\n",
      "Saving individual datasets...\n",
      "Creating and saving master dataset...\n",
      "\n",
      "Dataset Summaries:\n",
      "AVP Dataset: 4873 events\n",
      "\n",
      "AVP participants: 28\n",
      "AVP instrument distribution:\n",
      "instrument_label\n",
      "kd     1447\n",
      "sd     1253\n",
      "hhc    1164\n",
      "hho    1009\n",
      "Name: count, dtype: int64\n",
      "\n",
      "LVT Dataset: 841 events\n",
      "LVT subsets: ['Frase' 'Improviso']\n",
      "LVT participants: 40\n",
      "LVT instrument distribution:\n",
      "instrument_label\n",
      "hhc    334\n",
      "kd     329\n",
      "sd     178\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Master Dataset: 5714 total events\n",
      "Distribution by dataset:\n",
      "dataset\n",
      "AVP    4873\n",
      "LVT     841\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "avp_df, lvt_df, master_df = create_all_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have master_dataset.csv, and master_df, both of which contain the info for every single onset for every single sound in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVP Unique Onset Phonemes:\n",
      "['!', 'dʒ', 'k', 'kg', 'kʃ', 'p', 's', 't', 'ts', 'tɕ', 'tʃ', 'tʒ', 'ʡʢ']\n",
      "\n",
      "AVP Unique Coda Phonemes:\n",
      "['I', 'a', 'e', 'h', 'i', 'o', 'u', 'x', 'æ', 'œ', 'ɐ', 'ɘ', 'ə', 'ɪ', 'ɯ', 'ʊ', 'ʌ']\n",
      "\n",
      "LVT Unique Onset Phonemes:\n",
      "['!', 'k', 'p', 's', 't', 'ts', 'tʃ', 'ʔ', 'ʡʢ']\n",
      "\n",
      "LVT Unique Coda Phonemes:\n",
      "['a', 'h', 'u', 'x', 'ʊ']\n"
     ]
    }
   ],
   "source": [
    "def analyze_phonemes():\n",
    "    \"\"\"\n",
    "    Analyze and compare phonemes between AVP and LVT datasets\n",
    "    \"\"\"\n",
    "    # Read both datasets\n",
    "    avp_df = pd.read_csv('EDA/avp_dataset.csv')\n",
    "    lvt_df = pd.read_csv('EDA/lvt_dataset.csv')\n",
    "    \n",
    "    print(\"AVP Unique Onset Phonemes:\")\n",
    "    print(sorted(avp_df['onset_phoneme'].unique()))\n",
    "    print(\"\\nAVP Unique Coda Phonemes:\")\n",
    "    print(sorted(avp_df['coda_phoneme'].unique()))\n",
    "    \n",
    "    print(\"\\nLVT Unique Onset Phonemes:\")\n",
    "    print(sorted(lvt_df['onset_phoneme'].unique()))\n",
    "    print(\"\\nLVT Unique Coda Phonemes:\")\n",
    "    print(sorted(lvt_df['coda_phoneme'].unique()))\n",
    "\n",
    "# Run the analysis\n",
    "analyze_phonemes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Segmentation\n",
    "\n",
    "Using the master dataset's onset times, cut each continuous audio recording into individual \"boxemes\" (isolated vocal percussion sounds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def segment_audio(master_df, output_dir, segment_duration=0.5):\n",
    "    \"\"\"\n",
    "    Segments audio files and saves them to disk.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    segment_info = []\n",
    "    \n",
    "    # Group by wav_file_path\n",
    "    for wav_path, group in tqdm(master_df.groupby('wav_file_path')):\n",
    "        print(f\"\\nProcessing {wav_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Use soundfile instead of librosa.load\n",
    "            y, sr = sf.read(wav_path)\n",
    "            \n",
    "            # Process each onset in this file\n",
    "            for idx, row in group.iterrows():\n",
    "                start_sample = int(row['onset_time'] * sr)\n",
    "                end_sample = start_sample + int(segment_duration * sr)\n",
    "                \n",
    "                # Handle edge cases\n",
    "                if start_sample < 0:\n",
    "                    start_sample = 0\n",
    "                if end_sample > len(y):\n",
    "                    end_sample = len(y)\n",
    "                \n",
    "                if end_sample > start_sample:\n",
    "                    segment = y[start_sample:end_sample]\n",
    "                    \n",
    "                    # Pad if needed\n",
    "                    if len(segment) < int(segment_duration * sr):\n",
    "                        segment = np.pad(segment, \n",
    "                                      (0, int(segment_duration * sr) - len(segment)),\n",
    "                                      mode='constant')\n",
    "                    \n",
    "                    segment_filename = (f\"{row['dataset']}_{row['participant_id']}_\"\n",
    "                                     f\"{row['instrument_label']}_{idx:04d}.wav\")\n",
    "                    \n",
    "                    segment_path = output_dir / segment_filename\n",
    "                    sf.write(str(segment_path), segment, sr)\n",
    "                    \n",
    "                    segment_info.append({\n",
    "                        'segment_path': str(segment_path),\n",
    "                        'instrument_label': row['instrument_label'],\n",
    "                        'participant_id': row['participant_id'],\n",
    "                        'dataset': row['dataset'],\n",
    "                        'original_wav': wav_path,\n",
    "                        'onset_time': row['onset_time']\n",
    "                    })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {wav_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    segment_df = pd.DataFrame(segment_info)\n",
    "    segment_df.to_csv('EDA/segment_info.csv', index=False)\n",
    "    \n",
    "    return segment_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_csv('EDA/master_dataset.csv')\n",
    "    \n",
    "# Segment all audio files\n",
    "print(\"Starting audio segmentation...\")\n",
    "segment_df = segment_audio(master_df, output_dir='segments')\n",
    "    \n",
    "# Print summary\n",
    "print(\"\\nSegmentation Summary:\")\n",
    "print(f\"Total segments extracted: {len(segment_df)}\")\n",
    "print(\"\\nInstrument distribution:\")\n",
    "print(segment_df['instrument_label'].value_counts())\n",
    "print(\"\\nDataset distribution:\")\n",
    "print(segment_df['dataset'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment Segments via techniques in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def augment_segments(input_dir='segments', output_dir='augmentedSegments', num_augmentations=5):\n",
    "    \"\"\"\n",
    "    Augment audio segments using pitch shifting and time stretching.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Directory containing original segments\n",
    "        output_dir (str): Directory to save augmented segments\n",
    "        num_augmentations (int): Number of augmented versions to create per segment\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Load original segment info\n",
    "    original_info = pd.read_csv('EDA/segment_info.csv')\n",
    "    augmented_info = []\n",
    "    \n",
    "    # Copy original segments and their info\n",
    "    print(\"Copying original segments...\")\n",
    "    for _, row in tqdm(original_info.iterrows()):\n",
    "        orig_path = Path(row['segment_path'])\n",
    "        new_path = output_path / orig_path.name\n",
    "        \n",
    "        # Copy the audio file\n",
    "        y, sr = sf.read(orig_path)\n",
    "        sf.write(str(new_path), y, sr)\n",
    "        \n",
    "        # Add original file info to augmented dataset\n",
    "        augmented_info.append({\n",
    "            'segment_path': str(new_path),\n",
    "            'instrument_label': row['instrument_label'],\n",
    "            'participant_id': row['participant_id'],\n",
    "            'dataset': row['dataset'],\n",
    "            'original_wav': row['original_wav'],\n",
    "            'onset_time': row['onset_time']\n",
    "        })\n",
    "    \n",
    "    # Parameters for augmentation\n",
    "    pitch_shift_range = (-1.5, 1.5)  # semitones\n",
    "    time_stretch_range = (0.8, 1.2)   # rate\n",
    "    \n",
    "    # Process each file\n",
    "    print(\"\\nGenerating augmented segments...\")\n",
    "    for _, row in tqdm(original_info.iterrows()):\n",
    "        orig_path = Path(row['segment_path'])\n",
    "        y, sr = librosa.load(orig_path)\n",
    "        \n",
    "        # Create multiple augmented versions\n",
    "        for i in range(num_augmentations):\n",
    "            # Randomly choose augmentation order\n",
    "            if np.random.random() > 0.5:\n",
    "                # Pitch shift then time stretch\n",
    "                pitch_shift = np.random.uniform(*pitch_shift_range)\n",
    "                y_aug = librosa.effects.pitch_shift(y, sr=sr, n_steps=pitch_shift)\n",
    "                \n",
    "                time_stretch = np.random.uniform(*time_stretch_range)\n",
    "                y_aug = librosa.effects.time_stretch(y_aug, rate=time_stretch)\n",
    "            else:\n",
    "                # Time stretch then pitch shift\n",
    "                time_stretch = np.random.uniform(*time_stretch_range)\n",
    "                y_aug = librosa.effects.time_stretch(y, rate=time_stretch)\n",
    "                \n",
    "                pitch_shift = np.random.uniform(*pitch_shift_range)\n",
    "                y_aug = librosa.effects.pitch_shift(y_aug, sr=sr, n_steps=pitch_shift)\n",
    "            \n",
    "            # Generate augmented filename\n",
    "            aug_name = f\"{orig_path.stem}_aug{i+1}.wav\"\n",
    "            aug_path = output_path / aug_name\n",
    "            \n",
    "            # Save augmented audio\n",
    "            sf.write(str(aug_path), y_aug, sr)\n",
    "            \n",
    "            # Add augmented file info\n",
    "            augmented_info.append({\n",
    "                'segment_path': str(aug_path),\n",
    "                'instrument_label': row['instrument_label'],\n",
    "                'participant_id': row['participant_id'],\n",
    "                'dataset': row['dataset'],\n",
    "                'original_wav': row['original_wav'],\n",
    "                'onset_time': row['onset_time']\n",
    "            })\n",
    "    \n",
    "    # Create and save augmented segment info\n",
    "    augmented_df = pd.DataFrame(augmented_info)\n",
    "    augmented_df.to_csv('EDA/segment_info_augmented.csv', index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    total_files = len(augmented_info)\n",
    "    original_count = len(original_info)\n",
    "    print(f\"\\nAugmentation complete!\")\n",
    "    print(f\"Original segments: {original_count}\")\n",
    "    print(f\"Total segments after augmentation: {total_files}\")\n",
    "    print(f\"New segments added: {total_files - original_count}\")\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying original segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5714it [00:02, 2126.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating augmented segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5714it [01:44, 54.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmentation complete!\n",
      "Original segments: 5714\n",
      "Total segments after augmentation: 34284\n",
      "New segments added: 28570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "augmented_dir = augment_segments(\"segments\", \"augmentedSegments\", num_augmentations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract MFCC Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_features(segment_info_path, segments_dir='segments', n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Extract MFCC features from segmented audio files.\n",
    "    \n",
    "    Args:\n",
    "        segment_info_path: Path to segment_info.csv\n",
    "        segments_dir: Directory containing the audio segments (either 'segments' or 'augmentedSegments')\n",
    "        n_mfcc: Number of MFCC coefficients to compute\n",
    "    \"\"\"\n",
    "    # Load segment info\n",
    "    metadata = pd.read_csv(segment_info_path)\n",
    "    \n",
    "    # Update paths to use the specified segments directory\n",
    "    segments_path = Path(segments_dir)\n",
    "    metadata['segment_path'] = metadata['segment_path'].apply(\n",
    "        lambda x: str(segments_path / Path(x).name))\n",
    "    \n",
    "    # Initialize arrays to store features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"Extracting MFCC features from {segments_dir}...\")\n",
    "    for idx, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "        try:\n",
    "            # Load audio segment\n",
    "            y, sr = librosa.load(row['segment_path'])\n",
    "            \n",
    "            # Extract MFCCs\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "            \n",
    "            # Take mean of each coefficient over time\n",
    "            mfcc_mean = np.mean(mfcc, axis=1)\n",
    "            \n",
    "            features.append(mfcc_mean)\n",
    "            labels.append(row['instrument_label'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['segment_path']}: {str(e)}\")\n",
    "            metadata = metadata.drop(idx)\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Create features directory if it doesn't exist\n",
    "    output_dir = Path('features')\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Save features and labels with directory-specific names\n",
    "    dir_suffix = '_augmented' if segments_dir == 'augmentedSegments' else ''\n",
    "    np.save(output_dir / f'mfcc_features{dir_suffix}.npy', X)\n",
    "    np.save(output_dir / f'labels{dir_suffix}.npy', y)\n",
    "    \n",
    "    return X, y, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_features_expanded(segment_info_path, segments_dir='segments', n_mfcc=14):\n",
    "    \"\"\"\n",
    "    Extract expanded feature set including MFCCs, their deltas, and envelope descriptors.\n",
    "    Added robustness checks for empty or corrupted audio segments.\n",
    "    \"\"\"\n",
    "    # Load segment info\n",
    "    metadata = pd.read_csv(segment_info_path)\n",
    "    \n",
    "    # Update paths to use the specified segments directory\n",
    "    segments_path = Path(segments_dir)\n",
    "    metadata['segment_path'] = metadata['segment_path'].apply(\n",
    "        lambda x: str(segments_path / Path(x).name))\n",
    "    \n",
    "    # Initialize arrays to store features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"Extracting expanded feature set from {segments_dir}...\")\n",
    "    for idx, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "        try:\n",
    "            # Load audio segment\n",
    "            y, sr = librosa.load(row['segment_path'])\n",
    "            \n",
    "            # Check if the audio segment is valid\n",
    "            if len(y) == 0:\n",
    "                print(f\"Skipping empty audio file: {row['segment_path']}\")\n",
    "                metadata = metadata.drop(idx)\n",
    "                continue\n",
    "                \n",
    "            # 1. Extract MFCCs and their statistics\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "            mfcc_mean = np.mean(mfcc, axis=1)\n",
    "            \n",
    "            # 2. Compute MFCC deltas (first derivatives)\n",
    "            mfcc_delta = librosa.feature.delta(mfcc)\n",
    "            mfcc_delta_mean = np.mean(mfcc_delta, axis=1)\n",
    "            \n",
    "            # 3. Extract envelope-based descriptors\n",
    "            # Find the amplitude envelope\n",
    "            envelope = np.abs(y)\n",
    "            \n",
    "            # Safety check for empty envelope\n",
    "            if len(envelope) == 0:\n",
    "                print(f\"Skipping file with empty envelope: {row['segment_path']}\")\n",
    "                metadata = metadata.drop(idx)\n",
    "                continue\n",
    "            \n",
    "            # Find the maximum amplitude and its position\n",
    "            max_amp_pos = np.argmax(envelope)\n",
    "            max_amp = envelope[max_amp_pos]\n",
    "            \n",
    "            # 3.1 Maximum derivative before the maximum amplitude\n",
    "            pre_max_deriv = 0\n",
    "            if max_amp_pos > 0:\n",
    "                pre_envelope = envelope[:max_amp_pos]\n",
    "                if len(pre_envelope) > 1:  # Need at least 2 points for diff\n",
    "                    pre_max_deriv = np.max(np.diff(pre_envelope))\n",
    "            \n",
    "            # 3.2 Derivative after the maximum amplitude\n",
    "            post_max_deriv = 0\n",
    "            if max_amp_pos < len(envelope)-1:\n",
    "                post_envelope = envelope[max_amp_pos:]\n",
    "                if len(post_envelope) > 1:  # Need at least 2 points for diff\n",
    "                    post_max_deriv = np.min(np.diff(post_envelope))\n",
    "            \n",
    "            # 3.3 Temporal centroid\n",
    "            times = np.arange(len(y))\n",
    "            # Avoid division by zero\n",
    "            env_sum = np.sum(envelope)\n",
    "            if env_sum > 0:\n",
    "                temporal_centroid = np.sum(times * envelope) / env_sum\n",
    "                temporal_centroid_ratio = temporal_centroid / len(y)\n",
    "            else:\n",
    "                temporal_centroid_ratio = 0.5  # Default to middle if envelope is all zeros\n",
    "            \n",
    "            # 3.4 Flatness coefficient (spectral flatness as a proxy)\n",
    "            # Handle potential warnings from librosa\n",
    "            with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                flatness = librosa.feature.spectral_flatness(y=y)[0].mean()\n",
    "                flatness = 0.0 if np.isnan(flatness) else flatness\n",
    "            \n",
    "            # Combine all features\n",
    "            feature_vector = np.concatenate([\n",
    "                mfcc_mean,                    # 14 features\n",
    "                mfcc_delta_mean,              # 14 features\n",
    "                [pre_max_deriv,               # 1 feature\n",
    "                 post_max_deriv,              # 1 feature\n",
    "                 flatness,                    # 1 feature\n",
    "                 temporal_centroid_ratio]      # 1 feature\n",
    "            ])\n",
    "            \n",
    "            features.append(feature_vector)\n",
    "            labels.append(row['instrument_label'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['segment_path']}: {str(e)}\")\n",
    "            metadata = metadata.drop(idx)\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Print summary of processing\n",
    "    print(f\"\\nProcessing complete:\")\n",
    "    print(f\"Successfully processed: {len(features)} segments\")\n",
    "    print(f\"Failed/Skipped: {len(metadata.index) - len(features)} segments\")\n",
    "    \n",
    "    # Create features directory if it doesn't exist\n",
    "    output_dir = Path('features')\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Save features and labels with directory-specific names\n",
    "    dir_suffix = '_augmented' if segments_dir == 'augmentedSegments' else ''\n",
    "    np.save(output_dir / f'mfcc_features_expanded{dir_suffix}.npy', X)\n",
    "    np.save(output_dir / f'labels_expanded{dir_suffix}.npy', y)\n",
    "    \n",
    "    return X, y, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MFCC features from segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5714/5714 [00:09<00:00, 632.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MFCC features from augmentedSegments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34284/34284 [00:46<00:00, 736.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting expanded feature set from segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5714/5714 [00:10<00:00, 519.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete:\n",
      "Successfully processed: 5714 segments\n",
      "Failed/Skipped: 0 segments\n",
      "Extracting expanded feature set from augmentedSegments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34284/34284 [01:02<00:00, 544.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete:\n",
      "Successfully processed: 34284 segments\n",
      "Failed/Skipped: 0 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "segment_info_path = 'EDA/segment_info.csv'\n",
    "augmented_segment_info_path = 'EDA/segment_info_augmented.csv'\n",
    "    \n",
    "    \n",
    "X_orig, y_orig, metadata_orig = extract_mfcc_features(\n",
    "    segment_info_path,\n",
    "    segments_dir='segments'\n",
    ")\n",
    "\n",
    "X_aug, y_aug, metadata_aug = extract_mfcc_features(\n",
    "    augmented_segment_info_path,\n",
    "    segments_dir='augmentedSegments'\n",
    ")\n",
    "\n",
    "X_orig_expanded, y_orig_expanded, metadata_orig_expanded = extract_mfcc_features_expanded(\n",
    "    segment_info_path,\n",
    "    segments_dir='segments'\n",
    ")\n",
    "\n",
    "X_aug_expanded, y_aug_expanded, metadata_aug_expanded = extract_mfcc_features_expanded(\n",
    "    augmented_segment_info_path,\n",
    "    segments_dir='augmentedSegments'\n",
    ")\n",
    "\n",
    "    \n",
    "# # Show sample of the features\n",
    "# print(\"\\nSample MFCC features (first 3 segments):\")\n",
    "# print(X[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Important Features (Random Forest):\n",
      "     Feature  RF_Importance\n",
      "1     mfcc_2       0.086723\n",
      "15   delta_2       0.085599\n",
      "2     mfcc_3       0.068139\n",
      "3     mfcc_4       0.044547\n",
      "30  flatness       0.041299\n",
      "4     mfcc_5       0.036626\n",
      "6     mfcc_7       0.034778\n",
      "16   delta_3       0.034503\n",
      "9    mfcc_10       0.033933\n",
      "0     mfcc_1       0.030908\n",
      "\n",
      "PCA Explained Variance Ratio:\n",
      "First 5 components explain: 39.10% of variance\n",
      "\n",
      "Highly Correlated Feature Pairs:\n",
      "pre_max_deriv & post_max_deriv: -0.896\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_feature_importance(X, y):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using multiple methods\n",
    "    \"\"\"\n",
    "    # Create feature names for better interpretation\n",
    "    feature_names = (\n",
    "        [f'mfcc_{i+1}' for i in range(14)] +  # 14 MFCC features\n",
    "        [f'delta_{i+1}' for i in range(14)] +  # 14 delta features\n",
    "        ['pre_max_deriv', 'post_max_deriv', 'flatness', 'temporal_centroid']  # 4 envelope features\n",
    "    )\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # 1. PCA Analysis\n",
    "    pca = PCA()\n",
    "    pca.fit(X_scaled)\n",
    "    \n",
    "    # Calculate feature importance based on PCA components\n",
    "    feature_importance_pca = np.abs(pca.components_[0])  # Using first principal component\n",
    "    \n",
    "    # 2. Random Forest Feature Importance\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_scaled, y)\n",
    "    feature_importance_rf = rf.feature_importances_\n",
    "    \n",
    "    # 3. Correlation Analysis\n",
    "    correlation_matrix = np.corrcoef(X_scaled.T)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'PCA_Importance': feature_importance_pca,\n",
    "        'RF_Importance': feature_importance_rf\n",
    "    })\n",
    "    \n",
    "    # Sort by Random Forest importance\n",
    "    results_df = results_df.sort_values('RF_Importance', ascending=False)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Feature Importance\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.bar(range(len(feature_importance_rf)), results_df['RF_Importance'])\n",
    "    plt.xticks(range(len(feature_importance_rf)), results_df['Feature'], rotation=45, ha='right')\n",
    "    plt.title('Feature Importance (Random Forest)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Plot 2: Correlation Heatmap\n",
    "    plt.subplot(2, 1, 2)\n",
    "    sns.heatmap(correlation_matrix, xticklabels=feature_names, yticklabels=feature_names, \n",
    "                cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Print top features\n",
    "    print(\"\\nTop 10 Most Important Features (Random Forest):\")\n",
    "    print(results_df[['Feature', 'RF_Importance']].head(10))\n",
    "    \n",
    "    print(\"\\nPCA Explained Variance Ratio:\")\n",
    "    print(f\"First 5 components explain: {pca.explained_variance_ratio_[:5].cumsum()[-1]:.2%} of variance\")\n",
    "    \n",
    "    # Identify highly correlated features\n",
    "    print(\"\\nHighly Correlated Feature Pairs:\")\n",
    "    for i in range(len(feature_names)):\n",
    "        for j in range(i+1, len(feature_names)):\n",
    "            if abs(correlation_matrix[i,j]) > 0.8:  # Threshold for high correlation\n",
    "                print(f\"{feature_names[i]} & {feature_names[j]}: {correlation_matrix[i,j]:.3f}\")\n",
    "    \n",
    "    return results_df, correlation_matrix, pca\n",
    "\n",
    "# Load the data\n",
    "X_orig_expanded = np.load('features/mfcc_features_expanded.npy')\n",
    "y_orig = np.load('features/labels_expanded.npy')\n",
    "\n",
    "# Run the analysis\n",
    "results_df, correlation_matrix, pca = analyze_feature_importance(X_orig_expanded, y_orig)\n",
    "\n",
    "# Save the plots\n",
    "plt.savefig('visualization/feature_importance_analysis.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_features_optimized(segment_info_path, segments_dir='segments', n_mfcc=14):\n",
    "    \"\"\"\n",
    "    Extract only the most important features based on our analysis.\n",
    "    \"\"\"\n",
    "    # First, get all features\n",
    "    X, y, metadata = extract_mfcc_features_expanded(segment_info_path, segments_dir)\n",
    "    \n",
    "    # Define indices of important features based on analysis\n",
    "    important_feature_indices = [\n",
    "        1,  # mfcc_2\n",
    "        15, # delta_2\n",
    "        2,  # mfcc_3\n",
    "        3,  # mfcc_4\n",
    "        30, # flatness\n",
    "        4,  # mfcc_5\n",
    "        6,  # mfcc_7\n",
    "        16, # delta_3\n",
    "        9,  # mfcc_10\n",
    "        0,  # mfcc_1\n",
    "        # Add a few more that might be useful\n",
    "        29, # temporal_centroid\n",
    "        28  # pre_max_deriv (excluding post_max_deriv due to high correlation)\n",
    "    ]\n",
    "    \n",
    "    # Select only the important features\n",
    "    X_selected = X[:, important_feature_indices]\n",
    "    \n",
    "    # Save the selected features\n",
    "    output_dir = Path('features')\n",
    "    dir_suffix = '_augmented' if segments_dir == 'augmentedSegments' else ''\n",
    "    np.save(output_dir / f'mfcc_features_optimized{dir_suffix}.npy', X_selected)\n",
    "    np.save(output_dir / f'labels{dir_suffix}.npy', y)\n",
    "    \n",
    "    print(f\"Reduced feature dimension from {X.shape[1]} to {X_selected.shape[1]} features\")\n",
    "    return X_selected, y, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting expanded feature set from segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5714/5714 [00:13<00:00, 439.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete:\n",
      "Successfully processed: 5714 segments\n",
      "Failed/Skipped: 0 segments\n",
      "Reduced feature dimension from 32 to 12 features\n",
      "Extracting expanded feature set from augmentedSegments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34284/34284 [01:12<00:00, 471.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete:\n",
      "Successfully processed: 34284 segments\n",
      "Failed/Skipped: 0 segments\n",
      "Reduced feature dimension from 32 to 12 features\n"
     ]
    }
   ],
   "source": [
    "# Extract optimized features for both original and augmented datasets\n",
    "X_orig_opt, y_orig_opt, metadata_orig_opt = extract_mfcc_features_optimized(\n",
    "    segment_info_path,\n",
    "    segments_dir='segments'\n",
    ")\n",
    "\n",
    "X_aug_opt, y_aug_opt, metadata_aug_opt = extract_mfcc_features_optimized(\n",
    "    augmented_segment_info_path,\n",
    "    segments_dir='augmentedSegments'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def cluster_sounds(n_clusters=4, features_path='features'):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering on MFCC features\n",
    "    \"\"\"\n",
    "    # Load the MFCC features\n",
    "    # features_path = Path('../projectFiles/features')\n",
    "    # X = np.load(features_path / 'mfcc_features.npy')\n",
    "    \n",
    "    X = np.load(features_path)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    print(f\"Performing K-means clustering with {n_clusters} clusters...\")\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Save cluster assignments\n",
    "    if features_path == 'features/mfcc_features_augmented.npy':\n",
    "        np.save('features/cluster_labels_augmented.npy', cluster_labels)\n",
    "    if features_path == 'features/mfcc_features.npy':\n",
    "        np.save('features/cluster_labels.npy', cluster_labels)\n",
    "    if features_path == 'features/mfcc_features_expanded.npy':\n",
    "        np.save('features/cluster_labels_expanded.npy', cluster_labels)\n",
    "    else:\n",
    "        np.save('features/cluster_labels_expanded_augmented.npy', cluster_labels)\n",
    "        \n",
    "    if features_path == 'features/mfcc_features_optimized.npy':\n",
    "        np.save('features/cluster_labels_optimized.npy', cluster_labels)\n",
    "    if features_path == 'features/mfcc_features_optimized_augmented.npy':\n",
    "        np.save('features/cluster_labels_optimized_augmented.npy', cluster_labels)\n",
    "    \n",
    "    # Print cluster sizes\n",
    "    print(\"\\nCluster sizes:\")\n",
    "    for i in range(n_clusters):\n",
    "        print(f\"Cluster {i}: {np.sum(cluster_labels == i)} sounds\")\n",
    "    \n",
    "    return cluster_labels, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing K-means clustering with 4 clusters...\n",
      "\n",
      "Cluster sizes:\n",
      "Cluster 0: 9371 sounds\n",
      "Cluster 1: 5474 sounds\n",
      "Cluster 2: 11101 sounds\n",
      "Cluster 3: 8338 sounds\n",
      "Performing K-means clustering with 4 clusters...\n",
      "\n",
      "Cluster sizes:\n",
      "Cluster 0: 1026 sounds\n",
      "Cluster 1: 1522 sounds\n",
      "Cluster 2: 1992 sounds\n",
      "Cluster 3: 1174 sounds\n",
      "Performing K-means clustering with 4 clusters...\n",
      "\n",
      "Cluster sizes:\n",
      "Cluster 0: 1582 sounds\n",
      "Cluster 1: 965 sounds\n",
      "Cluster 2: 1307 sounds\n",
      "Cluster 3: 1860 sounds\n",
      "Performing K-means clustering with 4 clusters...\n",
      "\n",
      "Cluster sizes:\n",
      "Cluster 0: 5225 sounds\n",
      "Cluster 1: 11110 sounds\n",
      "Cluster 2: 7891 sounds\n",
      "Cluster 3: 10058 sounds\n",
      "Performing K-means clustering with 4 clusters...\n",
      "\n",
      "Cluster sizes:\n",
      "Cluster 0: 1428 sounds\n",
      "Cluster 1: 941 sounds\n",
      "Cluster 2: 1311 sounds\n",
      "Cluster 3: 2034 sounds\n",
      "Performing K-means clustering with 4 clusters...\n",
      "\n",
      "Cluster sizes:\n",
      "Cluster 0: 14564 sounds\n",
      "Cluster 1: 9806 sounds\n",
      "Cluster 2: 9554 sounds\n",
      "Cluster 3: 360 sounds\n"
     ]
    }
   ],
   "source": [
    "# First run clustering\n",
    "cluster_labels_aug, kmeans_modelaug = cluster_sounds(n_clusters=4, features_path='features/mfcc_features_augmented.npy')\n",
    "\n",
    "cluster_labels_orig, kmeans_model_orig = cluster_sounds(n_clusters=4, features_path='features/mfcc_features.npy')\n",
    "\n",
    "cluster_labels_orig_expanded, kmeans_model_orig_expanded = cluster_sounds(n_clusters=4, features_path='features/mfcc_features_expanded.npy')\n",
    "\n",
    "cluster_labels_aug_expanded, kmeans_model_aug_expanded = cluster_sounds(n_clusters=4, features_path='features/mfcc_features_expanded_augmented.npy')\n",
    "\n",
    "cluster_labels_opt, kmeans_model_opt = cluster_sounds(n_clusters=4, features_path='features/mfcc_features_optimized.npy')\n",
    "\n",
    "cluster_labels_aug_opt, kmeans_model_aug_opt = cluster_sounds(n_clusters=4, features_path='features/mfcc_features_optimized_augmented.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_clustering(features_path='features', labels_path='labels.npy', cluster_labels_path='cluster_labels.npy'):\n",
    "    \"\"\"\n",
    "    Evaluate clustering results using both internal and external metrics\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    # features_path = Path('../projectFiles/features')\n",
    "    X = np.load(features_path)\n",
    "    y_true = np.load(labels_path)\n",
    "    cluster_labels = np.load(cluster_labels_path)\n",
    "    \n",
    "    # Standardize features (same as in clustering)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # 1. Internal Metrics\n",
    "    silhouette = silhouette_score(X_scaled, cluster_labels)\n",
    "    davies_bouldin = davies_bouldin_score(X_scaled, cluster_labels)\n",
    "    calinski_harabasz = calinski_harabasz_score(X_scaled, cluster_labels)\n",
    "    \n",
    "    print(\"Internal Metrics:\")\n",
    "    print(f\"Silhouette Score: {silhouette:.3f} (ranges from -1 to 1, higher is better)\")\n",
    "    print(f\"Davies-Bouldin Index: {davies_bouldin:.3f} (lower is better)\")\n",
    "    print(f\"Calinski-Harabasz Index: {calinski_harabasz:.3f} (higher is better)\")\n",
    "    \n",
    "    # 2. External Metrics\n",
    "    ari = adjusted_rand_score(y_true, cluster_labels)\n",
    "    nmi = normalized_mutual_info_score(y_true, cluster_labels)\n",
    "    \n",
    "    print(\"\\nExternal Metrics:\")\n",
    "    print(f\"Adjusted Rand Index: {ari:.3f} (ranges from -1 to 1, higher is better)\")\n",
    "    print(f\"Normalized Mutual Information: {nmi:.3f} (ranges from 0 to 1, higher is better)\")\n",
    "    \n",
    "    # 3. Create confusion matrix\n",
    "    unique_labels = np.unique(y_true)\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    confusion_matrix = np.zeros((len(unique_labels), len(unique_clusters)))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        for j, cluster in enumerate(unique_clusters):\n",
    "            confusion_matrix[i, j] = np.sum((y_true == label) & (cluster_labels == cluster))\n",
    "    \n",
    "    # Normalize by row (true labels)\n",
    "    confusion_matrix_normalized = confusion_matrix / confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(confusion_matrix_normalized, \n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                xticklabels=[f'Cluster {i}' for i in range(len(unique_clusters))],\n",
    "                yticklabels=unique_labels,\n",
    "                cmap='YlOrRd')\n",
    "    plt.title('Normalized Confusion Matrix:\\nTrue Labels vs Cluster Assignments')\n",
    "    plt.xlabel('Predicted Cluster')\n",
    "    plt.ylabel('True Label')\n",
    "    \n",
    "    # Save confusion matrix\n",
    "    viz_dir = Path(features_path).parent.parent / 'visualization'\n",
    "    viz_dir.mkdir(exist_ok=True, parents=True)\n",
    "    if features_path == 'features/mfcc_features_augmented.npy':\n",
    "        plt.savefig(viz_dir / 'confusion_matrix_augmented.png', dpi=300, bbox_inches='tight')\n",
    "    if features_path == 'features/mfcc_features_expanded.npy':\n",
    "        plt.savefig(viz_dir / 'confusion_matrix_expanded.png', dpi=300, bbox_inches='tight')\n",
    "    if features_path == 'features/mfcc_features_expanded_augmented.npy':\n",
    "        plt.savefig(viz_dir / 'confusion_matrix_expanded_augmented.png', dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(viz_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Print cluster composition\n",
    "    print(\"\\nCluster Composition:\")\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_mask = cluster_labels == cluster\n",
    "        print(f\"\\nCluster {cluster}:\")\n",
    "        for label in unique_labels:\n",
    "            count = np.sum((y_true == label) & cluster_mask)\n",
    "            percentage = (count / np.sum(cluster_mask)) * 100\n",
    "            print(f\"{label}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "    return {\n",
    "        'silhouette': silhouette,\n",
    "        'davies_bouldin': davies_bouldin,\n",
    "        'calinski_harabasz': calinski_harabasz,\n",
    "        'ari': ari,\n",
    "        'nmi': nmi,\n",
    "        'confusion_matrix': confusion_matrix,\n",
    "        'confusion_matrix_normalized': confusion_matrix_normalized\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Metrics:\n",
      "Silhouette Score: 0.120 (ranges from -1 to 1, higher is better)\n",
      "Davies-Bouldin Index: 2.136 (lower is better)\n",
      "Calinski-Harabasz Index: 611.699 (higher is better)\n",
      "\n",
      "External Metrics:\n",
      "Adjusted Rand Index: 0.108 (ranges from -1 to 1, higher is better)\n",
      "Normalized Mutual Information: 0.115 (ranges from 0 to 1, higher is better)\n",
      "\n",
      "Cluster Composition:\n",
      "\n",
      "Cluster 0:\n",
      "hhc: 174 samples (17.0%)\n",
      "hho: 312 samples (30.4%)\n",
      "kd: 125 samples (12.2%)\n",
      "sd: 415 samples (40.4%)\n",
      "\n",
      "Cluster 1:\n",
      "hhc: 288 samples (18.9%)\n",
      "hho: 115 samples (7.6%)\n",
      "kd: 951 samples (62.5%)\n",
      "sd: 168 samples (11.0%)\n",
      "\n",
      "Cluster 2:\n",
      "hhc: 519 samples (26.1%)\n",
      "hho: 184 samples (9.2%)\n",
      "kd: 596 samples (29.9%)\n",
      "sd: 693 samples (34.8%)\n",
      "\n",
      "Cluster 3:\n",
      "hhc: 517 samples (44.0%)\n",
      "hho: 398 samples (33.9%)\n",
      "kd: 104 samples (8.9%)\n",
      "sd: 155 samples (13.2%)\n"
     ]
    }
   ],
   "source": [
    "evaluation_results_orig = evaluate_clustering(features_path='features/mfcc_features.npy', labels_path='features/labels.npy', cluster_labels_path='features/cluster_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Metrics:\n",
      "Silhouette Score: 0.118 (ranges from -1 to 1, higher is better)\n",
      "Davies-Bouldin Index: 2.219 (lower is better)\n",
      "Calinski-Harabasz Index: 3998.029 (higher is better)\n",
      "\n",
      "External Metrics:\n",
      "Adjusted Rand Index: 0.076 (ranges from -1 to 1, higher is better)\n",
      "Normalized Mutual Information: 0.076 (ranges from 0 to 1, higher is better)\n",
      "\n",
      "Cluster Composition:\n",
      "\n",
      "Cluster 0:\n",
      "hhc: 3199 samples (34.1%)\n",
      "hho: 2739 samples (29.2%)\n",
      "kd: 1133 samples (12.1%)\n",
      "sd: 2300 samples (24.5%)\n",
      "\n",
      "Cluster 1:\n",
      "hhc: 1704 samples (31.1%)\n",
      "hho: 1342 samples (24.5%)\n",
      "kd: 1078 samples (19.7%)\n",
      "sd: 1350 samples (24.7%)\n",
      "\n",
      "Cluster 2:\n",
      "hhc: 2670 samples (24.1%)\n",
      "hho: 1238 samples (11.2%)\n",
      "kd: 3193 samples (28.8%)\n",
      "sd: 4000 samples (36.0%)\n",
      "\n",
      "Cluster 3:\n",
      "hhc: 1415 samples (17.0%)\n",
      "hho: 735 samples (8.8%)\n",
      "kd: 5252 samples (63.0%)\n",
      "sd: 936 samples (11.2%)\n"
     ]
    }
   ],
   "source": [
    "evaluation_results_aug = evaluate_clustering(features_path='features/mfcc_features_augmented.npy', labels_path='features/labels_augmented.npy', cluster_labels_path='features/cluster_labels_augmented.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Metrics:\n",
      "Silhouette Score: 0.075 (ranges from -1 to 1, higher is better)\n",
      "Davies-Bouldin Index: 3.063 (lower is better)\n",
      "Calinski-Harabasz Index: 365.253 (higher is better)\n",
      "\n",
      "External Metrics:\n",
      "Adjusted Rand Index: 0.100 (ranges from -1 to 1, higher is better)\n",
      "Normalized Mutual Information: 0.112 (ranges from 0 to 1, higher is better)\n",
      "\n",
      "Cluster Composition:\n",
      "\n",
      "Cluster 0:\n",
      "hhc: 572 samples (36.2%)\n",
      "hho: 349 samples (22.1%)\n",
      "kd: 156 samples (9.9%)\n",
      "sd: 505 samples (31.9%)\n",
      "\n",
      "Cluster 1:\n",
      "hhc: 199 samples (20.6%)\n",
      "hho: 347 samples (36.0%)\n",
      "kd: 69 samples (7.2%)\n",
      "sd: 350 samples (36.3%)\n",
      "\n",
      "Cluster 2:\n",
      "hhc: 175 samples (13.4%)\n",
      "hho: 94 samples (7.2%)\n",
      "kd: 911 samples (69.7%)\n",
      "sd: 127 samples (9.7%)\n",
      "\n",
      "Cluster 3:\n",
      "hhc: 552 samples (29.7%)\n",
      "hho: 219 samples (11.8%)\n",
      "kd: 640 samples (34.4%)\n",
      "sd: 449 samples (24.1%)\n"
     ]
    }
   ],
   "source": [
    "evaluation_results_expanded = evaluate_clustering(features_path='features/mfcc_features_expanded.npy', labels_path='features/labels_expanded.npy', cluster_labels_path='features/cluster_labels_expanded.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Metrics:\n",
      "Silhouette Score: 0.078 (ranges from -1 to 1, higher is better)\n",
      "Davies-Bouldin Index: 2.809 (lower is better)\n",
      "Calinski-Harabasz Index: 2356.650 (higher is better)\n",
      "\n",
      "External Metrics:\n",
      "Adjusted Rand Index: 0.083 (ranges from -1 to 1, higher is better)\n",
      "Normalized Mutual Information: 0.089 (ranges from 0 to 1, higher is better)\n",
      "\n",
      "Cluster Composition:\n",
      "\n",
      "Cluster 0:\n",
      "hhc: 1688 samples (32.3%)\n",
      "hho: 1458 samples (27.9%)\n",
      "kd: 590 samples (11.3%)\n",
      "sd: 1489 samples (28.5%)\n",
      "\n",
      "Cluster 1:\n",
      "hhc: 3090 samples (27.8%)\n",
      "hho: 1557 samples (14.0%)\n",
      "kd: 3735 samples (33.6%)\n",
      "sd: 2728 samples (24.6%)\n",
      "\n",
      "Cluster 2:\n",
      "hhc: 1063 samples (13.5%)\n",
      "hho: 713 samples (9.0%)\n",
      "kd: 5313 samples (67.3%)\n",
      "sd: 802 samples (10.2%)\n",
      "\n",
      "Cluster 3:\n",
      "hhc: 3147 samples (31.3%)\n",
      "hho: 2326 samples (23.1%)\n",
      "kd: 1018 samples (10.1%)\n",
      "sd: 3567 samples (35.5%)\n"
     ]
    }
   ],
   "source": [
    "evaluation_results_expanded_aug = evaluate_clustering(features_path='features/mfcc_features_expanded_augmented.npy', labels_path='features/labels_expanded_augmented.npy', cluster_labels_path='features/cluster_labels_expanded_augmented.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internal Metrics:\n",
      "Silhouette Score: 0.132 (ranges from -1 to 1, higher is better)\n",
      "Davies-Bouldin Index: 2.148 (lower is better)\n",
      "Calinski-Harabasz Index: 796.187 (higher is better)\n",
      "\n",
      "External Metrics:\n",
      "Adjusted Rand Index: 0.107 (ranges from -1 to 1, higher is better)\n",
      "Normalized Mutual Information: 0.112 (ranges from 0 to 1, higher is better)\n",
      "\n",
      "Cluster Composition:\n",
      "\n",
      "Cluster 0:\n",
      "hhc: 203 samples (14.2%)\n",
      "hho: 127 samples (8.9%)\n",
      "kd: 474 samples (33.2%)\n",
      "sd: 624 samples (43.7%)\n",
      "\n",
      "Cluster 1:\n",
      "hhc: 291 samples (30.9%)\n",
      "hho: 187 samples (19.9%)\n",
      "kd: 238 samples (25.3%)\n",
      "sd: 225 samples (23.9%)\n",
      "\n",
      "Cluster 2:\n",
      "hhc: 205 samples (15.6%)\n",
      "hho: 100 samples (7.6%)\n",
      "kd: 873 samples (66.6%)\n",
      "sd: 133 samples (10.1%)\n",
      "\n",
      "Cluster 3:\n",
      "hhc: 799 samples (39.3%)\n",
      "hho: 595 samples (29.3%)\n",
      "kd: 191 samples (9.4%)\n",
      "sd: 449 samples (22.1%)\n",
      "Internal Metrics:\n",
      "Silhouette Score: 0.143 (ranges from -1 to 1, higher is better)\n",
      "Davies-Bouldin Index: 1.853 (lower is better)\n",
      "Calinski-Harabasz Index: 4485.471 (higher is better)\n",
      "\n",
      "External Metrics:\n",
      "Adjusted Rand Index: 0.081 (ranges from -1 to 1, higher is better)\n",
      "Normalized Mutual Information: 0.082 (ranges from 0 to 1, higher is better)\n",
      "\n",
      "Cluster Composition:\n",
      "\n",
      "Cluster 0:\n",
      "hhc: 4926 samples (33.8%)\n",
      "hho: 3591 samples (24.7%)\n",
      "kd: 2090 samples (14.4%)\n",
      "sd: 3957 samples (27.2%)\n",
      "\n",
      "Cluster 1:\n",
      "hhc: 2616 samples (26.7%)\n",
      "hho: 1625 samples (16.6%)\n",
      "kd: 2357 samples (24.0%)\n",
      "sd: 3208 samples (32.7%)\n",
      "\n",
      "Cluster 2:\n",
      "hhc: 1355 samples (14.2%)\n",
      "hho: 783 samples (8.2%)\n",
      "kd: 6085 samples (63.7%)\n",
      "sd: 1331 samples (13.9%)\n",
      "\n",
      "Cluster 3:\n",
      "hhc: 91 samples (25.3%)\n",
      "hho: 55 samples (15.3%)\n",
      "kd: 124 samples (34.4%)\n",
      "sd: 90 samples (25.0%)\n"
     ]
    }
   ],
   "source": [
    "evaluation_results_opt = evaluate_clustering(features_path='features/mfcc_features_optimized.npy', labels_path='features/labels.npy', cluster_labels_path='features/cluster_labels_optimized.npy')\n",
    "evaluation_results_opt_aug = evaluate_clustering(features_path='features/mfcc_features_optimized_augmented.npy', labels_path='features/labels_augmented.npy', cluster_labels_path='features/cluster_labels_optimized_augmented.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Clustering Results from K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_3d_visualization():\n",
    "    \"\"\"\n",
    "    Create 3D visualizations of both true labels and cluster assignments\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    features_path = Path('../projectFiles/features')\n",
    "    X = np.load(features_path / 'mfcc_features.npy')\n",
    "    y = np.load(features_path / 'labels.npy')\n",
    "    cluster_labels = np.load(features_path / 'cluster_labels.npy')\n",
    "    \n",
    "    # Standardize features (same as in clustering)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Reduce to 3 dimensions using PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create visualization directories\n",
    "    viz_dir = features_path.parent / 'visualization'\n",
    "    viz_3d_dir = viz_dir / '3d'\n",
    "    viz_3d_clusters = viz_3d_dir / 'clusters'\n",
    "    viz_3d_true = viz_3d_dir / 'true'\n",
    "    \n",
    "    # Create all directories\n",
    "    for dir_path in [viz_dir, viz_3d_dir, viz_3d_clusters, viz_3d_true]:\n",
    "        dir_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Plot 1: True Labels\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot each instrument type\n",
    "    for label in np.unique(y):\n",
    "        mask = y == label\n",
    "        ax.scatter(X_pca[mask, 0], X_pca[mask, 1], X_pca[mask, 2],\n",
    "                  label=label, alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "    ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%})')\n",
    "    ax.set_title('Drum Sounds with True Labels (3D PCA)')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Save with multiple angles\n",
    "    for angle in [0, 45, 90, 135]:\n",
    "        ax.view_init(elev=20, azim=angle)\n",
    "        plt.savefig(viz_3d_true / f'true_labels_angle_{angle}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 2: Cluster Assignments\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
    "                        c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "    ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%})')\n",
    "    ax.set_title('Drum Sound Clusters (3D PCA)')\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    \n",
    "    # Save with multiple angles\n",
    "    for angle in [0, 45, 90, 135]:\n",
    "        ax.view_init(elev=20, azim=angle)\n",
    "        plt.savefig(viz_3d_clusters / f'clusters_angle_{angle}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    # Print explained variance\n",
    "    total_var = sum(pca.explained_variance_ratio_[:3])\n",
    "    print(f\"\\nTotal variance explained by first 3 PCs: {total_var:.1%}\")\n",
    "    print(\"Individual contributions:\")\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_[:3]):\n",
    "        print(f\"PC{i+1}: {var:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_visualization():\n",
    "    \"\"\"\n",
    "    Create 2D visualizations of both true labels and cluster assignments\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    features_path = Path('../projectFiles/features')\n",
    "    X = np.load(features_path / 'mfcc_features.npy')\n",
    "    cluster_labels = np.load(features_path / 'cluster_labels.npy')\n",
    "    \n",
    "    # Standardize features (same as in clustering)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Use PCA to reduce to 2 dimensions for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create visualization directories\n",
    "    viz_dir = features_path.parent / 'visualization'\n",
    "    viz_2d_dir = viz_dir / '2d'\n",
    "    viz_2d_clusters = viz_2d_dir / 'clusters'\n",
    "    viz_2d_true = viz_2d_dir / 'true'\n",
    "    \n",
    "    # Create all directories\n",
    "    for dir_path in [viz_dir, viz_2d_dir, viz_2d_clusters, viz_2d_true]:\n",
    "        dir_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Create scatter plot for clusters\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                         c=cluster_labels, \n",
    "                         cmap='viridis', \n",
    "                         alpha=0.6)\n",
    "    plt.title('Drum Sound Clusters (PCA Visualization)')\n",
    "    plt.xlabel(f'First Principal Component\\nExplained Variance: {pca.explained_variance_ratio_[0]:.2%}')\n",
    "    plt.ylabel(f'Second Principal Component\\nExplained Variance: {pca.explained_variance_ratio_[1]:.2%}')\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    \n",
    "    # Add cluster centers if kmeans model is available\n",
    "    try:\n",
    "        kmeans = KMeans(n_clusters=len(np.unique(cluster_labels)), random_state=42)\n",
    "        kmeans.fit(X_scaled)\n",
    "        centers_pca = pca.transform(kmeans.cluster_centers_)\n",
    "        plt.scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
    "                   c='red', marker='x', s=200, linewidths=3, \n",
    "                   label='Cluster Centers')\n",
    "        plt.legend()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not plot cluster centers: {e}\")\n",
    "    \n",
    "    # Save the cluster plot\n",
    "    plt.savefig(viz_2d_clusters / 'cluster_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Optional: Plot true labels if available\n",
    "    try:\n",
    "        y = np.load(features_path / 'labels.npy')\n",
    "        \n",
    "        # Create a second plot with true labels\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for label in np.unique(y):\n",
    "            mask = y == label\n",
    "            plt.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                       label=label, alpha=0.6)\n",
    "        plt.title('Drum Sounds with True Labels (PCA Visualization)')\n",
    "        plt.xlabel(f'First Principal Component\\nExplained Variance: {pca.explained_variance_ratio_[0]:.2%}')\n",
    "        plt.ylabel(f'Second Principal Component\\nExplained Variance: {pca.explained_variance_ratio_[1]:.2%}')\n",
    "        plt.legend()\n",
    "        plt.savefig(viz_2d_true / 'true_labels_visualization.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"No labels file found - skipping true label visualization\")\n",
    "    \n",
    "    # Print explained variance\n",
    "    print(f\"\\nTotal variance explained by first 2 PCs: {sum(pca.explained_variance_ratio_[:2]):.1%}\")\n",
    "    print(\"Individual contributions:\")\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_[:2]):\n",
    "        print(f\"PC{i+1}: {var:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
