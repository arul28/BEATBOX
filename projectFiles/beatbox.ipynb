{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting and Organizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to make a master dataframe with all the relevant data. This master dataframe will contain an entry for every single onset, for every single wav file in the audio file. If an audio file is multiple drum sounds, then there is a single onset for each drum sound, and an single audio file will contirnbute to multiple entries in the dataset. We will have to parse AVP and LVT seperately and then combine them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse AVP csv, get the onset time, instrument label, onset phoneme, coda phoneme, dataset, participant id, subset, csv file path, wav file path\n",
    "def parse_avp_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Parses an AVP CSV with no header, returning a list of dicts.\n",
    "    Each dict has:\n",
    "      - onset_time (float)\n",
    "      - instrument_label (str)\n",
    "      - onset_phoneme (str)\n",
    "      - coda_phoneme (str)\n",
    "      - dataset (str) = \"AVP\"\n",
    "      - participant_id (str)\n",
    "      - subset (str) = \"personal\"\n",
    "      - csv_file_path (str)\n",
    "      - wav_file_path (str)\n",
    "    \"\"\"\n",
    "    # 1) Extract some metadata from the file path\n",
    "    csv_dir = os.path.dirname(csv_path)             # e.g., \"AVP_Dataset/Personal/Participant_1\"\n",
    "    csv_file_name = os.path.basename(csv_path)      # e.g., \"P1_kd.csv\" or \"Participant_1_kd.csv\"\n",
    "    base_name, _ = os.path.splitext(csv_file_name)  # e.g., \"P1_kd\" or \"Participant_1_kd\"\n",
    "    \n",
    "    # 2) Determine participant_id from the file name\n",
    "    #    Suppose your file name is \"P1_kd.csv\", so the participant part is \"P1\".\n",
    "    #    If it’s \"Participant_1_kd.csv\", you might want the first two segments. Adapt as needed.\n",
    "    #    Here's a simple approach:\n",
    "    parts = base_name.split(\"_\")  # e.g. [\"P1\", \"kd\"] or [\"Participant\", \"1\", \"kd\"]\n",
    "    participant_id = parts[0]     # e.g. \"P1\" or \"Participant\"\n",
    "    # If your actual naming is more complicated, tweak the logic. \n",
    "    # For instance, if \"Participant_1\" is always 2 segments, you might do participant_id = \"_\".join(parts[:2])\n",
    "    \n",
    "    # 3) Build the wav path. If the CSV is \"P1_kd.csv\", the wav is \"P1_kd.wav\" in the same folder.\n",
    "    wav_file_name = base_name + \".wav\"\n",
    "    wav_file_path = os.path.join(csv_dir, wav_file_name)\n",
    "    \n",
    "    # 4) We'll fix \"dataset\" = \"AVP\" and \"subset\" = \"personal\" \n",
    "    #    (since that's the arrangement you described for the AVP dataset).\n",
    "    dataset = \"AVP\"\n",
    "    subset = \"personal\"\n",
    "    \n",
    "    # 5) Parse each row of the CSV\n",
    "    data = []\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            # Expecting: [onset_time, instrument_label, onset_phoneme, coda_phoneme]\n",
    "            if len(row) < 2:\n",
    "                continue  # skip empty or malformed lines\n",
    "            onset_time = float(row[0])\n",
    "            instrument_label = row[1]\n",
    "            onset_phoneme = row[2] if len(row) > 2 else ''\n",
    "            coda_phoneme = row[3] if len(row) > 3 else ''\n",
    "            \n",
    "            entry = {\n",
    "                'onset_time': onset_time,\n",
    "                'instrument_label': instrument_label,\n",
    "                'onset_phoneme': onset_phoneme,\n",
    "                'coda_phoneme': coda_phoneme,\n",
    "                'dataset': dataset,\n",
    "                'participant_id': participant_id,\n",
    "                'subset': subset,\n",
    "                'csv_file_path': csv_path,\n",
    "                'wav_file_path': wav_file_path\n",
    "            }\n",
    "            data.append(entry)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to collect all AVP data, takes a root directory as input, walks through the AVP dataset directory and collects all CSV data into a master DataFrame\n",
    "def collect_all_avp_data(root_dir):\n",
    "    \"\"\"\n",
    "    Walks through the AVP dataset directory and collects all CSV data into a master DataFrame,\n",
    "    maintaining the grouping of entries from the same CSV file\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    personal_dir = os.path.join(root_dir, \"Personal\")\n",
    "    \n",
    "    # Walk through all participant directories in sorted order\n",
    "    for participant_dir in sorted(os.listdir(personal_dir)):\n",
    "        participant_path = os.path.join(personal_dir, participant_dir)\n",
    "        \n",
    "        # Skip if not a directory or hidden files\n",
    "        if not os.path.isdir(participant_path) or participant_dir.startswith('.'):\n",
    "            continue\n",
    "            \n",
    "        # Process CSV files in sorted order\n",
    "        for file_name in sorted(os.listdir(participant_path)):\n",
    "            if file_name.endswith('.csv'):\n",
    "                csv_path = os.path.join(participant_path, file_name)\n",
    "                \n",
    "                try:\n",
    "                    parsed_data = parse_avp_csv(csv_path)\n",
    "                    # Add the source filename as a field for sorting\n",
    "                    for entry in parsed_data:\n",
    "                        entry['source_file'] = file_name\n",
    "                    all_data.extend(parsed_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {csv_path}: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Sort to maintain grouping:\n",
    "    # First by participant_id, then by source file, then by onset_time\n",
    "    df = df.sort_values(['participant_id', 'source_file', 'onset_time'])\n",
    "    \n",
    "    # Optionally remove the temporary source_file column if you don't need it\n",
    "    df = df.drop('source_file', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_standardized_phoneme(simple_phoneme, is_onset=True):\n",
    "#     \"\"\"\n",
    "#     Converts LVT phoneme notation to match AVP's IPA notation\n",
    "    \n",
    "#     Args:\n",
    "#         simple_phoneme: The phoneme from LVT dataset\n",
    "#         is_onset: Boolean indicating if this is an onset (True) or coda (False) phoneme\n",
    "#     \"\"\"\n",
    "#     # Onset phonemes (consonants at start)\n",
    "#     onset_map = {\n",
    "#         '!': '!',        # keep as is (appears in both)\n",
    "#         'k': 'k',        # keep as is\n",
    "#         'p': 'p',        # keep as is\n",
    "#         's': 's',        # keep as is\n",
    "#         't': 't',        # keep as is\n",
    "#         'ts': 'ts',      # keep as is\n",
    "#         'tʃ': 'tʃ',      # keep as is\n",
    "#         'ʔ': 'tʃ',       # map glottal stop to tʃ\n",
    "#         'ʡʢ': 'ʡʢ'       # keep as is (appears in both)\n",
    "#     }\n",
    "    \n",
    "#     # Coda phonemes (vowels/endings)\n",
    "#     coda_map = {\n",
    "#         'a': 'æ',        # map 'a' to 'æ' as it's more common in AVP\n",
    "#         'h': 'h',        # keep as is\n",
    "#         'u': 'u',        # keep as is\n",
    "#         'x': 'x',        # keep as is\n",
    "#         'ʊ': 'ʊ'         # keep as is\n",
    "#     }\n",
    "    \n",
    "#     # Choose which mapping to use\n",
    "#     phoneme_map = onset_map if is_onset else coda_map\n",
    "    \n",
    "#     # Return mapped phoneme if it exists, otherwise return original\n",
    "#     return phoneme_map.get(simple_phoneme, simple_phoneme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functiuon to parse LVT csv, get the onset time, instrument label, onset phoneme, coda phoneme, dataset, participant id, subset, csv file path, wav file path\n",
    "def parse_lvt_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Parses an LVT CSV with no header, returning a list of dicts.\n",
    "    Similar to parse_avp_csv but handles LVT-specific formatting.\n",
    "    \"\"\"\n",
    "    # Extract metadata from the file path\n",
    "    csv_dir = os.path.dirname(csv_path)             \n",
    "    csv_file_name = os.path.basename(csv_path)      # e.g., \"AFRP.csv\"\n",
    "    base_name, _ = os.path.splitext(csv_file_name)  # e.g., \"AFRP\"\n",
    "    \n",
    "    # Determine if this is from Frase or Improviso folder\n",
    "    subset = \"Frase\" if \"Frase\" in csv_dir else \"Improviso\"\n",
    "    \n",
    "    # Participant ID is the filename without extension\n",
    "    participant_id = base_name\n",
    "    \n",
    "    # Build the wav path (add \"3\" before .wav)\n",
    "    wav_file_name = base_name + \"3.wav\"\n",
    "    wav_file_path = os.path.join(csv_dir, wav_file_name)\n",
    "    \n",
    "    # Mapping for instrument labels\n",
    "    instrument_map = {\n",
    "        \"Kick\": \"kd\",\n",
    "        \"Snare\": \"sd\",\n",
    "        \"HH\": \"hhc\"  # Assuming all HH in LVT are closed hi-hats\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue  # skip empty or malformed lines\n",
    "                \n",
    "            onset_time = float(row[0])\n",
    "            original_label = row[1]\n",
    "            instrument_label = instrument_map.get(original_label, original_label)\n",
    "            onset_phoneme = row[2] if len(row) > 2 else ''\n",
    "            coda_phoneme = row[3] if len(row) > 3 else ''\n",
    "            \n",
    "            # onset_phoneme = get_standardized_phoneme(row[2], is_onset=True)   # converts 'ts' if needed\n",
    "            # coda_phoneme = get_standardized_phoneme(row[3], is_onset=False)   # converts 'x' if needed\n",
    "            \n",
    "            entry = {\n",
    "                'onset_time': onset_time,\n",
    "                'instrument_label': instrument_label,\n",
    "                'onset_phoneme': onset_phoneme,\n",
    "                'coda_phoneme': coda_phoneme,\n",
    "                'dataset': \"LVT\",\n",
    "                'participant_id': participant_id,\n",
    "                'subset': subset,\n",
    "                'csv_file_path': csv_path,\n",
    "                'wav_file_path': wav_file_path\n",
    "            }\n",
    "            data.append(entry)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to collect all LVT data, takes a root directory as input, walks through the LVT dataset directory and collects all CSV data into a master DataFrame\n",
    "def collect_all_lvt_data(root_dir):\n",
    "    \"\"\"\n",
    "    Walks through the LVT dataset directory and collects all CSV data into a master DataFrame\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # Process both Frase and Improviso folders\n",
    "    for subset_dir in [\"Frase\", \"Improviso\"]:\n",
    "        subset_path = os.path.join(root_dir, subset_dir)\n",
    "        \n",
    "        # Skip if directory doesn't exist\n",
    "        if not os.path.isdir(subset_path):\n",
    "            continue\n",
    "            \n",
    "        # Process CSV files in sorted order\n",
    "        for file_name in sorted(os.listdir(subset_path)):\n",
    "            if file_name.endswith('.csv') and not file_name.startswith('.'):\n",
    "                csv_path = os.path.join(subset_path, file_name)\n",
    "                \n",
    "                try:\n",
    "                    parsed_data = parse_lvt_csv(csv_path)\n",
    "                    # Add source file for sorting\n",
    "                    for entry in parsed_data:\n",
    "                        entry['source_file'] = file_name\n",
    "                    all_data.extend(parsed_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {csv_path}: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Sort to maintain grouping\n",
    "    df = df.sort_values(['subset', 'participant_id', 'source_file', 'onset_time'])\n",
    "    \n",
    "    # Remove temporary sorting column\n",
    "    df = df.drop('source_file', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create all datasets, calls the functions to collect AVP and LVT data and then combines them into a master dataframe\n",
    "def create_all_datasets():\n",
    "    avp_dataset_path = \"../AVP-LVT_Dataset/AVP_Dataset\"\n",
    "    lvt_dataset_path = \"../AVP-LVT_Dataset/LVT_Dataset\"\n",
    "    \n",
    "    # Collect data from both datasets\n",
    "    print(\"Processing AVP dataset...\")\n",
    "    avp_df = collect_all_avp_data(avp_dataset_path)\n",
    "    \n",
    "    print(\"Processing LVT dataset...\")\n",
    "    lvt_df = collect_all_lvt_data(lvt_dataset_path)\n",
    "    \n",
    "    # Save individual datasets\n",
    "    print(\"\\nSaving individual datasets...\")\n",
    "    avp_df.to_csv('EDA/avp_dataset.csv', index=False)\n",
    "    lvt_df.to_csv('EDA/lvt_dataset.csv', index=False)\n",
    "    \n",
    "    # Combine and save master dataset\n",
    "    print(\"Creating and saving master dataset...\")\n",
    "    master_df = pd.concat([avp_df, lvt_df], ignore_index=True)\n",
    "    master_df.to_csv('EDA/master_dataset.csv', index=False)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nDataset Summaries:\")\n",
    "    print(f\"AVP Dataset: {len(avp_df)} events\")\n",
    "    print(\"\\nAVP participants:\", len(avp_df['participant_id'].unique()))\n",
    "    print(\"AVP instrument distribution:\")\n",
    "    print(avp_df['instrument_label'].value_counts())\n",
    "    \n",
    "    print(f\"\\nLVT Dataset: {len(lvt_df)} events\")\n",
    "    print(\"LVT subsets:\", lvt_df['subset'].unique())\n",
    "    print(\"LVT participants:\", len(lvt_df['participant_id'].unique()))\n",
    "    print(\"LVT instrument distribution:\")\n",
    "    print(lvt_df['instrument_label'].value_counts())\n",
    "    \n",
    "    print(f\"\\nMaster Dataset: {len(master_df)} total events\")\n",
    "    print(\"Distribution by dataset:\")\n",
    "    print(master_df['dataset'].value_counts())\n",
    "    \n",
    "    return avp_df, lvt_df, master_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AVP dataset...\n",
      "Processing LVT dataset...\n",
      "\n",
      "Saving individual datasets...\n",
      "Creating and saving master dataset...\n",
      "\n",
      "Dataset Summaries:\n",
      "AVP Dataset: 4873 events\n",
      "\n",
      "AVP participants: 28\n",
      "AVP instrument distribution:\n",
      "instrument_label\n",
      "kd     1447\n",
      "sd     1253\n",
      "hhc    1164\n",
      "hho    1009\n",
      "Name: count, dtype: int64\n",
      "\n",
      "LVT Dataset: 841 events\n",
      "LVT subsets: ['Frase' 'Improviso']\n",
      "LVT participants: 40\n",
      "LVT instrument distribution:\n",
      "instrument_label\n",
      "hhc    334\n",
      "kd     329\n",
      "sd     178\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Master Dataset: 5714 total events\n",
      "Distribution by dataset:\n",
      "dataset\n",
      "AVP    4873\n",
      "LVT     841\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "avp_df, lvt_df, master_df = create_all_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have master_dataset.csv, and master_df, both of which contain the info for every single onset for every single sound in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVP Unique Onset Phonemes:\n",
      "['!', 'dʒ', 'k', 'kg', 'kʃ', 'p', 's', 't', 'ts', 'tɕ', 'tʃ', 'tʒ', 'ʡʢ']\n",
      "\n",
      "AVP Unique Coda Phonemes:\n",
      "['I', 'a', 'e', 'h', 'i', 'o', 'u', 'x', 'æ', 'œ', 'ɐ', 'ɘ', 'ə', 'ɪ', 'ɯ', 'ʊ', 'ʌ']\n",
      "\n",
      "LVT Unique Onset Phonemes:\n",
      "['!', 'k', 'p', 's', 't', 'ts', 'tʃ', 'ʔ', 'ʡʢ']\n",
      "\n",
      "LVT Unique Coda Phonemes:\n",
      "['a', 'h', 'u', 'x', 'ʊ']\n"
     ]
    }
   ],
   "source": [
    "def analyze_phonemes():\n",
    "    \"\"\"\n",
    "    Analyze and compare phonemes between AVP and LVT datasets\n",
    "    \"\"\"\n",
    "    # Read both datasets\n",
    "    avp_df = pd.read_csv('EDA/avp_dataset.csv')\n",
    "    lvt_df = pd.read_csv('EDA/lvt_dataset.csv')\n",
    "    \n",
    "    print(\"AVP Unique Onset Phonemes:\")\n",
    "    print(sorted(avp_df['onset_phoneme'].unique()))\n",
    "    print(\"\\nAVP Unique Coda Phonemes:\")\n",
    "    print(sorted(avp_df['coda_phoneme'].unique()))\n",
    "    \n",
    "    print(\"\\nLVT Unique Onset Phonemes:\")\n",
    "    print(sorted(lvt_df['onset_phoneme'].unique()))\n",
    "    print(\"\\nLVT Unique Coda Phonemes:\")\n",
    "    print(sorted(lvt_df['coda_phoneme'].unique()))\n",
    "\n",
    "# Run the analysis\n",
    "analyze_phonemes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Segmentation\n",
    "\n",
    "Using the master dataset's onset times, cut each continuous audio recording into individual \"boxemes\" (isolated vocal percussion sounds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def segment_audio(master_df, output_dir, segment_duration=0.5):\n",
    "    \"\"\"\n",
    "    Segments audio files and saves them to disk.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    segment_info = []\n",
    "    \n",
    "    # Group by wav_file_path\n",
    "    for wav_path, group in tqdm(master_df.groupby('wav_file_path')):\n",
    "        print(f\"\\nProcessing {wav_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Use soundfile instead of librosa.load\n",
    "            y, sr = sf.read(wav_path)\n",
    "            \n",
    "            # Process each onset in this file\n",
    "            for idx, row in group.iterrows():\n",
    "                start_sample = int(row['onset_time'] * sr)\n",
    "                end_sample = start_sample + int(segment_duration * sr)\n",
    "                \n",
    "                # Handle edge cases\n",
    "                if start_sample < 0:\n",
    "                    start_sample = 0\n",
    "                if end_sample > len(y):\n",
    "                    end_sample = len(y)\n",
    "                \n",
    "                if end_sample > start_sample:\n",
    "                    segment = y[start_sample:end_sample]\n",
    "                    \n",
    "                    # Pad if needed\n",
    "                    if len(segment) < int(segment_duration * sr):\n",
    "                        segment = np.pad(segment, \n",
    "                                      (0, int(segment_duration * sr) - len(segment)),\n",
    "                                      mode='constant')\n",
    "                    \n",
    "                    segment_filename = (f\"{row['dataset']}_{row['participant_id']}_\"\n",
    "                                     f\"{row['instrument_label']}_{idx:04d}.wav\")\n",
    "                    \n",
    "                    segment_path = output_dir / segment_filename\n",
    "                    sf.write(str(segment_path), segment, sr)\n",
    "                    \n",
    "                    segment_info.append({\n",
    "                        'segment_path': str(segment_path),\n",
    "                        'instrument_label': row['instrument_label'],\n",
    "                        'participant_id': row['participant_id'],\n",
    "                        'dataset': row['dataset'],\n",
    "                        'original_wav': wav_path,\n",
    "                        'onset_time': row['onset_time']\n",
    "                    })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {wav_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    segment_df = pd.DataFrame(segment_info)\n",
    "    segment_df.to_csv('EDA/segment_info.csv', index=False)\n",
    "    \n",
    "    return segment_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting audio segmentation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 10/180 [00:00<00:01, 99.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_1/P1_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_1/P1_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_1/P1_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_1/P1_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_1/P1_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_10/P10_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_10/P10_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_10/P10_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_10/P10_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_10/P10_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_11/P11_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_11/P11_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_11/P11_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_11/P11_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_11/P11_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_12/P12_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_12/P12_HHopened_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 20/180 [00:00<00:02, 70.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_12/P12_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_12/P12_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_12/P12_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_13/P13_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_13/P13_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_13/P13_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_13/P13_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_13/P13_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_14/P14_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_14/P14_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_14/P14_Improvisation_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 38/180 [00:00<00:01, 73.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_14/P14_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_14/P14_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_15/P15_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_15/P15_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_15/P15_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_15/P15_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_15/P15_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_16/P16_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_16/P16_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_16/P16_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_16/P16_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_16/P16_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_17/P17_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_17/P17_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_17/P17_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_17/P17_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_17/P17_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_18/P18_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_18/P18_HHopened_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 47/180 [00:00<00:01, 77.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_18/P18_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_18/P18_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_18/P18_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_19/P19_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_19/P19_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_19/P19_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_19/P19_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_19/P19_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_2/P2_HHclosed_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 56/180 [00:00<00:01, 75.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_2/P2_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_2/P2_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_2/P2_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_2/P2_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_20/P20_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_20/P20_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_20/P20_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_20/P20_Kick_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 66/180 [00:00<00:01, 81.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_20/P20_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_21/P21_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_21/P21_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_21/P21_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_21/P21_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_21/P21_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_22/P22_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_22/P22_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_22/P22_Improvisation_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 75/180 [00:01<00:01, 73.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_22/P22_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_22/P22_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_23/P23_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_23/P23_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_23/P23_Improvisation_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 83/180 [00:01<00:01, 74.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_23/P23_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_23/P23_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_24/P24_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_24/P24_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_24/P24_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_24/P24_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_24/P24_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_25/P25_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_25/P25_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_25/P25_Improvisation_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 91/180 [00:01<00:01, 73.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_25/P25_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_25/P25_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_26/P26_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_26/P26_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_26/P26_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_26/P26_Kick_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 101/180 [00:01<00:00, 79.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_26/P26_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_27/P27_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_27/P27_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_27/P27_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_27/P27_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_27/P27_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_28/P28_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_28/P28_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_28/P28_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_28/P28_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_28/P28_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_3/P3_HHclosed_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 110/180 [00:01<00:00, 80.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_3/P3_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_3/P3_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_3/P3_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_3/P3_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_4/P4_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_4/P4_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_4/P4_Improvisation_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 120/180 [00:01<00:00, 85.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_4/P4_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_4/P4_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_5/P5_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_5/P5_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_5/P5_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_5/P5_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_5/P5_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_6/P6_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_6/P6_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_6/P6_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_6/P6_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_6/P6_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_7/P7_HHclosed_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 129/180 [00:01<00:00, 85.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_7/P7_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_7/P7_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_7/P7_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_7/P7_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_8/P8_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_8/P8_HHopened_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 140/180 [00:01<00:00, 91.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_8/P8_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_8/P8_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_8/P8_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_9/P9_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_9/P9_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_9/P9_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_9/P9_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_9/P9_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/AFRP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/AZiP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/BeaP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/BicP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/CatP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/CavP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/CraP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/IsaP3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 154/180 [00:01<00:00, 103.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/JOlP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/JSiP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/JoSP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/MCoP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/MafP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/MarP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/NorP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/RicP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/RobP3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 167/180 [00:01<00:00, 110.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/SofP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/ZgaP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/ZizP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/AFRI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/AZiI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/BeaI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/BicI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/CatI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/CavI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/CraI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/IsaI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/JOlI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/JSiI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/JSoI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/MCoI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/MafI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/MarI3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:02<00:00, 87.56it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/NorI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/RicI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/RobI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/SofI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/ZgaI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/ZizI3.wav\n",
      "\n",
      "Segmentation Summary:\n",
      "Total segments extracted: 5714\n",
      "\n",
      "Instrument distribution:\n",
      "instrument_label\n",
      "kd     1776\n",
      "hhc    1498\n",
      "sd     1431\n",
      "hho    1009\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset distribution:\n",
      "dataset\n",
      "AVP    4873\n",
      "LVT     841\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "master_df = pd.read_csv('EDA/master_dataset.csv')\n",
    "    \n",
    "# Segment all audio files\n",
    "print(\"Starting audio segmentation...\")\n",
    "segment_df = segment_audio(master_df, output_dir='segments')\n",
    "    \n",
    "# Print summary\n",
    "print(\"\\nSegmentation Summary:\")\n",
    "print(f\"Total segments extracted: {len(segment_df)}\")\n",
    "print(\"\\nInstrument distribution:\")\n",
    "print(segment_df['instrument_label'].value_counts())\n",
    "print(\"\\nDataset distribution:\")\n",
    "print(segment_df['dataset'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc_features(segment_info_path, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Extract MFCC features from segmented audio files.\n",
    "    \n",
    "    Args:\n",
    "        segment_info_path: Path to segment_info.csv\n",
    "        n_mfcc: Number of MFCC coefficients to compute (default 13)\n",
    "    \"\"\"\n",
    "    # Load segment info\n",
    "    metadata = pd.read_csv(segment_info_path)\n",
    "    \n",
    "    # Initialize arrays to store features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    print(\"Extracting MFCC features...\")\n",
    "    for idx, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "        try:\n",
    "            # Load audio segment\n",
    "            y, sr = librosa.load(row['segment_path'])\n",
    "            \n",
    "            # Extract MFCCs\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "            \n",
    "            # Take mean of each coefficient over time\n",
    "            mfcc_mean = np.mean(mfcc, axis=1)\n",
    "            \n",
    "            features.append(mfcc_mean)\n",
    "            labels.append(row['instrument_label'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {row['segment_path']}: {str(e)}\")\n",
    "            metadata = metadata.drop(idx)\n",
    "            continue\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(features)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Create features directory if it doesn't exist\n",
    "    output_dir = Path('../projectFiles/features')\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Save features and labels\n",
    "    np.save(output_dir / 'mfcc_features.npy', X)\n",
    "    np.save(output_dir / 'labels.npy', y)\n",
    "    \n",
    "    # Save summary statistics\n",
    "    print(\"\\nMFCC Feature Extraction Summary:\")\n",
    "    print(f\"Number of segments processed: {len(X)}\")\n",
    "    print(f\"Feature vector dimension: {X.shape[1]}\")\n",
    "    print(\"\\nInstrument distribution:\")\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for instrument, count in zip(unique, counts):\n",
    "        print(f\"{instrument}: {count}\")\n",
    "    \n",
    "    return X, y, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MFCC features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5714/5714 [00:06<00:00, 817.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MFCC Feature Extraction Summary:\n",
      "Number of segments processed: 5714\n",
      "Feature vector dimension: 13\n",
      "\n",
      "Instrument distribution:\n",
      "hhc: 1498\n",
      "hho: 1009\n",
      "kd: 1776\n",
      "sd: 1431\n",
      "\n",
      "Sample MFCC features (first 3 segments):\n",
      "[[-368.12234     13.607724     5.1915603    6.9208407  -20.250082\n",
      "    -9.857764   -22.99795      6.713321     4.099225   -13.504395\n",
      "   -12.596493    -4.041532    -7.4583116]\n",
      " [-462.65042     62.623695     6.7496505    6.1936417  -13.171479\n",
      "   -13.129227   -20.055077     5.5094595    6.369789    -8.635697\n",
      "   -12.423996    -5.821914   -10.663061 ]\n",
      " [-411.08704     34.730343    10.276816    13.407352   -10.680303\n",
      "   -11.195518   -17.607727     1.775097     1.2602392   -7.449579\n",
      "   -10.51625     -3.2223802   -7.674043 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "segment_info_path = 'EDA/segment_info.csv'\n",
    "    \n",
    "# Extract features\n",
    "X, y, metadata = extract_mfcc_features(segment_info_path)\n",
    "    \n",
    "# Show sample of the features\n",
    "print(\"\\nSample MFCC features (first 3 segments):\")\n",
    "print(X[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def cluster_sounds(n_clusters=4):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering on MFCC features with better visualization\n",
    "    \"\"\"\n",
    "    # Load the MFCC features\n",
    "    features_path = Path('../projectFiles/features')\n",
    "    X = np.load(features_path / 'mfcc_features.npy')\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    print(f\"Performing K-means clustering with {n_clusters} clusters...\")\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Save cluster assignments\n",
    "    np.save(features_path / 'cluster_labels.npy', cluster_labels)\n",
    "    \n",
    "    # Print cluster sizes\n",
    "    print(\"\\nCluster sizes:\")\n",
    "    for i in range(n_clusters):\n",
    "        print(f\"Cluster {i}: {np.sum(cluster_labels == i)} sounds\")\n",
    "    \n",
    "    # Use PCA to reduce to 2 dimensions for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create visualization directory\n",
    "    viz_dir = features_path / 'visualizations'\n",
    "    viz_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                         c=cluster_labels, \n",
    "                         cmap='viridis', \n",
    "                         alpha=0.6)\n",
    "    plt.title('Drum Sound Clusters (PCA Visualization)')\n",
    "    plt.xlabel(f'First Principal Component\\nExplained Variance: {pca.explained_variance_ratio_[0]:.2%}')\n",
    "    plt.ylabel(f'Second Principal Component\\nExplained Variance: {pca.explained_variance_ratio_[1]:.2%}')\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    \n",
    "    # Add cluster centers\n",
    "    centers_pca = pca.transform(kmeans.cluster_centers_)\n",
    "    plt.scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
    "               c='red', marker='x', s=200, linewidths=3, \n",
    "               label='Cluster Centers')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(viz_dir / 'cluster_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Optional: Let's also look at the actual labels if we have them\n",
    "    try:\n",
    "        y = np.load(features_path / 'labels.npy')\n",
    "        \n",
    "        # Create a second plot with true labels\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for label in np.unique(y):\n",
    "            mask = y == label\n",
    "            plt.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                       label=label, alpha=0.6)\n",
    "        plt.title('Drum Sounds with True Labels (PCA Visualization)')\n",
    "        plt.xlabel(f'First Principal Component\\nExplained Variance: {pca.explained_variance_ratio_[0]:.2%}')\n",
    "        plt.ylabel(f'Second Principal Component\\nExplained Variance: {pca.explained_variance_ratio_[1]:.2%}')\n",
    "        plt.legend()\n",
    "        plt.savefig(viz_dir / 'true_labels_visualization.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"No labels file found - skipping true label visualization\")\n",
    "    \n",
    "    return cluster_labels, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing K-means clustering with 4 clusters...\n",
      "\n",
      "Cluster sizes:\n",
      "Cluster 0: 1026 sounds\n",
      "Cluster 1: 1522 sounds\n",
      "Cluster 2: 1992 sounds\n",
      "Cluster 3: 1174 sounds\n"
     ]
    }
   ],
   "source": [
    "cluster_labels, kmeans_model = cluster_sounds(n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total variance explained by first 3 PCs: 42.2%\n",
      "Individual contributions:\n",
      "PC1: 16.5%\n",
      "PC2: 14.1%\n",
      "PC3: 11.5%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_3d_visualization():\n",
    "    \"\"\"\n",
    "    Create 3D visualizations of both true labels and cluster assignments\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    features_path = Path('../projectFiles/features')\n",
    "    X = np.load(features_path / 'mfcc_features.npy')\n",
    "    y = np.load(features_path / 'labels.npy')\n",
    "    cluster_labels = np.load(features_path / 'cluster_labels.npy')\n",
    "    \n",
    "    # Standardize features (same as in clustering)\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Reduce to 3 dimensions using PCA\n",
    "    pca = PCA(n_components=3)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create visualization directory\n",
    "    viz_dir = features_path / 'visualizations'\n",
    "    viz_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Plot 1: True Labels\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot each instrument type\n",
    "    for label in np.unique(y):\n",
    "        mask = y == label\n",
    "        ax.scatter(X_pca[mask, 0], X_pca[mask, 1], X_pca[mask, 2],\n",
    "                  label=label, alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "    ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%})')\n",
    "    ax.set_title('Drum Sounds with True Labels (3D PCA)')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Save with multiple angles\n",
    "    for angle in [0, 45, 90, 135]:\n",
    "        ax.view_init(elev=20, azim=angle)\n",
    "        plt.savefig(viz_dir / f'true_labels_3d_angle_{angle}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 2: Cluster Assignments\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
    "                        c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "    ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2]:.1%})')\n",
    "    ax.set_title('Drum Sound Clusters (3D PCA)')\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    \n",
    "    # Save with multiple angles\n",
    "    for angle in [0, 45, 90, 135]:\n",
    "        ax.view_init(elev=20, azim=angle)\n",
    "        plt.savefig(viz_dir / f'clusters_3d_angle_{angle}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    # Print explained variance\n",
    "    total_var = sum(pca.explained_variance_ratio_[:3])\n",
    "    print(f\"\\nTotal variance explained by first 3 PCs: {total_var:.1%}\")\n",
    "    print(\"Individual contributions:\")\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_[:3]):\n",
    "        print(f\"PC{i+1}: {var:.1%}\")\n",
    "\n",
    "plot_3d_visualization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
