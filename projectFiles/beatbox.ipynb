{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting and Organizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to make a master dataframe with all the relevant data. This master dataframe will contain an entry for every single onset, for every single wav file in the audio file. If an audio file is multiple drum sounds, then there is a single onset for each drum sound, and an single audio file will contirnbute to multiple entries in the dataset. We will have to parse AVP and LVT seperately and then combine them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse AVP csv, get the onset time, instrument label, onset phoneme, coda phoneme, dataset, participant id, subset, csv file path, wav file path\n",
    "def parse_avp_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Parses an AVP CSV with no header, returning a list of dicts.\n",
    "    Each dict has:\n",
    "      - onset_time (float)\n",
    "      - instrument_label (str)\n",
    "      - onset_phoneme (str)\n",
    "      - coda_phoneme (str)\n",
    "      - dataset (str) = \"AVP\"\n",
    "      - participant_id (str)\n",
    "      - subset (str) = \"personal\"\n",
    "      - csv_file_path (str)\n",
    "      - wav_file_path (str)\n",
    "    \"\"\"\n",
    "    # 1) Extract some metadata from the file path\n",
    "    csv_dir = os.path.dirname(csv_path)             # e.g., \"AVP_Dataset/Personal/Participant_1\"\n",
    "    csv_file_name = os.path.basename(csv_path)      # e.g., \"P1_kd.csv\" or \"Participant_1_kd.csv\"\n",
    "    base_name, _ = os.path.splitext(csv_file_name)  # e.g., \"P1_kd\" or \"Participant_1_kd\"\n",
    "    \n",
    "    # 2) Determine participant_id from the file name\n",
    "    #    Suppose your file name is \"P1_kd.csv\", so the participant part is \"P1\".\n",
    "    #    If it’s \"Participant_1_kd.csv\", you might want the first two segments. Adapt as needed.\n",
    "    #    Here's a simple approach:\n",
    "    parts = base_name.split(\"_\")  # e.g. [\"P1\", \"kd\"] or [\"Participant\", \"1\", \"kd\"]\n",
    "    participant_id = parts[0]     # e.g. \"P1\" or \"Participant\"\n",
    "    # If your actual naming is more complicated, tweak the logic. \n",
    "    # For instance, if \"Participant_1\" is always 2 segments, you might do participant_id = \"_\".join(parts[:2])\n",
    "    \n",
    "    # 3) Build the wav path. If the CSV is \"P1_kd.csv\", the wav is \"P1_kd.wav\" in the same folder.\n",
    "    wav_file_name = base_name + \".wav\"\n",
    "    wav_file_path = os.path.join(csv_dir, wav_file_name)\n",
    "    \n",
    "    # 4) We'll fix \"dataset\" = \"AVP\" and \"subset\" = \"personal\" \n",
    "    #    (since that's the arrangement you described for the AVP dataset).\n",
    "    dataset = \"AVP\"\n",
    "    subset = \"personal\"\n",
    "    \n",
    "    # 5) Parse each row of the CSV\n",
    "    data = []\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            # Expecting: [onset_time, instrument_label, onset_phoneme, coda_phoneme]\n",
    "            if len(row) < 2:\n",
    "                continue  # skip empty or malformed lines\n",
    "            onset_time = float(row[0])\n",
    "            instrument_label = row[1]\n",
    "            onset_phoneme = row[2] if len(row) > 2 else ''\n",
    "            coda_phoneme = row[3] if len(row) > 3 else ''\n",
    "            \n",
    "            entry = {\n",
    "                'onset_time': onset_time,\n",
    "                'instrument_label': instrument_label,\n",
    "                'onset_phoneme': onset_phoneme,\n",
    "                'coda_phoneme': coda_phoneme,\n",
    "                'dataset': dataset,\n",
    "                'participant_id': participant_id,\n",
    "                'subset': subset,\n",
    "                'csv_file_path': csv_path,\n",
    "                'wav_file_path': wav_file_path\n",
    "            }\n",
    "            data.append(entry)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to collect all AVP data, takes a root directory as input, walks through the AVP dataset directory and collects all CSV data into a master DataFrame\n",
    "def collect_all_avp_data(root_dir):\n",
    "    \"\"\"\n",
    "    Walks through the AVP dataset directory and collects all CSV data into a master DataFrame,\n",
    "    maintaining the grouping of entries from the same CSV file\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    personal_dir = os.path.join(root_dir, \"Personal\")\n",
    "    \n",
    "    # Walk through all participant directories in sorted order\n",
    "    for participant_dir in sorted(os.listdir(personal_dir)):\n",
    "        participant_path = os.path.join(personal_dir, participant_dir)\n",
    "        \n",
    "        # Skip if not a directory or hidden files\n",
    "        if not os.path.isdir(participant_path) or participant_dir.startswith('.'):\n",
    "            continue\n",
    "            \n",
    "        # Process CSV files in sorted order\n",
    "        for file_name in sorted(os.listdir(participant_path)):\n",
    "            if file_name.endswith('.csv'):\n",
    "                csv_path = os.path.join(participant_path, file_name)\n",
    "                \n",
    "                try:\n",
    "                    parsed_data = parse_avp_csv(csv_path)\n",
    "                    # Add the source filename as a field for sorting\n",
    "                    for entry in parsed_data:\n",
    "                        entry['source_file'] = file_name\n",
    "                    all_data.extend(parsed_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {csv_path}: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Sort to maintain grouping:\n",
    "    # First by participant_id, then by source file, then by onset_time\n",
    "    df = df.sort_values(['participant_id', 'source_file', 'onset_time'])\n",
    "    \n",
    "    # Optionally remove the temporary source_file column if you don't need it\n",
    "    df = df.drop('source_file', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_standardized_phoneme(simple_phoneme, is_onset=True):\n",
    "#     \"\"\"\n",
    "#     Converts LVT phoneme notation to match AVP's IPA notation\n",
    "    \n",
    "#     Args:\n",
    "#         simple_phoneme: The phoneme from LVT dataset\n",
    "#         is_onset: Boolean indicating if this is an onset (True) or coda (False) phoneme\n",
    "#     \"\"\"\n",
    "#     # Onset phonemes (consonants at start)\n",
    "#     onset_map = {\n",
    "#         '!': '!',        # keep as is (appears in both)\n",
    "#         'k': 'k',        # keep as is\n",
    "#         'p': 'p',        # keep as is\n",
    "#         's': 's',        # keep as is\n",
    "#         't': 't',        # keep as is\n",
    "#         'ts': 'ts',      # keep as is\n",
    "#         'tʃ': 'tʃ',      # keep as is\n",
    "#         'ʔ': 'tʃ',       # map glottal stop to tʃ\n",
    "#         'ʡʢ': 'ʡʢ'       # keep as is (appears in both)\n",
    "#     }\n",
    "    \n",
    "#     # Coda phonemes (vowels/endings)\n",
    "#     coda_map = {\n",
    "#         'a': 'æ',        # map 'a' to 'æ' as it's more common in AVP\n",
    "#         'h': 'h',        # keep as is\n",
    "#         'u': 'u',        # keep as is\n",
    "#         'x': 'x',        # keep as is\n",
    "#         'ʊ': 'ʊ'         # keep as is\n",
    "#     }\n",
    "    \n",
    "#     # Choose which mapping to use\n",
    "#     phoneme_map = onset_map if is_onset else coda_map\n",
    "    \n",
    "#     # Return mapped phoneme if it exists, otherwise return original\n",
    "#     return phoneme_map.get(simple_phoneme, simple_phoneme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functiuon to parse LVT csv, get the onset time, instrument label, onset phoneme, coda phoneme, dataset, participant id, subset, csv file path, wav file path\n",
    "def parse_lvt_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Parses an LVT CSV with no header, returning a list of dicts.\n",
    "    Similar to parse_avp_csv but handles LVT-specific formatting.\n",
    "    \"\"\"\n",
    "    # Extract metadata from the file path\n",
    "    csv_dir = os.path.dirname(csv_path)             \n",
    "    csv_file_name = os.path.basename(csv_path)      # e.g., \"AFRP.csv\"\n",
    "    base_name, _ = os.path.splitext(csv_file_name)  # e.g., \"AFRP\"\n",
    "    \n",
    "    # Determine if this is from Frase or Improviso folder\n",
    "    subset = \"Frase\" if \"Frase\" in csv_dir else \"Improviso\"\n",
    "    \n",
    "    # Participant ID is the filename without extension\n",
    "    participant_id = base_name\n",
    "    \n",
    "    # Build the wav path (add \"3\" before .wav)\n",
    "    wav_file_name = base_name + \"3.wav\"\n",
    "    wav_file_path = os.path.join(csv_dir, wav_file_name)\n",
    "    \n",
    "    # Mapping for instrument labels\n",
    "    instrument_map = {\n",
    "        \"Kick\": \"kd\",\n",
    "        \"Snare\": \"sd\",\n",
    "        \"HH\": \"hhc\"  # Assuming all HH in LVT are closed hi-hats\n",
    "    }\n",
    "    \n",
    "    data = []\n",
    "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue  # skip empty or malformed lines\n",
    "                \n",
    "            onset_time = float(row[0])\n",
    "            original_label = row[1]\n",
    "            instrument_label = instrument_map.get(original_label, original_label)\n",
    "            onset_phoneme = row[2] if len(row) > 2 else ''\n",
    "            coda_phoneme = row[3] if len(row) > 3 else ''\n",
    "            \n",
    "            # onset_phoneme = get_standardized_phoneme(row[2], is_onset=True)   # converts 'ts' if needed\n",
    "            # coda_phoneme = get_standardized_phoneme(row[3], is_onset=False)   # converts 'x' if needed\n",
    "            \n",
    "            entry = {\n",
    "                'onset_time': onset_time,\n",
    "                'instrument_label': instrument_label,\n",
    "                'onset_phoneme': onset_phoneme,\n",
    "                'coda_phoneme': coda_phoneme,\n",
    "                'dataset': \"LVT\",\n",
    "                'participant_id': participant_id,\n",
    "                'subset': subset,\n",
    "                'csv_file_path': csv_path,\n",
    "                'wav_file_path': wav_file_path\n",
    "            }\n",
    "            data.append(entry)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to collect all LVT data, takes a root directory as input, walks through the LVT dataset directory and collects all CSV data into a master DataFrame\n",
    "def collect_all_lvt_data(root_dir):\n",
    "    \"\"\"\n",
    "    Walks through the LVT dataset directory and collects all CSV data into a master DataFrame\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # Process both Frase and Improviso folders\n",
    "    for subset_dir in [\"Frase\", \"Improviso\"]:\n",
    "        subset_path = os.path.join(root_dir, subset_dir)\n",
    "        \n",
    "        # Skip if directory doesn't exist\n",
    "        if not os.path.isdir(subset_path):\n",
    "            continue\n",
    "            \n",
    "        # Process CSV files in sorted order\n",
    "        for file_name in sorted(os.listdir(subset_path)):\n",
    "            if file_name.endswith('.csv') and not file_name.startswith('.'):\n",
    "                csv_path = os.path.join(subset_path, file_name)\n",
    "                \n",
    "                try:\n",
    "                    parsed_data = parse_lvt_csv(csv_path)\n",
    "                    # Add source file for sorting\n",
    "                    for entry in parsed_data:\n",
    "                        entry['source_file'] = file_name\n",
    "                    all_data.extend(parsed_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {csv_path}: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data)\n",
    "    \n",
    "    # Sort to maintain grouping\n",
    "    df = df.sort_values(['subset', 'participant_id', 'source_file', 'onset_time'])\n",
    "    \n",
    "    # Remove temporary sorting column\n",
    "    df = df.drop('source_file', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create all datasets, calls the functions to collect AVP and LVT data and then combines them into a master dataframe\n",
    "def create_all_datasets():\n",
    "    avp_dataset_path = \"../AVP-LVT_Dataset/AVP_Dataset\"\n",
    "    lvt_dataset_path = \"../AVP-LVT_Dataset/LVT_Dataset\"\n",
    "    \n",
    "    # Collect data from both datasets\n",
    "    print(\"Processing AVP dataset...\")\n",
    "    avp_df = collect_all_avp_data(avp_dataset_path)\n",
    "    \n",
    "    print(\"Processing LVT dataset...\")\n",
    "    lvt_df = collect_all_lvt_data(lvt_dataset_path)\n",
    "    \n",
    "    # Save individual datasets\n",
    "    print(\"\\nSaving individual datasets...\")\n",
    "    avp_df.to_csv('EDA/avp_dataset.csv', index=False)\n",
    "    lvt_df.to_csv('EDA/lvt_dataset.csv', index=False)\n",
    "    \n",
    "    # Combine and save master dataset\n",
    "    print(\"Creating and saving master dataset...\")\n",
    "    master_df = pd.concat([avp_df, lvt_df], ignore_index=True)\n",
    "    master_df.to_csv('EDA/master_dataset.csv', index=False)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nDataset Summaries:\")\n",
    "    print(f\"AVP Dataset: {len(avp_df)} events\")\n",
    "    print(\"\\nAVP participants:\", len(avp_df['participant_id'].unique()))\n",
    "    print(\"AVP instrument distribution:\")\n",
    "    print(avp_df['instrument_label'].value_counts())\n",
    "    \n",
    "    print(f\"\\nLVT Dataset: {len(lvt_df)} events\")\n",
    "    print(\"LVT subsets:\", lvt_df['subset'].unique())\n",
    "    print(\"LVT participants:\", len(lvt_df['participant_id'].unique()))\n",
    "    print(\"LVT instrument distribution:\")\n",
    "    print(lvt_df['instrument_label'].value_counts())\n",
    "    \n",
    "    print(f\"\\nMaster Dataset: {len(master_df)} total events\")\n",
    "    print(\"Distribution by dataset:\")\n",
    "    print(master_df['dataset'].value_counts())\n",
    "    \n",
    "    return avp_df, lvt_df, master_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AVP dataset...\n",
      "Processing LVT dataset...\n",
      "\n",
      "Saving individual datasets...\n",
      "Creating and saving master dataset...\n",
      "\n",
      "Dataset Summaries:\n",
      "AVP Dataset: 4873 events\n",
      "\n",
      "AVP participants: 28\n",
      "AVP instrument distribution:\n",
      "instrument_label\n",
      "kd     1447\n",
      "sd     1253\n",
      "hhc    1164\n",
      "hho    1009\n",
      "Name: count, dtype: int64\n",
      "\n",
      "LVT Dataset: 841 events\n",
      "LVT subsets: ['Frase' 'Improviso']\n",
      "LVT participants: 40\n",
      "LVT instrument distribution:\n",
      "instrument_label\n",
      "hhc    334\n",
      "kd     329\n",
      "sd     178\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Master Dataset: 5714 total events\n",
      "Distribution by dataset:\n",
      "dataset\n",
      "AVP    4873\n",
      "LVT     841\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "avp_df, lvt_df, master_df = create_all_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have master_dataset.csv, and master_df, both of which contain the info for every single onset for every single sound in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVP Unique Onset Phonemes:\n",
      "['!', 'dʒ', 'k', 'kg', 'kʃ', 'p', 's', 't', 'ts', 'tɕ', 'tʃ', 'tʒ', 'ʡʢ']\n",
      "\n",
      "AVP Unique Coda Phonemes:\n",
      "['I', 'a', 'e', 'h', 'i', 'o', 'u', 'x', 'æ', 'œ', 'ɐ', 'ɘ', 'ə', 'ɪ', 'ɯ', 'ʊ', 'ʌ']\n",
      "\n",
      "LVT Unique Onset Phonemes:\n",
      "['!', 'k', 'p', 's', 't', 'ts', 'tʃ', 'ʔ', 'ʡʢ']\n",
      "\n",
      "LVT Unique Coda Phonemes:\n",
      "['a', 'h', 'u', 'x', 'ʊ']\n"
     ]
    }
   ],
   "source": [
    "def analyze_phonemes():\n",
    "    \"\"\"\n",
    "    Analyze and compare phonemes between AVP and LVT datasets\n",
    "    \"\"\"\n",
    "    # Read both datasets\n",
    "    avp_df = pd.read_csv('EDA/avp_dataset.csv')\n",
    "    lvt_df = pd.read_csv('EDA/lvt_dataset.csv')\n",
    "    \n",
    "    print(\"AVP Unique Onset Phonemes:\")\n",
    "    print(sorted(avp_df['onset_phoneme'].unique()))\n",
    "    print(\"\\nAVP Unique Coda Phonemes:\")\n",
    "    print(sorted(avp_df['coda_phoneme'].unique()))\n",
    "    \n",
    "    print(\"\\nLVT Unique Onset Phonemes:\")\n",
    "    print(sorted(lvt_df['onset_phoneme'].unique()))\n",
    "    print(\"\\nLVT Unique Coda Phonemes:\")\n",
    "    print(sorted(lvt_df['coda_phoneme'].unique()))\n",
    "\n",
    "# Run the analysis\n",
    "analyze_phonemes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Try using soundfile directly instead of librosa.load\n",
    "def segment_audio(master_df, output_dir, segment_duration=0.5):\n",
    "    \"\"\"\n",
    "    Segments audio files and saves them to disk.\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    segment_info = []\n",
    "    \n",
    "    # Group by wav_file_path\n",
    "    for wav_path, group in tqdm(master_df.groupby('wav_file_path')):\n",
    "        print(f\"\\nProcessing {wav_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Use soundfile instead of librosa.load\n",
    "            y, sr = sf.read(wav_path)\n",
    "            \n",
    "            # Process each onset in this file\n",
    "            for idx, row in group.iterrows():\n",
    "                start_sample = int(row['onset_time'] * sr)\n",
    "                end_sample = start_sample + int(segment_duration * sr)\n",
    "                \n",
    "                # Handle edge cases\n",
    "                if start_sample < 0:\n",
    "                    start_sample = 0\n",
    "                if end_sample > len(y):\n",
    "                    end_sample = len(y)\n",
    "                \n",
    "                if end_sample > start_sample:\n",
    "                    segment = y[start_sample:end_sample]\n",
    "                    \n",
    "                    # Pad if needed\n",
    "                    if len(segment) < int(segment_duration * sr):\n",
    "                        segment = np.pad(segment, \n",
    "                                      (0, int(segment_duration * sr) - len(segment)),\n",
    "                                      mode='constant')\n",
    "                    \n",
    "                    segment_filename = (f\"{row['dataset']}_{row['participant_id']}_\"\n",
    "                                     f\"{row['instrument_label']}_{idx:04d}.wav\")\n",
    "                    \n",
    "                    segment_path = output_dir / segment_filename\n",
    "                    sf.write(str(segment_path), segment, sr)\n",
    "                    \n",
    "                    segment_info.append({\n",
    "                        'segment_path': str(segment_path),\n",
    "                        'instrument_label': row['instrument_label'],\n",
    "                        'participant_id': row['participant_id'],\n",
    "                        'dataset': row['dataset'],\n",
    "                        'original_wav': wav_path,\n",
    "                        'onset_time': row['onset_time']\n",
    "                    })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {wav_path}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    segment_df = pd.DataFrame(segment_info)\n",
    "    segment_df.to_csv('segment_info.csv', index=False)\n",
    "    \n",
    "    return segment_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting audio segmentation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8/180 [00:00<00:02, 78.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_1/P1_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_1/P1_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_1/P1_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_1/P1_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_1/P1_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_10/P10_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_10/P10_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_10/P10_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_10/P10_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_10/P10_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_11/P11_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_11/P11_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_11/P11_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_11/P11_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_11/P11_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_12/P12_HHclosed_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 27/180 [00:00<00:01, 86.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_12/P12_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_12/P12_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_12/P12_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_12/P12_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_13/P13_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_13/P13_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_13/P13_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_13/P13_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_13/P13_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_14/P14_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_14/P14_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_14/P14_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_14/P14_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_14/P14_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_15/P15_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_15/P15_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_15/P15_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_15/P15_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_15/P15_Snare_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 45/180 [00:00<00:01, 86.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_16/P16_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_16/P16_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_16/P16_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_16/P16_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_16/P16_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_17/P17_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_17/P17_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_17/P17_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_17/P17_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_17/P17_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_18/P18_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_18/P18_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_18/P18_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_18/P18_Kick_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 63/180 [00:00<00:01, 79.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_18/P18_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_19/P19_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_19/P19_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_19/P19_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_19/P19_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_19/P19_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_2/P2_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_2/P2_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_2/P2_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_2/P2_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_2/P2_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_20/P20_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_20/P20_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_20/P20_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_20/P20_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_20/P20_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_21/P21_HHclosed_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 72/180 [00:00<00:01, 75.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_21/P21_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_21/P21_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_21/P21_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_21/P21_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_22/P22_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_22/P22_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_22/P22_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_22/P22_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_22/P22_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_23/P23_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_23/P23_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_23/P23_Improvisation_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 80/180 [00:01<00:01, 67.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_23/P23_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_23/P23_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_24/P24_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_24/P24_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_24/P24_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_24/P24_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_24/P24_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_25/P25_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_25/P25_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_25/P25_Improvisation_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 96/180 [00:01<00:01, 62.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_25/P25_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_25/P25_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_26/P26_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_26/P26_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_26/P26_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_26/P26_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_26/P26_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_27/P27_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_27/P27_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_27/P27_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_27/P27_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_27/P27_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_28/P28_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_28/P28_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_28/P28_Improvisation_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 112/180 [00:01<00:00, 69.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_28/P28_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_28/P28_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_3/P3_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_3/P3_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_3/P3_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_3/P3_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_3/P3_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_4/P4_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_4/P4_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_4/P4_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_4/P4_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_4/P4_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_5/P5_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_5/P5_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_5/P5_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_5/P5_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_5/P5_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_6/P6_HHclosed_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 132/180 [00:01<00:00, 80.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_6/P6_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_6/P6_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_6/P6_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_6/P6_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_7/P7_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_7/P7_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_7/P7_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_7/P7_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_7/P7_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_8/P8_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_8/P8_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_8/P8_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_8/P8_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_8/P8_Snare_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_9/P9_HHclosed_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_9/P9_HHopened_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_9/P9_Improvisation_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_9/P9_Kick_Personal.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/AVP_Dataset/Personal/Participant_9/P9_Snare_Personal.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 155/180 [00:01<00:00, 96.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/AFRP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/AZiP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/BeaP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/BicP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/CatP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/CavP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/CraP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/IsaP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/JOlP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/JSiP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/JoSP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/MCoP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/MafP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/MarP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/NorP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/RicP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/RobP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/SofP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/ZgaP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Frase/ZizP3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/AFRI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/AZiI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/BeaI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/BicI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/CatI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/CavI3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:02<00:00, 81.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/CraI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/IsaI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/JOlI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/JSiI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/JSoI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/MCoI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/MafI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/MarI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/NorI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/RicI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/RobI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/SofI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/ZgaI3.wav\n",
      "\n",
      "Processing ../AVP-LVT_Dataset/LVT_Dataset/Improviso/ZizI3.wav\n",
      "\n",
      "Segmentation Summary:\n",
      "Total segments extracted: 5714\n",
      "\n",
      "Instrument distribution:\n",
      "instrument_label\n",
      "kd     1776\n",
      "hhc    1498\n",
      "sd     1431\n",
      "hho    1009\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset distribution:\n",
      "dataset\n",
      "AVP    4873\n",
      "LVT     841\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "master_df = pd.read_csv('EDA/master_dataset.csv')\n",
    "    \n",
    "# Segment all audio files\n",
    "print(\"Starting audio segmentation...\")\n",
    "segment_df = segment_audio(master_df, output_dir='segments')\n",
    "    \n",
    "# Print summary\n",
    "print(\"\\nSegmentation Summary:\")\n",
    "print(f\"Total segments extracted: {len(segment_df)}\")\n",
    "print(\"\\nInstrument distribution:\")\n",
    "print(segment_df['instrument_label'].value_counts())\n",
    "print(\"\\nDataset distribution:\")\n",
    "print(segment_df['dataset'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
